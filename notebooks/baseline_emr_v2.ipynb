{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Déployez un modèle dans le cloud**\n",
    "\n",
    "# Sommaire\n",
    "    \n",
    "* **✔ 1. [Préambule](#introduction)**\n",
    "    * ✔ 1.1 [Problématique](#problem-statement)\n",
    "    * ✔ 1.2 Objectifs dans ce projet\n",
    "    * ✔ 1.3 Déroulement des étapes du projet\n",
    "* **2. Choix techniques généraux retenus**\n",
    "    * 2.1 Calcul distribué\n",
    "    * 2.2 Transfert Learning\n",
    "* **3. Déploiement de la solution en local**\n",
    "    * 3.1 Environnement de travail\n",
    "    * 3.2 Installation de Spark\n",
    "    * 3.3 Installation des packages\n",
    "    * 3.4 Import des librairies\n",
    "    * 3.5 Définition des PATH pour charger les images et enregistrer les résultats\n",
    "    * 3.6 Création de la SparkSession\n",
    "    * 3.7 Traitement des données\n",
    "        * 3.7.1 Chargement des données\n",
    "        * 3.7.2 Préparation du modèle\n",
    "        * 3.7.3 Définition du processus de chargement des images et application de leur featurisation à travers l'utilisation de pandas UDF\n",
    "        * 3.7.4 Exécution des actions d'extractions de features\n",
    "    * 3.8 Chargement des données enregistrées et validation du résultat\n",
    "* **4. Déploiement de la solution sur le cloud**\n",
    "    * 4.1 Choix du prestataire cloud : AWS\n",
    "    * 4.2 Choix de la solution technique : EMR\n",
    "    * 4.3 Choix de la solution de stockage des données : Amazon S3\n",
    "    * 4.4 Configuration de l'environnement de travail\n",
    "    * 4.5 Upload de nos données sur S3\n",
    "    * 4.6 Configuration du serveur EMR\n",
    "        * 4.6.1 Étape 1 : Logiciels et étapes\n",
    "            * 4.6.1.1 Configuration des logiciels\n",
    "            * 4.6.1.2 Modifier les paramètres du logiciel\n",
    "        * 4.6.2 Étape 2 : Matériel\n",
    "        * 4.6.3 Étape 3 : Paramètres de cluster généraux\n",
    "            * 4.6.3.1 Options générales\n",
    "            * 4.6.3.2 Actions d'amorçage\n",
    "        * 4.6.4 Étape 4 : Sécurité\n",
    "            * 4.6.4.1 Options de sécurité\n",
    "    * 4.7 Instanciation du serveur\n",
    "    * 4.8 Création du tunnel SSH à l'instance EC2 (Maître)\n",
    "        * 4.8.1 Création des autorisations sur les connexions entrantes\n",
    "        * 4.8.2 Création du tunnel ssh vers le Driver\n",
    "        * 4.8.3 Configuration de FoxyProxy\n",
    "        * 4.8.4 Accès aux applications du serveur EMR via le tunnel ssh\n",
    "    * 4.9 Connexion au notebook JupyterHub\n",
    "    * 4.10 Exécution du code\n",
    "        * 4.10.1 Démarrage de la session Spark\n",
    "        * 4.10.2 Installation des packages\n",
    "        * 4.10.3 Import des librairies\n",
    "        * 4.10.4 Définition des PATH pour charger les images et enregistrer les résultats\n",
    "        * 4.10.5 Traitement des données\n",
    "            * 4.10.5.1 Chargement des données\n",
    "            * 4.10.5.2 Préparation du modèle\n",
    "            * 4.10.5.3 Définition du processus de chargement des images et application de leur featurisation à travers l'utilisation de pandas UDF\n",
    "            * 4.10.5.4 Exécutions des actions d'extractions de features\n",
    "        * 4.10.6 Chargement des données enregistrées et validation du résultat\n",
    "    * 4.11 Suivi de l'avancement des tâches avec le Serveur d'Historique Spark\n",
    "    * 4.12 Résiliation de l'instance EMR\n",
    "    * 4.13 Cloner le serveur EMR (si besoin)\n",
    "    * 4.14 Arborescence du serveur S3 à la fin du projet\n",
    "* **5. Conclusion**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"introduction\"></a> 1. **Préambule**\n",
    "\n",
    "## <a id=\"problem-statement\"></a> 1.1 Problématique\n",
    "\n",
    "La très jeune start-up de l'AgriTech, nommée \"**Fruits**!\", cherche à proposer des solutions innovantes pour la récolte des fruits.\n",
    "\n",
    "La volonté de l’entreprise est de **préserver la biodiversité des fruits** en permettant des traitements spécifiques pour chaque espèce de fruits en développant des robots cueilleurs intelligents.\n",
    "\n",
    "La start-up souhaite dans un premier temps se faire connaître en mettant à disposition du grand public une application mobile qui permettrait aux utilisateurs de prendre en photo un fruit et d'obtenir des informations sur ce fruit.\n",
    "\n",
    "Pour la start-up, cette application permettrait de **sensibiliser le grand public** à la biodiversité des fruits et de mettre en place une première version du moteur de classification des images de fruits.\n",
    "\n",
    "De plus, le développement de l’**application mobile** permettra de construire une première version de l'architecture **Big Data** nécessaire.\n",
    "\n",
    "## 1.2 Objectifs dans ce projet\n",
    "\n",
    "1. Développer une première chaîne de traitement des données qui comprendra :\n",
    "   * le **preprocessing**,\n",
    "   * et une étape de **réduction de dimension**.\n",
    "2. Tenir compte du fait que le volume de données va augmenter très rapidement après la livraison de ce projet, ce qui implique de:\n",
    "   * Déployer le traitement des données dans un **environnement Big Data**\n",
    "   * Développer les scripts en **pyspark** pour effectuer du **calcul distribué**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Déroulement des étapes du projet\n",
    "\n",
    "Le projet va être réalisé en 2 temps, dans deux environnements différents.\n",
    "\n",
    "Nous allons dans un premier temps développer et exécuter notre code en local, en travaillant sur un nombre limité d'images à traiter.\n",
    "\n",
    "Une fois les choix techniques validés, nous déploierons notre solution dans un environnement Big Data en mode distribué.\n",
    "\n",
    "<u>Pour cette raison, ce projet sera divisé en 3 parties</u>:\n",
    "1. Liste des choix techniques généraux retenus\n",
    "2. Déploiement de la solution en local\n",
    "3. Déploiement de la solution dans le cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\\. **Choix techniques généraux retenus**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Calcul distribué\n",
    "\n",
    "L’énoncé du projet nous impose de développer des scripts en **pyspark** afin de <u>prendre en compte l’augmentation très rapide du volume de donné après la livraison du projet</u>.\n",
    "\n",
    "Pour comprendre rapidement et simplement ce qu’est **pyspark** et son principe de fonctionnement, nous vous conseillons de lire cet article : [PySpark : Tout savoir sur la librairie Python](https://datascientest.com/pyspark)\n",
    "\n",
    "<u>Le début de l’article nous dit ceci </u>:\n",
    "\n",
    "« *Lorsque l’on parle de traitement de bases de données sur python, on pense immédiatement à la librairie pandas. Cependant, lorsqu’on a affaire à des bases de données trop massives, les calculs deviennent trop lents. Heureusement, il existe une autre librairie python, assez proche de pandas, qui permet de traiter des très grandes quantités de données : PySpark. Apache Spark est un framework open-source développé par l’AMPLab de UC Berkeley permettant de traiter des bases de données massives en utilisant le calcul distribué, technique qui consiste à exploiter plusieurs unités de calcul réparties en clusters au profit d’un seul projet afin de diviser le temps d’exécution d’une requête. \n",
    "Spark a été développé en Scala et est au meilleur de ses capacités dans son langage natif. Cependant, la librairie PySpark propose de l’utiliser avec le langage Python, en gardant des performances similaires à des implémentations en Scala. Pyspark est donc une bonne alternative à la librairie pandas lorsqu’on cherche à traiter des jeux de données trop volumineux qui entraînent des calculs trop chronophages.* »\n",
    "\n",
    "Comme nous le constatons, **pySpark** est un moyen de communiquer avec **Spark** via le langage **Python**.\n",
    "\n",
    "**Spark**, quant à lui, est un outil qui permet de gérer et de coordonner l'exécution de tâches sur des données à travers un groupe d'ordinateurs.\n",
    "\n",
    "<u>Spark (ou Apache Spark) est un framework open source de calcul distribué in-memory pour le traitement et l'analyse de données massives</u>.\n",
    "\n",
    "Un autre [article très intéressant et beaucoup plus complet pour comprendre le **fonctionnement de Spark**](https://www.veonum.com/apache-spark-pour-les-nuls/), ainsi que le rôle des **Spark Session** que nous utiliserons dans ce projet.\n",
    "\n",
    "<u>Voici également un extrait</u>:\n",
    "\n",
    "*Les applications Spark se composent d’un pilote (« driver process ») et de plusieurs exécuteurs (« executor processes »). Il peut être configuré pour être lui-même l’exécuteur (local mode) ou en utiliser autant que nécessaire pour traiter l’application, Spark prenant en charge la mise à l’échelle automatique par une configuration d’un nombre minimum et maximum d’exécuteurs.*\n",
    "\n",
    "![Schéma de Spark](../baseline/img/spark-schema.png)\n",
    "\n",
    "*Le driver (parfois appelé « Spark Session ») distribue et planifie les tâches entre les différents exécuteurs qui les exécutent et permettent un traitement réparti. Il est le responsable de l’exécution du code sur les différentes machines.<br />\n",
    "Chaque exécuteur est un processus Java Virtual Machine (JVM) distinct dont il est possible de configurer le nombre de CPU et la quantité de mémoire qui lui est alloué.<br />\n",
    "Une seule tâche peut traiter un fractionnement de données à la fois.*\n",
    "\n",
    "Dans les deux environnements (Local et Cloud) nous utiliserons donc **Spark** et nous l’exploiterons à travers des scripts python grâce à **PySpark**.\n",
    "\n",
    "Dans la <u>version locale</u> de notre script nous **simulerons le calcul distribué** afin de valider que notre solution fonctionne.\n",
    "\n",
    "Dans la <u>version cloud</u> nous **réaliserons les opérations sur un cluster de machines**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Transfert Learning\n",
    "\n",
    "L'énoncé du projet nous demande également de réaliser une première chaîne de traitement des données qui comprendra la **préparation des données** et une étape de **réduction de dimensionnalité**.\n",
    "\n",
    "Il est également précisé qu'il n'est pas nécessaire d'entraîner un modèle pour le moment.\n",
    "\n",
    "Nous décidons de partir sur une solution de **transfert learning**, qui consiste à utiliser un modèle pré-entraîné  (ici **MobileNetV2**) pour résoudre un problème similaire plus général et à l'adapter à notre problématique spécifique.\n",
    "\n",
    "Nous allons fournir au modèle nos images, et nous allons <u>récupérer l'avant dernière couche</u> du modèle.\n",
    "\n",
    "En effet la dernière couche de modèle est une couche **softmax** qui permet la classification des images ce que nous ne souhaitons pas dans ce projet.\n",
    "\n",
    "L'avant dernière couche correspond à un **vecteur réduit** de dimension (1, 1, 1280).\n",
    "\n",
    "Cela permettra de réaliser une première version du moteur pour la classification des images des fruits.\n",
    "\n",
    "**MobileNetV2** a été retenu pour sa <u>rapidité d'exécution</u>, particulièrement adaptée pour le traitement d'un gros volume de données ainsi que la <u>faible dimensionnalité du vecteur de caractéristique en sortie</u> (1, 1, 1280)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\\. Déploiement de la solution en local\n",
    "\n",
    "## 3.1 Environnement de travail\n",
    "\n",
    "Pour des raisons de simplicité, nous développons dans un environnement Linux Ubuntu (exécuté depuis une machine Windows dans une machine virtuelle)\n",
    "* Pour installer une machine virtuelle :  https://www.malekal.com/meilleurs-logiciels-de-machine-virtuelle-gratuits-ou-payants/\n",
    "\n",
    "\n",
    "Compte tenu de l'usage dans l'entreprise de développer dans un environnement intégré de développement (IDE) VS Code sur Windows, nous avons profité de ce projet pilote pour comparer 3 solutions pour le développement et le test en local avant le déploiement dans le cloud :\n",
    "1. Installation de Spark directement sur Windows, ce qui peux se faire avec la version 3.4.0 de Spark sans devoir nécessairement installer Winutils, l'interface Hadoop (HDFS) vers les fonctionnalités natives de Windows.\n",
    "2. Installation de Spark sur un sous-système WSL Ubuntu 20.04, et mise en place d'une communication client-serveur en réseau local entre les deux environnements : déploiement depuis les notebooks Jupyter/VS Code/Windows de scripts PySpark vers le master Spark hébergé sur le système Ubuntu.\n",
    "3. Réseau local physique avec cette fois-ci une communication en réseau privé avec une seconde machine équipé du seul système Ubuntu.\n",
    "4. Utilisation des fonctionnalités Remote Development de VS Code, avec docker / ssh / ... \n",
    "\n",
    "\n",
    "## 3.2 Installation de Spark\n",
    "\n",
    "[La première étape consiste à installer Spark ](https://computingforgeeks.com/how-to-install-apache-spark-on-ubuntu-debian/)\n",
    "\n",
    "## 3.3 Installation des packages\n",
    "\n",
    "On installe ensuite à l'aide du gestionnaire de paquets **pip** les packages qui nous seront nécessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Pandas pillow tensorflow pyspark pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pandas\n",
      "Version: 2.0.0\n",
      "Summary: Powerful data structures for data analysis, time series, and statistics\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: The Pandas Development Team <pandas-dev@python.org>\n",
      "License: BSD 3-Clause License\n",
      "\n",
      "Copyright (c) 2008-2011, AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team\n",
      "All rights reserved.\n",
      "\n",
      "Copyright (c) 2011-2023, Open source contributors.\n",
      "\n",
      "Redistribution and use in source and binary forms, with or without\n",
      "modification, are permitted provided that the following conditions are met:\n",
      "\n",
      "* Redistributions of source code must retain the above copyright notice, this\n",
      "  list of conditions and the following disclaimer.\n",
      "\n",
      "* Redistributions in binary form must reproduce the above copyright notice,\n",
      "  this list of conditions and the following disclaimer in the documentation\n",
      "  and/or other materials provided with the distribution.\n",
      "\n",
      "* Neither the name of the copyright holder nor the names of its\n",
      "  contributors may be used to endorse or promote products derived from\n",
      "  this software without specific prior written permission.\n",
      "\n",
      "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "\n",
      "Location: c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\n",
      "Requires: numpy, numpy, python-dateutil, pytz, tzdata\n",
      "Required-by: evidently, fastparquet, gspread-pandas, seaborn, statsmodels, xarray\n",
      "---\n",
      "Name: Pillow\n",
      "Version: 9.5.0\n",
      "Summary: Python Imaging Library (Fork)\n",
      "Home-page: https://python-pillow.org\n",
      "Author: Jeffrey A. Clark (Alex)\n",
      "Author-email: aclark@aclark.net\n",
      "License: HPND\n",
      "Location: c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\n",
      "Requires: \n",
      "Required-by: bokeh, matplotlib\n",
      "---\n",
      "Name: tensorflow\n",
      "Version: 2.12.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\n",
      "Requires: tensorflow-intel\n",
      "Required-by: \n",
      "---\n",
      "Name: pyspark\n",
      "Version: 3.4.0\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n",
      "---\n",
      "Name: pyarrow\n",
      "Version: 11.0.0\n",
      "Summary: Python library for Apache Arrow\n",
      "Home-page: https://arrow.apache.org/\n",
      "Author: \n",
      "Author-email: \n",
      "License: Apache License, Version 2.0\n",
      "Location: c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\n",
      "Requires: numpy\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show Pandas pillow tensorflow pyspark pyarrow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from keras.utils import img_to_array\n",
    "from keras import Model\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 Description des librairies et fonctions utilisées\n",
    "\n",
    "📌 **UDF** est l'acronyme de *User-Defined Function*.\n",
    "\n",
    "Description en français :\n",
    "* **`pandas`** : bibliothèque Python pour la manipulation et l'analyse de données.\n",
    "* **`PIL (Python Imaging Library)`** : bibliothèque Python pour l'ouverture et la manipulation d'images.\n",
    "* **`numpy`** : bibliothèque Python pour la manipulation de tableaux multidimensionnels.\n",
    "* **`io`** : bibliothèque Python pour la gestion des entrées/sorties.\n",
    "* **`os`** : bibliothèque Python pour l'interaction avec le système d'exploitation.\n",
    "* **`tensorflow`** : bibliothèque open-source d'apprentissage automatique pour les données numériques et les réseaux de neurones.\n",
    "* **`MobileNetV2`** : modèle de réseau de neurones pré-entraîné pour la classification d'images.\n",
    "* **`preprocess_input`** : fonction de prétraitement pour les images avant l'utilisation de MobileNetV2.\n",
    "* **`img_to_array`** : fonction pour convertir une image PIL en tableau numpy.\n",
    "* **`Model`** : classe Keras pour la définition de modèles d'apprentissage en profondeur.\n",
    "* **`pyspark`** : framework open-source pour le traitement de données en masse distribuées sur des clusters.\n",
    "* **`col`** : fonction pour sélectionner une colonne dans un DataFrame Spark.\n",
    "* **`pandas_udf`** : fonction pour exécuter une fonction Pandas sur un DataFrame Spark.\n",
    "* **`PandasUDFType`** : enum pour spécifier le type de la fonction PandasUDF.\n",
    "* **`element_at`** : fonction pour extraire l'élément d'une liste à une position donnée dans un DataFrame Spark.\n",
    "* **`split`** : fonction pour diviser une chaîne en une liste de sous-chaînes en utilisant un délimiteur spécifié dans un DataFrame Spark.\n",
    "* **`SparkSession`** : point d'entrée pour l'interaction avec les données dans Spark.\n",
    "\n",
    "Description en anglais :\n",
    "* **`pandas`**: Python library for data manipulation and analysis.\n",
    "* **`PIL (Python Imaging Library)`**: Python library for opening and manipulating images.\n",
    "* **`numpy`**: Python library for multidimensional array manipulation.\n",
    "* **`io`**: Python library for managing input/output operations.\n",
    "* **`os`**: Python library for interacting with the operating system.\n",
    "* **`tensorflow`**: open-source machine learning library for numerical data and neural networks.\n",
    "* **`MobileNetV2`**: pre-trained neural network model for image classification.\n",
    "* **`preprocess_input`**: pre-processing function for images before using MobileNetV2.\n",
    "* **`img_to_array`**: function to convert a PIL image to a numpy array.\n",
    "* **`Model`**: Keras class for defining deep learning models.\n",
    "* **`pyspark`**: open-source framework for processing distributed data on clusters.\n",
    "* **`col`**: function to select a column in a Spark DataFrame.\n",
    "* **`pandas_udf`**: function to execute a Pandas function on a Spark DataFrame.\n",
    "* **`PandasUDFType`**: enum to specify the type of PandasUDF function.\n",
    "* **`element_at`**: function to extract the element of a list at a given position in a Spark DataFrame.\n",
    "* **`split`**: function to split a string into a list of sub-strings using a specified delimiter in a Spark DataFrame.\n",
    "* **`SparkSession`**: entry point for interacting with data in Spark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Définition des PATH pour charger les images <br /> et enregistrer les résultats\n",
    "\n",
    "Dans cette version locale nous partons du principe que les données sont stockées dans le même répertoire que le notebook.\n",
    "\n",
    "Nous n'utilisons qu'un extrait de **300 images** à traiter dans cette première version en local.\n",
    "\n",
    "L'extrait des images à charger est stockée dans le dossier `data/im/sample_300`. <mark>**Test1**</mark>\n",
    "\n",
    "Nous enregistrerons le résultat de notre traitement dans le dossier `tmp/prep/im/sample_300`. <mark>\"**Results_Local**\"</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚧 **TODO** 2 choses à faire ici :\n",
    "1. Former l'extrait\n",
    "2. Mettre en place mon env habituel (pepper.env)\n",
    "\n",
    "Pour extraire le sample 300 ?\n",
    "\n",
    "A priori, il faut prendre équitablement sur les 131 espèces (de fruits ET légumes). Donc, dans le cadre strict du projet, nous devrions ne prendre que les images de fruits. Mais clairement, c'est le type d'attention et de scrupule dont ils se f...\n",
    "\n",
    "Il y a environ 150 exemplaires par catégorie.\n",
    "\n",
    "L'apprentissage n'est pas supervisé, c'est du transfer learning.\n",
    "\n",
    "Donc on n'a pas à conserver les labels donnés par les noms de dossiers.\n",
    "\n",
    "Faisons une première version, il sera toujours temps d'améliorer ensuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\Projects\\pepper_cloud_based_model\\data\\im\\sample_300\n",
      "C:\\Users\\franc\\Projects\\pepper_cloud_based_model\\tmp\\prep\\im\\sample_300\n"
     ]
    }
   ],
   "source": [
    "from fruits.env import get_data_im_sample_300_dir, get_prep_im_sample_300_dir\n",
    "\n",
    "print(get_data_im_sample_300_dir())\n",
    "print(get_prep_im_sample_300_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\franc\\\\Projects\\\\pepper_cloud_based_model\\\\dataset\\\\fruits-360_dataset\\\\Test'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Apple Braeburn/321_100.jpg',\n",
       " 'Apple Braeburn/322_100.jpg',\n",
       " 'Apple Braeburn/323_100.jpg',\n",
       " 'Apple Braeburn/324_100.jpg',\n",
       " 'Apple Braeburn/325_100.jpg',\n",
       " 'Apple Braeburn/326_100.jpg',\n",
       " 'Apple Braeburn/327_100.jpg',\n",
       " 'Apple Braeburn/32_100.jpg',\n",
       " 'Apple Braeburn/33_100.jpg',\n",
       " 'Apple Braeburn/34_100.jpg',\n",
       " 'Apple Braeburn/35_100.jpg',\n",
       " 'Apple Braeburn/36_100.jpg',\n",
       " 'Apple Braeburn/37_100.jpg',\n",
       " 'Apple Braeburn/38_100.jpg',\n",
       " 'Apple Braeburn/39_100.jpg',\n",
       " 'Apple Braeburn/3_100.jpg',\n",
       " 'Apple Braeburn/40_100.jpg',\n",
       " 'Apple Braeburn/41_100.jpg',\n",
       " 'Apple Braeburn/42_100.jpg',\n",
       " 'Apple Braeburn/43_100.jpg',\n",
       " 'Apple Braeburn/44_100.jpg',\n",
       " 'Apple Braeburn/45_100.jpg',\n",
       " 'Apple Braeburn/46_100.jpg',\n",
       " 'Apple Braeburn/47_100.jpg',\n",
       " 'Apple Braeburn/48_100.jpg',\n",
       " 'Apple Braeburn/49_100.jpg',\n",
       " 'Apple Braeburn/4_100.jpg',\n",
       " 'Apple Braeburn/50_100.jpg',\n",
       " 'Apple Braeburn/51_100.jpg',\n",
       " 'Apple Braeburn/52_100.jpg',\n",
       " 'Apple Braeburn/53_100.jpg',\n",
       " 'Apple Braeburn/54_100.jpg',\n",
       " 'Apple Braeburn/55_100.jpg',\n",
       " 'Apple Braeburn/56_100.jpg',\n",
       " 'Apple Braeburn/57_100.jpg',\n",
       " 'Apple Braeburn/58_100.jpg',\n",
       " 'Apple Braeburn/59_100.jpg',\n",
       " 'Apple Braeburn/5_100.jpg',\n",
       " 'Apple Braeburn/60_100.jpg',\n",
       " 'Apple Braeburn/61_100.jpg',\n",
       " 'Apple Braeburn/62_100.jpg',\n",
       " 'Apple Braeburn/63_100.jpg',\n",
       " 'Apple Braeburn/64_100.jpg',\n",
       " 'Apple Braeburn/65_100.jpg',\n",
       " 'Apple Braeburn/66_100.jpg',\n",
       " 'Apple Braeburn/67_100.jpg',\n",
       " 'Apple Braeburn/68_100.jpg',\n",
       " 'Apple Braeburn/69_100.jpg',\n",
       " 'Apple Braeburn/6_100.jpg',\n",
       " 'Apple Braeburn/70_100.jpg',\n",
       " 'Apple Braeburn/71_100.jpg',\n",
       " 'Apple Braeburn/72_100.jpg',\n",
       " 'Apple Braeburn/73_100.jpg',\n",
       " 'Apple Braeburn/74_100.jpg',\n",
       " 'Apple Braeburn/75_100.jpg',\n",
       " 'Apple Braeburn/76_100.jpg',\n",
       " 'Apple Braeburn/77_100.jpg',\n",
       " 'Apple Braeburn/78_100.jpg',\n",
       " 'Apple Braeburn/79_100.jpg',\n",
       " 'Apple Braeburn/7_100.jpg',\n",
       " 'Apple Braeburn/80_100.jpg',\n",
       " 'Apple Braeburn/81_100.jpg',\n",
       " 'Apple Braeburn/82_100.jpg',\n",
       " 'Apple Braeburn/83_100.jpg',\n",
       " 'Apple Braeburn/84_100.jpg',\n",
       " 'Apple Braeburn/85_100.jpg',\n",
       " 'Apple Braeburn/86_100.jpg',\n",
       " 'Apple Braeburn/87_100.jpg',\n",
       " 'Apple Braeburn/88_100.jpg',\n",
       " 'Apple Braeburn/89_100.jpg',\n",
       " 'Apple Braeburn/8_100.jpg',\n",
       " 'Apple Braeburn/90_100.jpg',\n",
       " 'Apple Braeburn/91_100.jpg',\n",
       " 'Apple Braeburn/92_100.jpg',\n",
       " 'Apple Braeburn/93_100.jpg',\n",
       " 'Apple Braeburn/94_100.jpg',\n",
       " 'Apple Braeburn/95_100.jpg',\n",
       " 'Apple Braeburn/96_100.jpg',\n",
       " 'Apple Braeburn/97_100.jpg',\n",
       " 'Apple Braeburn/98_100.jpg',\n",
       " 'Apple Braeburn/99_100.jpg',\n",
       " 'Apple Braeburn/9_100.jpg',\n",
       " 'Apple Braeburn/r_321_100.jpg',\n",
       " 'Apple Braeburn/r_322_100.jpg',\n",
       " 'Apple Braeburn/r_323_100.jpg',\n",
       " 'Apple Braeburn/r_324_100.jpg',\n",
       " 'Apple Braeburn/r_325_100.jpg',\n",
       " 'Apple Braeburn/r_326_100.jpg',\n",
       " 'Apple Braeburn/r_327_100.jpg',\n",
       " 'Apple Braeburn/r_32_100.jpg',\n",
       " 'Apple Braeburn/r_33_100.jpg',\n",
       " 'Apple Braeburn/r_34_100.jpg',\n",
       " 'Apple Braeburn/r_35_100.jpg',\n",
       " 'Apple Braeburn/r_36_100.jpg',\n",
       " 'Apple Braeburn/r_37_100.jpg',\n",
       " 'Apple Braeburn/r_38_100.jpg',\n",
       " 'Apple Braeburn/r_39_100.jpg',\n",
       " 'Apple Braeburn/r_3_100.jpg',\n",
       " 'Apple Braeburn/r_40_100.jpg',\n",
       " 'Apple Braeburn/r_41_100.jpg',\n",
       " 'Apple Braeburn/r_42_100.jpg',\n",
       " 'Apple Braeburn/r_43_100.jpg',\n",
       " 'Apple Braeburn/r_44_100.jpg',\n",
       " 'Apple Braeburn/r_45_100.jpg',\n",
       " 'Apple Braeburn/r_46_100.jpg',\n",
       " 'Apple Braeburn/r_47_100.jpg',\n",
       " 'Apple Braeburn/r_48_100.jpg',\n",
       " 'Apple Braeburn/r_49_100.jpg',\n",
       " 'Apple Braeburn/r_4_100.jpg',\n",
       " 'Apple Braeburn/r_50_100.jpg',\n",
       " 'Apple Braeburn/r_51_100.jpg',\n",
       " 'Apple Braeburn/r_52_100.jpg',\n",
       " 'Apple Braeburn/r_53_100.jpg',\n",
       " 'Apple Braeburn/r_54_100.jpg',\n",
       " 'Apple Braeburn/r_55_100.jpg',\n",
       " 'Apple Braeburn/r_56_100.jpg',\n",
       " 'Apple Braeburn/r_57_100.jpg',\n",
       " 'Apple Braeburn/r_58_100.jpg',\n",
       " 'Apple Braeburn/r_59_100.jpg',\n",
       " 'Apple Braeburn/r_5_100.jpg',\n",
       " 'Apple Braeburn/r_60_100.jpg',\n",
       " 'Apple Braeburn/r_61_100.jpg',\n",
       " 'Apple Braeburn/r_62_100.jpg',\n",
       " 'Apple Braeburn/r_63_100.jpg',\n",
       " 'Apple Braeburn/r_64_100.jpg',\n",
       " 'Apple Braeburn/r_65_100.jpg',\n",
       " 'Apple Braeburn/r_66_100.jpg',\n",
       " 'Apple Braeburn/r_67_100.jpg',\n",
       " 'Apple Braeburn/r_68_100.jpg',\n",
       " 'Apple Braeburn/r_69_100.jpg',\n",
       " 'Apple Braeburn/r_6_100.jpg',\n",
       " 'Apple Braeburn/r_70_100.jpg',\n",
       " 'Apple Braeburn/r_71_100.jpg',\n",
       " 'Apple Braeburn/r_72_100.jpg',\n",
       " 'Apple Braeburn/r_73_100.jpg',\n",
       " 'Apple Braeburn/r_74_100.jpg',\n",
       " 'Apple Braeburn/r_75_100.jpg',\n",
       " 'Apple Braeburn/r_76_100.jpg',\n",
       " 'Apple Braeburn/r_77_100.jpg',\n",
       " 'Apple Braeburn/r_78_100.jpg',\n",
       " 'Apple Braeburn/r_79_100.jpg',\n",
       " 'Apple Braeburn/r_7_100.jpg',\n",
       " 'Apple Braeburn/r_80_100.jpg',\n",
       " 'Apple Braeburn/r_81_100.jpg',\n",
       " 'Apple Braeburn/r_82_100.jpg',\n",
       " 'Apple Braeburn/r_83_100.jpg',\n",
       " 'Apple Braeburn/r_84_100.jpg',\n",
       " 'Apple Braeburn/r_85_100.jpg',\n",
       " 'Apple Braeburn/r_86_100.jpg',\n",
       " 'Apple Braeburn/r_87_100.jpg',\n",
       " 'Apple Braeburn/r_88_100.jpg',\n",
       " 'Apple Braeburn/r_89_100.jpg',\n",
       " 'Apple Braeburn/r_8_100.jpg',\n",
       " 'Apple Braeburn/r_90_100.jpg',\n",
       " 'Apple Braeburn/r_91_100.jpg',\n",
       " 'Apple Braeburn/r_92_100.jpg',\n",
       " 'Apple Braeburn/r_93_100.jpg',\n",
       " 'Apple Braeburn/r_94_100.jpg',\n",
       " 'Apple Braeburn/r_95_100.jpg',\n",
       " 'Apple Braeburn/r_96_100.jpg',\n",
       " 'Apple Braeburn/r_97_100.jpg',\n",
       " 'Apple Braeburn/r_98_100.jpg',\n",
       " 'Apple Braeburn/r_99_100.jpg',\n",
       " 'Apple Braeburn/r_9_100.jpg',\n",
       " 'Apple Crimson Snow/100_100.jpg',\n",
       " 'Apple Crimson Snow/101_100.jpg',\n",
       " 'Apple Crimson Snow/102_100.jpg',\n",
       " 'Apple Crimson Snow/103_100.jpg',\n",
       " 'Apple Crimson Snow/104_100.jpg',\n",
       " 'Apple Crimson Snow/105_100.jpg',\n",
       " 'Apple Crimson Snow/106_100.jpg',\n",
       " 'Apple Crimson Snow/107_100.jpg',\n",
       " 'Apple Crimson Snow/108_100.jpg',\n",
       " 'Apple Crimson Snow/109_100.jpg',\n",
       " 'Apple Crimson Snow/110_100.jpg',\n",
       " 'Apple Crimson Snow/112_100.jpg',\n",
       " 'Apple Crimson Snow/113_100.jpg',\n",
       " 'Apple Crimson Snow/114_100.jpg',\n",
       " 'Apple Crimson Snow/115_100.jpg',\n",
       " 'Apple Crimson Snow/116_100.jpg',\n",
       " 'Apple Crimson Snow/117_100.jpg',\n",
       " 'Apple Crimson Snow/118_100.jpg',\n",
       " 'Apple Crimson Snow/119_100.jpg',\n",
       " 'Apple Crimson Snow/120_100.jpg',\n",
       " 'Apple Crimson Snow/121_100.jpg',\n",
       " 'Apple Crimson Snow/122_100.jpg',\n",
       " 'Apple Crimson Snow/123_100.jpg',\n",
       " 'Apple Crimson Snow/124_100.jpg',\n",
       " 'Apple Crimson Snow/125_100.jpg',\n",
       " 'Apple Crimson Snow/126_100.jpg',\n",
       " 'Apple Crimson Snow/127_100.jpg',\n",
       " 'Apple Crimson Snow/128_100.jpg',\n",
       " 'Apple Crimson Snow/129_100.jpg',\n",
       " 'Apple Crimson Snow/130_100.jpg',\n",
       " 'Apple Crimson Snow/131_100.jpg',\n",
       " 'Apple Crimson Snow/132_100.jpg',\n",
       " 'Apple Crimson Snow/133_100.jpg',\n",
       " 'Apple Crimson Snow/134_100.jpg',\n",
       " 'Apple Crimson Snow/135_100.jpg',\n",
       " 'Apple Crimson Snow/136_100.jpg',\n",
       " 'Apple Crimson Snow/137_100.jpg',\n",
       " 'Apple Crimson Snow/138_100.jpg',\n",
       " 'Apple Crimson Snow/139_100.jpg',\n",
       " 'Apple Crimson Snow/140_100.jpg',\n",
       " 'Apple Crimson Snow/141_100.jpg',\n",
       " 'Apple Crimson Snow/142_100.jpg',\n",
       " 'Apple Crimson Snow/143_100.jpg',\n",
       " 'Apple Crimson Snow/144_100.jpg',\n",
       " 'Apple Crimson Snow/145_100.jpg',\n",
       " 'Apple Crimson Snow/146_100.jpg',\n",
       " 'Apple Crimson Snow/147_100.jpg',\n",
       " 'Apple Crimson Snow/148_100.jpg',\n",
       " 'Apple Crimson Snow/149_100.jpg',\n",
       " 'Apple Crimson Snow/150_100.jpg',\n",
       " 'Apple Crimson Snow/151_100.jpg',\n",
       " 'Apple Crimson Snow/152_100.jpg',\n",
       " 'Apple Crimson Snow/153_100.jpg',\n",
       " 'Apple Crimson Snow/154_100.jpg',\n",
       " 'Apple Crimson Snow/155_100.jpg',\n",
       " 'Apple Crimson Snow/80_100.jpg',\n",
       " 'Apple Crimson Snow/81_100.jpg',\n",
       " 'Apple Crimson Snow/83_100.jpg',\n",
       " 'Apple Crimson Snow/84_100.jpg',\n",
       " 'Apple Crimson Snow/85_100.jpg',\n",
       " 'Apple Crimson Snow/86_100.jpg',\n",
       " 'Apple Crimson Snow/87_100.jpg',\n",
       " 'Apple Crimson Snow/88_100.jpg',\n",
       " 'Apple Crimson Snow/89_100.jpg',\n",
       " 'Apple Crimson Snow/90_100.jpg',\n",
       " 'Apple Crimson Snow/91_100.jpg',\n",
       " 'Apple Crimson Snow/92_100.jpg',\n",
       " 'Apple Crimson Snow/93_100.jpg',\n",
       " 'Apple Crimson Snow/94_100.jpg',\n",
       " 'Apple Crimson Snow/95_100.jpg',\n",
       " 'Apple Crimson Snow/96_100.jpg',\n",
       " 'Apple Crimson Snow/97_100.jpg',\n",
       " 'Apple Crimson Snow/98_100.jpg',\n",
       " 'Apple Crimson Snow/99_100.jpg',\n",
       " 'Apple Crimson Snow/r_105_100.jpg',\n",
       " 'Apple Crimson Snow/r_106_100.jpg',\n",
       " 'Apple Crimson Snow/r_111_100.jpg',\n",
       " 'Apple Crimson Snow/r_112_100.jpg',\n",
       " 'Apple Crimson Snow/r_113_100.jpg',\n",
       " 'Apple Crimson Snow/r_114_100.jpg',\n",
       " 'Apple Crimson Snow/r_115_100.jpg',\n",
       " 'Apple Crimson Snow/r_116_100.jpg',\n",
       " 'Apple Crimson Snow/r_117_100.jpg',\n",
       " 'Apple Crimson Snow/r_118_100.jpg',\n",
       " 'Apple Crimson Snow/r_120_100.jpg',\n",
       " 'Apple Crimson Snow/r_121_100.jpg',\n",
       " 'Apple Crimson Snow/r_122_100.jpg',\n",
       " 'Apple Crimson Snow/r_123_100.jpg',\n",
       " 'Apple Crimson Snow/r_124_100.jpg',\n",
       " 'Apple Crimson Snow/r_13_100.jpg',\n",
       " 'Apple Crimson Snow/r_14_100.jpg',\n",
       " 'Apple Crimson Snow/r_15_100.jpg',\n",
       " 'Apple Crimson Snow/r_16_100.jpg',\n",
       " 'Apple Crimson Snow/r_17_100.jpg',\n",
       " 'Apple Crimson Snow/r_183_100.jpg',\n",
       " 'Apple Crimson Snow/r_184_100.jpg',\n",
       " 'Apple Crimson Snow/r_18_100.jpg',\n",
       " 'Apple Crimson Snow/r_19_100.jpg',\n",
       " 'Apple Crimson Snow/r_20_100.jpg',\n",
       " 'Apple Crimson Snow/r_21_100.jpg',\n",
       " 'Apple Crimson Snow/r_22_100.jpg',\n",
       " 'Apple Crimson Snow/r_23_100.jpg',\n",
       " 'Apple Crimson Snow/r_24_100.jpg',\n",
       " 'Apple Crimson Snow/r_25_100.jpg',\n",
       " 'Apple Crimson Snow/r_26_100.jpg',\n",
       " 'Apple Crimson Snow/r_27_100.jpg',\n",
       " 'Apple Crimson Snow/r_28_100.jpg',\n",
       " 'Apple Crimson Snow/r_29_100.jpg',\n",
       " 'Apple Crimson Snow/r_30_100.jpg',\n",
       " 'Apple Crimson Snow/r_31_100.jpg',\n",
       " 'Apple Crimson Snow/r_32_100.jpg',\n",
       " 'Apple Crimson Snow/r_33_100.jpg',\n",
       " 'Apple Crimson Snow/r_34_100.jpg',\n",
       " 'Apple Crimson Snow/r_35_100.jpg',\n",
       " 'Apple Crimson Snow/r_36_100.jpg',\n",
       " 'Apple Crimson Snow/r_37_100.jpg',\n",
       " 'Apple Crimson Snow/r_38_100.jpg',\n",
       " 'Apple Crimson Snow/r_39_100.jpg',\n",
       " 'Apple Crimson Snow/r_40_100.jpg',\n",
       " 'Apple Crimson Snow/r_41_100.jpg',\n",
       " 'Apple Crimson Snow/r_42_100.jpg',\n",
       " 'Apple Crimson Snow/r_43_100.jpg',\n",
       " 'Apple Crimson Snow/r_44_100.jpg',\n",
       " 'Apple Crimson Snow/r_45_100.jpg',\n",
       " 'Apple Crimson Snow/r_46_100.jpg',\n",
       " 'Apple Crimson Snow/r_47_100.jpg',\n",
       " 'Apple Crimson Snow/r_48_100.jpg',\n",
       " 'Apple Crimson Snow/r_49_100.jpg',\n",
       " 'Apple Crimson Snow/r_50_100.jpg',\n",
       " 'Apple Crimson Snow/r_51_100.jpg',\n",
       " 'Apple Crimson Snow/r_52_100.jpg',\n",
       " 'Apple Crimson Snow/r_53_100.jpg',\n",
       " 'Apple Crimson Snow/r_54_100.jpg',\n",
       " 'Apple Crimson Snow/r_55_100.jpg',\n",
       " 'Apple Crimson Snow/r_56_100.jpg',\n",
       " 'Apple Crimson Snow/r_57_100.jpg',\n",
       " 'Apple Crimson Snow/r_58_100.jpg',\n",
       " 'Apple Crimson Snow/r_59_100.jpg',\n",
       " 'Apple Crimson Snow/r_60_100.jpg',\n",
       " 'Apple Crimson Snow/r_61_100.jpg',\n",
       " 'Apple Crimson Snow/r_62_100.jpg',\n",
       " 'Apple Crimson Snow/r_63_100.jpg',\n",
       " 'Apple Crimson Snow/r_64_100.jpg',\n",
       " 'Apple Crimson Snow/r_65_100.jpg',\n",
       " 'Apple Crimson Snow/r_66_100.jpg',\n",
       " 'Apple Crimson Snow/r_67_100.jpg',\n",
       " 'Apple Crimson Snow/r_69_100.jpg',\n",
       " 'Apple Crimson Snow/r_71_100.jpg',\n",
       " 'Apple Golden 1/100_100.jpg',\n",
       " 'Apple Golden 1/101_100.jpg',\n",
       " 'Apple Golden 1/102_100.jpg',\n",
       " 'Apple Golden 1/103_100.jpg',\n",
       " 'Apple Golden 1/104_100.jpg',\n",
       " 'Apple Golden 1/105_100.jpg',\n",
       " 'Apple Golden 1/106_100.jpg',\n",
       " 'Apple Golden 1/107_100.jpg',\n",
       " 'Apple Golden 1/108_100.jpg',\n",
       " 'Apple Golden 1/109_100.jpg',\n",
       " 'Apple Golden 1/110_100.jpg',\n",
       " 'Apple Golden 1/111_100.jpg',\n",
       " 'Apple Golden 1/112_100.jpg',\n",
       " 'Apple Golden 1/113_100.jpg',\n",
       " 'Apple Golden 1/114_100.jpg',\n",
       " 'Apple Golden 1/115_100.jpg',\n",
       " 'Apple Golden 1/116_100.jpg',\n",
       " 'Apple Golden 1/117_100.jpg',\n",
       " 'Apple Golden 1/118_100.jpg',\n",
       " 'Apple Golden 1/119_100.jpg',\n",
       " 'Apple Golden 1/120_100.jpg',\n",
       " 'Apple Golden 1/121_100.jpg',\n",
       " 'Apple Golden 1/122_100.jpg',\n",
       " 'Apple Golden 1/123_100.jpg',\n",
       " 'Apple Golden 1/124_100.jpg',\n",
       " 'Apple Golden 1/125_100.jpg',\n",
       " 'Apple Golden 1/126_100.jpg',\n",
       " 'Apple Golden 1/127_100.jpg',\n",
       " 'Apple Golden 1/128_100.jpg',\n",
       " 'Apple Golden 1/129_100.jpg',\n",
       " 'Apple Golden 1/130_100.jpg',\n",
       " 'Apple Golden 1/131_100.jpg',\n",
       " 'Apple Golden 1/132_100.jpg',\n",
       " 'Apple Golden 1/133_100.jpg',\n",
       " 'Apple Golden 1/134_100.jpg',\n",
       " 'Apple Golden 1/135_100.jpg',\n",
       " 'Apple Golden 1/136_100.jpg',\n",
       " 'Apple Golden 1/137_100.jpg',\n",
       " 'Apple Golden 1/138_100.jpg',\n",
       " 'Apple Golden 1/139_100.jpg',\n",
       " 'Apple Golden 1/140_100.jpg',\n",
       " 'Apple Golden 1/141_100.jpg',\n",
       " 'Apple Golden 1/142_100.jpg',\n",
       " 'Apple Golden 1/143_100.jpg',\n",
       " 'Apple Golden 1/144_100.jpg',\n",
       " 'Apple Golden 1/145_100.jpg',\n",
       " 'Apple Golden 1/146_100.jpg',\n",
       " 'Apple Golden 1/149_100.jpg',\n",
       " 'Apple Golden 1/150_100.jpg',\n",
       " 'Apple Golden 1/63_100.jpg',\n",
       " 'Apple Golden 1/72_100.jpg',\n",
       " 'Apple Golden 1/73_100.jpg',\n",
       " 'Apple Golden 1/74_100.jpg',\n",
       " 'Apple Golden 1/75_100.jpg',\n",
       " 'Apple Golden 1/76_100.jpg',\n",
       " 'Apple Golden 1/77_100.jpg',\n",
       " 'Apple Golden 1/78_100.jpg',\n",
       " 'Apple Golden 1/79_100.jpg',\n",
       " 'Apple Golden 1/80_100.jpg',\n",
       " 'Apple Golden 1/81_100.jpg',\n",
       " 'Apple Golden 1/82_100.jpg',\n",
       " 'Apple Golden 1/83_100.jpg',\n",
       " 'Apple Golden 1/84_100.jpg',\n",
       " 'Apple Golden 1/85_100.jpg',\n",
       " 'Apple Golden 1/86_100.jpg',\n",
       " 'Apple Golden 1/87_100.jpg',\n",
       " 'Apple Golden 1/88_100.jpg',\n",
       " 'Apple Golden 1/89_100.jpg',\n",
       " 'Apple Golden 1/90_100.jpg',\n",
       " 'Apple Golden 1/91_100.jpg',\n",
       " 'Apple Golden 1/92_100.jpg',\n",
       " 'Apple Golden 1/93_100.jpg',\n",
       " 'Apple Golden 1/94_100.jpg',\n",
       " 'Apple Golden 1/95_100.jpg',\n",
       " 'Apple Golden 1/96_100.jpg',\n",
       " 'Apple Golden 1/97_100.jpg',\n",
       " 'Apple Golden 1/98_100.jpg',\n",
       " 'Apple Golden 1/99_100.jpg',\n",
       " 'Apple Golden 1/r_321_100.jpg',\n",
       " 'Apple Golden 1/r_322_100.jpg',\n",
       " 'Apple Golden 1/r_323_100.jpg',\n",
       " 'Apple Golden 1/r_324_100.jpg',\n",
       " 'Apple Golden 1/r_325_100.jpg',\n",
       " 'Apple Golden 1/r_326_100.jpg',\n",
       " 'Apple Golden 1/r_327_100.jpg',\n",
       " 'Apple Golden 1/r_32_100.jpg',\n",
       " 'Apple Golden 1/r_33_100.jpg',\n",
       " 'Apple Golden 1/r_34_100.jpg',\n",
       " 'Apple Golden 1/r_35_100.jpg',\n",
       " 'Apple Golden 1/r_36_100.jpg',\n",
       " 'Apple Golden 1/r_37_100.jpg',\n",
       " 'Apple Golden 1/r_38_100.jpg',\n",
       " 'Apple Golden 1/r_39_100.jpg',\n",
       " 'Apple Golden 1/r_3_100.jpg',\n",
       " 'Apple Golden 1/r_40_100.jpg',\n",
       " 'Apple Golden 1/r_41_100.jpg',\n",
       " 'Apple Golden 1/r_42_100.jpg',\n",
       " 'Apple Golden 1/r_43_100.jpg',\n",
       " 'Apple Golden 1/r_44_100.jpg',\n",
       " 'Apple Golden 1/r_45_100.jpg',\n",
       " 'Apple Golden 1/r_46_100.jpg',\n",
       " 'Apple Golden 1/r_47_100.jpg',\n",
       " 'Apple Golden 1/r_48_100.jpg',\n",
       " 'Apple Golden 1/r_49_100.jpg',\n",
       " 'Apple Golden 1/r_4_100.jpg',\n",
       " 'Apple Golden 1/r_50_100.jpg',\n",
       " 'Apple Golden 1/r_51_100.jpg',\n",
       " 'Apple Golden 1/r_52_100.jpg',\n",
       " 'Apple Golden 1/r_53_100.jpg',\n",
       " 'Apple Golden 1/r_54_100.jpg',\n",
       " 'Apple Golden 1/r_55_100.jpg',\n",
       " 'Apple Golden 1/r_56_100.jpg',\n",
       " 'Apple Golden 1/r_57_100.jpg',\n",
       " 'Apple Golden 1/r_58_100.jpg',\n",
       " 'Apple Golden 1/r_59_100.jpg',\n",
       " 'Apple Golden 1/r_5_100.jpg',\n",
       " 'Apple Golden 1/r_60_100.jpg',\n",
       " 'Apple Golden 1/r_61_100.jpg',\n",
       " 'Apple Golden 1/r_62_100.jpg',\n",
       " 'Apple Golden 1/r_63_100.jpg',\n",
       " 'Apple Golden 1/r_64_100.jpg',\n",
       " 'Apple Golden 1/r_65_100.jpg',\n",
       " 'Apple Golden 1/r_66_100.jpg',\n",
       " 'Apple Golden 1/r_67_100.jpg',\n",
       " 'Apple Golden 1/r_68_100.jpg',\n",
       " 'Apple Golden 1/r_69_100.jpg',\n",
       " 'Apple Golden 1/r_6_100.jpg',\n",
       " 'Apple Golden 1/r_70_100.jpg',\n",
       " 'Apple Golden 1/r_71_100.jpg',\n",
       " 'Apple Golden 1/r_72_100.jpg',\n",
       " 'Apple Golden 1/r_73_100.jpg',\n",
       " 'Apple Golden 1/r_74_100.jpg',\n",
       " 'Apple Golden 1/r_75_100.jpg',\n",
       " 'Apple Golden 1/r_76_100.jpg',\n",
       " 'Apple Golden 1/r_77_100.jpg',\n",
       " 'Apple Golden 1/r_78_100.jpg',\n",
       " 'Apple Golden 1/r_79_100.jpg',\n",
       " 'Apple Golden 1/r_7_100.jpg',\n",
       " 'Apple Golden 1/r_80_100.jpg',\n",
       " 'Apple Golden 1/r_81_100.jpg',\n",
       " 'Apple Golden 1/r_82_100.jpg',\n",
       " 'Apple Golden 1/r_83_100.jpg',\n",
       " 'Apple Golden 1/r_84_100.jpg',\n",
       " 'Apple Golden 1/r_85_100.jpg',\n",
       " 'Apple Golden 1/r_86_100.jpg',\n",
       " 'Apple Golden 1/r_87_100.jpg',\n",
       " 'Apple Golden 1/r_88_100.jpg',\n",
       " 'Apple Golden 1/r_89_100.jpg',\n",
       " 'Apple Golden 1/r_8_100.jpg',\n",
       " 'Apple Golden 1/r_90_100.jpg',\n",
       " 'Apple Golden 1/r_91_100.jpg',\n",
       " 'Apple Golden 1/r_92_100.jpg',\n",
       " 'Apple Golden 1/r_93_100.jpg',\n",
       " 'Apple Golden 1/r_94_100.jpg',\n",
       " 'Apple Golden 1/r_95_100.jpg',\n",
       " 'Apple Golden 1/r_96_100.jpg',\n",
       " 'Apple Golden 1/r_97_100.jpg',\n",
       " 'Apple Golden 1/r_98_100.jpg',\n",
       " 'Apple Golden 1/r_99_100.jpg',\n",
       " 'Apple Golden 1/r_9_100.jpg',\n",
       " 'Apple Golden 2/321_100.jpg',\n",
       " 'Apple Golden 2/322_100.jpg',\n",
       " 'Apple Golden 2/323_100.jpg',\n",
       " 'Apple Golden 2/324_100.jpg',\n",
       " 'Apple Golden 2/325_100.jpg',\n",
       " 'Apple Golden 2/326_100.jpg',\n",
       " 'Apple Golden 2/327_100.jpg',\n",
       " 'Apple Golden 2/32_100.jpg',\n",
       " 'Apple Golden 2/33_100.jpg',\n",
       " 'Apple Golden 2/34_100.jpg',\n",
       " 'Apple Golden 2/35_100.jpg',\n",
       " 'Apple Golden 2/36_100.jpg',\n",
       " 'Apple Golden 2/37_100.jpg',\n",
       " 'Apple Golden 2/38_100.jpg',\n",
       " 'Apple Golden 2/39_100.jpg',\n",
       " 'Apple Golden 2/3_100.jpg',\n",
       " 'Apple Golden 2/40_100.jpg',\n",
       " 'Apple Golden 2/41_100.jpg',\n",
       " 'Apple Golden 2/42_100.jpg',\n",
       " 'Apple Golden 2/43_100.jpg',\n",
       " 'Apple Golden 2/44_100.jpg',\n",
       " 'Apple Golden 2/45_100.jpg',\n",
       " 'Apple Golden 2/46_100.jpg',\n",
       " 'Apple Golden 2/47_100.jpg',\n",
       " 'Apple Golden 2/48_100.jpg',\n",
       " 'Apple Golden 2/49_100.jpg',\n",
       " 'Apple Golden 2/4_100.jpg',\n",
       " 'Apple Golden 2/50_100.jpg',\n",
       " 'Apple Golden 2/51_100.jpg',\n",
       " 'Apple Golden 2/52_100.jpg',\n",
       " 'Apple Golden 2/53_100.jpg',\n",
       " 'Apple Golden 2/54_100.jpg',\n",
       " 'Apple Golden 2/55_100.jpg',\n",
       " 'Apple Golden 2/56_100.jpg',\n",
       " 'Apple Golden 2/57_100.jpg',\n",
       " 'Apple Golden 2/58_100.jpg',\n",
       " 'Apple Golden 2/59_100.jpg',\n",
       " 'Apple Golden 2/5_100.jpg',\n",
       " 'Apple Golden 2/60_100.jpg',\n",
       " 'Apple Golden 2/61_100.jpg',\n",
       " 'Apple Golden 2/62_100.jpg',\n",
       " 'Apple Golden 2/63_100.jpg',\n",
       " 'Apple Golden 2/64_100.jpg',\n",
       " 'Apple Golden 2/65_100.jpg',\n",
       " 'Apple Golden 2/66_100.jpg',\n",
       " 'Apple Golden 2/67_100.jpg',\n",
       " 'Apple Golden 2/68_100.jpg',\n",
       " 'Apple Golden 2/69_100.jpg',\n",
       " 'Apple Golden 2/6_100.jpg',\n",
       " 'Apple Golden 2/70_100.jpg',\n",
       " 'Apple Golden 2/71_100.jpg',\n",
       " 'Apple Golden 2/72_100.jpg',\n",
       " 'Apple Golden 2/73_100.jpg',\n",
       " 'Apple Golden 2/74_100.jpg',\n",
       " 'Apple Golden 2/75_100.jpg',\n",
       " 'Apple Golden 2/76_100.jpg',\n",
       " 'Apple Golden 2/77_100.jpg',\n",
       " 'Apple Golden 2/78_100.jpg',\n",
       " 'Apple Golden 2/79_100.jpg',\n",
       " 'Apple Golden 2/7_100.jpg',\n",
       " 'Apple Golden 2/80_100.jpg',\n",
       " 'Apple Golden 2/81_100.jpg',\n",
       " 'Apple Golden 2/82_100.jpg',\n",
       " 'Apple Golden 2/83_100.jpg',\n",
       " 'Apple Golden 2/84_100.jpg',\n",
       " 'Apple Golden 2/85_100.jpg',\n",
       " 'Apple Golden 2/86_100.jpg',\n",
       " 'Apple Golden 2/87_100.jpg',\n",
       " 'Apple Golden 2/88_100.jpg',\n",
       " 'Apple Golden 2/89_100.jpg',\n",
       " 'Apple Golden 2/8_100.jpg',\n",
       " 'Apple Golden 2/90_100.jpg',\n",
       " 'Apple Golden 2/91_100.jpg',\n",
       " 'Apple Golden 2/92_100.jpg',\n",
       " 'Apple Golden 2/93_100.jpg',\n",
       " 'Apple Golden 2/94_100.jpg',\n",
       " 'Apple Golden 2/95_100.jpg',\n",
       " 'Apple Golden 2/96_100.jpg',\n",
       " 'Apple Golden 2/97_100.jpg',\n",
       " 'Apple Golden 2/98_100.jpg',\n",
       " 'Apple Golden 2/99_100.jpg',\n",
       " 'Apple Golden 2/9_100.jpg',\n",
       " 'Apple Golden 2/r_321_100.jpg',\n",
       " 'Apple Golden 2/r_322_100.jpg',\n",
       " 'Apple Golden 2/r_323_100.jpg',\n",
       " 'Apple Golden 2/r_324_100.jpg',\n",
       " 'Apple Golden 2/r_325_100.jpg',\n",
       " 'Apple Golden 2/r_326_100.jpg',\n",
       " 'Apple Golden 2/r_327_100.jpg',\n",
       " 'Apple Golden 2/r_32_100.jpg',\n",
       " 'Apple Golden 2/r_33_100.jpg',\n",
       " 'Apple Golden 2/r_34_100.jpg',\n",
       " 'Apple Golden 2/r_35_100.jpg',\n",
       " 'Apple Golden 2/r_36_100.jpg',\n",
       " 'Apple Golden 2/r_37_100.jpg',\n",
       " 'Apple Golden 2/r_38_100.jpg',\n",
       " 'Apple Golden 2/r_39_100.jpg',\n",
       " 'Apple Golden 2/r_3_100.jpg',\n",
       " 'Apple Golden 2/r_40_100.jpg',\n",
       " 'Apple Golden 2/r_41_100.jpg',\n",
       " 'Apple Golden 2/r_42_100.jpg',\n",
       " 'Apple Golden 2/r_43_100.jpg',\n",
       " 'Apple Golden 2/r_44_100.jpg',\n",
       " 'Apple Golden 2/r_45_100.jpg',\n",
       " 'Apple Golden 2/r_46_100.jpg',\n",
       " 'Apple Golden 2/r_47_100.jpg',\n",
       " 'Apple Golden 2/r_48_100.jpg',\n",
       " 'Apple Golden 2/r_49_100.jpg',\n",
       " 'Apple Golden 2/r_4_100.jpg',\n",
       " 'Apple Golden 2/r_50_100.jpg',\n",
       " 'Apple Golden 2/r_51_100.jpg',\n",
       " 'Apple Golden 2/r_52_100.jpg',\n",
       " 'Apple Golden 2/r_53_100.jpg',\n",
       " 'Apple Golden 2/r_54_100.jpg',\n",
       " 'Apple Golden 2/r_55_100.jpg',\n",
       " 'Apple Golden 2/r_56_100.jpg',\n",
       " 'Apple Golden 2/r_57_100.jpg',\n",
       " 'Apple Golden 2/r_58_100.jpg',\n",
       " 'Apple Golden 2/r_59_100.jpg',\n",
       " 'Apple Golden 2/r_5_100.jpg',\n",
       " 'Apple Golden 2/r_60_100.jpg',\n",
       " 'Apple Golden 2/r_61_100.jpg',\n",
       " 'Apple Golden 2/r_62_100.jpg',\n",
       " 'Apple Golden 2/r_63_100.jpg',\n",
       " 'Apple Golden 2/r_64_100.jpg',\n",
       " 'Apple Golden 2/r_65_100.jpg',\n",
       " 'Apple Golden 2/r_66_100.jpg',\n",
       " 'Apple Golden 2/r_67_100.jpg',\n",
       " 'Apple Golden 2/r_68_100.jpg',\n",
       " 'Apple Golden 2/r_69_100.jpg',\n",
       " 'Apple Golden 2/r_6_100.jpg',\n",
       " 'Apple Golden 2/r_70_100.jpg',\n",
       " 'Apple Golden 2/r_71_100.jpg',\n",
       " 'Apple Golden 2/r_72_100.jpg',\n",
       " 'Apple Golden 2/r_73_100.jpg',\n",
       " 'Apple Golden 2/r_74_100.jpg',\n",
       " 'Apple Golden 2/r_75_100.jpg',\n",
       " 'Apple Golden 2/r_76_100.jpg',\n",
       " 'Apple Golden 2/r_77_100.jpg',\n",
       " 'Apple Golden 2/r_78_100.jpg',\n",
       " 'Apple Golden 2/r_79_100.jpg',\n",
       " 'Apple Golden 2/r_7_100.jpg',\n",
       " 'Apple Golden 2/r_80_100.jpg',\n",
       " 'Apple Golden 2/r_81_100.jpg',\n",
       " 'Apple Golden 2/r_82_100.jpg',\n",
       " 'Apple Golden 2/r_83_100.jpg',\n",
       " 'Apple Golden 2/r_84_100.jpg',\n",
       " 'Apple Golden 2/r_85_100.jpg',\n",
       " 'Apple Golden 2/r_86_100.jpg',\n",
       " 'Apple Golden 2/r_87_100.jpg',\n",
       " 'Apple Golden 2/r_88_100.jpg',\n",
       " 'Apple Golden 2/r_89_100.jpg',\n",
       " 'Apple Golden 2/r_8_100.jpg',\n",
       " 'Apple Golden 2/r_90_100.jpg',\n",
       " 'Apple Golden 2/r_91_100.jpg',\n",
       " 'Apple Golden 2/r_92_100.jpg',\n",
       " 'Apple Golden 2/r_93_100.jpg',\n",
       " 'Apple Golden 2/r_94_100.jpg',\n",
       " 'Apple Golden 2/r_95_100.jpg',\n",
       " 'Apple Golden 2/r_96_100.jpg',\n",
       " 'Apple Golden 2/r_97_100.jpg',\n",
       " 'Apple Golden 2/r_98_100.jpg',\n",
       " 'Apple Golden 2/r_99_100.jpg',\n",
       " 'Apple Golden 2/r_9_100.jpg',\n",
       " 'Apple Golden 3/311_100.jpg',\n",
       " 'Apple Golden 3/312_100.jpg',\n",
       " 'Apple Golden 3/313_100.jpg',\n",
       " 'Apple Golden 3/31_100.jpg',\n",
       " 'Apple Golden 3/32_100.jpg',\n",
       " 'Apple Golden 3/33_100.jpg',\n",
       " 'Apple Golden 3/34_100.jpg',\n",
       " 'Apple Golden 3/35_100.jpg',\n",
       " 'Apple Golden 3/36_100.jpg',\n",
       " 'Apple Golden 3/37_100.jpg',\n",
       " 'Apple Golden 3/38_100.jpg',\n",
       " 'Apple Golden 3/39_100.jpg',\n",
       " 'Apple Golden 3/3_100.jpg',\n",
       " 'Apple Golden 3/40_100.jpg',\n",
       " 'Apple Golden 3/41_100.jpg',\n",
       " 'Apple Golden 3/42_100.jpg',\n",
       " 'Apple Golden 3/43_100.jpg',\n",
       " 'Apple Golden 3/44_100.jpg',\n",
       " 'Apple Golden 3/45_100.jpg',\n",
       " 'Apple Golden 3/46_100.jpg',\n",
       " 'Apple Golden 3/47_100.jpg',\n",
       " 'Apple Golden 3/48_100.jpg',\n",
       " 'Apple Golden 3/49_100.jpg',\n",
       " 'Apple Golden 3/4_100.jpg',\n",
       " 'Apple Golden 3/50_100.jpg',\n",
       " 'Apple Golden 3/51_100.jpg',\n",
       " 'Apple Golden 3/52_100.jpg',\n",
       " 'Apple Golden 3/53_100.jpg',\n",
       " 'Apple Golden 3/54_100.jpg',\n",
       " 'Apple Golden 3/55_100.jpg',\n",
       " 'Apple Golden 3/56_100.jpg',\n",
       " 'Apple Golden 3/57_100.jpg',\n",
       " 'Apple Golden 3/58_100.jpg',\n",
       " 'Apple Golden 3/59_100.jpg',\n",
       " 'Apple Golden 3/5_100.jpg',\n",
       " 'Apple Golden 3/60_100.jpg',\n",
       " 'Apple Golden 3/61_100.jpg',\n",
       " 'Apple Golden 3/62_100.jpg',\n",
       " 'Apple Golden 3/63_100.jpg',\n",
       " 'Apple Golden 3/64_100.jpg',\n",
       " 'Apple Golden 3/65_100.jpg',\n",
       " 'Apple Golden 3/66_100.jpg',\n",
       " 'Apple Golden 3/67_100.jpg',\n",
       " 'Apple Golden 3/68_100.jpg',\n",
       " 'Apple Golden 3/69_100.jpg',\n",
       " 'Apple Golden 3/6_100.jpg',\n",
       " 'Apple Golden 3/70_100.jpg',\n",
       " 'Apple Golden 3/71_100.jpg',\n",
       " 'Apple Golden 3/72_100.jpg',\n",
       " 'Apple Golden 3/73_100.jpg',\n",
       " 'Apple Golden 3/74_100.jpg',\n",
       " 'Apple Golden 3/75_100.jpg',\n",
       " 'Apple Golden 3/76_100.jpg',\n",
       " 'Apple Golden 3/77_100.jpg',\n",
       " 'Apple Golden 3/78_100.jpg',\n",
       " 'Apple Golden 3/79_100.jpg',\n",
       " 'Apple Golden 3/7_100.jpg',\n",
       " 'Apple Golden 3/80_100.jpg',\n",
       " 'Apple Golden 3/81_100.jpg',\n",
       " 'Apple Golden 3/82_100.jpg',\n",
       " 'Apple Golden 3/83_100.jpg',\n",
       " 'Apple Golden 3/84_100.jpg',\n",
       " 'Apple Golden 3/85_100.jpg',\n",
       " 'Apple Golden 3/86_100.jpg',\n",
       " 'Apple Golden 3/87_100.jpg',\n",
       " 'Apple Golden 3/88_100.jpg',\n",
       " 'Apple Golden 3/89_100.jpg',\n",
       " 'Apple Golden 3/8_100.jpg',\n",
       " 'Apple Golden 3/90_100.jpg',\n",
       " 'Apple Golden 3/91_100.jpg',\n",
       " 'Apple Golden 3/92_100.jpg',\n",
       " 'Apple Golden 3/93_100.jpg',\n",
       " 'Apple Golden 3/94_100.jpg',\n",
       " 'Apple Golden 3/95_100.jpg',\n",
       " 'Apple Golden 3/96_100.jpg',\n",
       " 'Apple Golden 3/97_100.jpg',\n",
       " 'Apple Golden 3/98_100.jpg',\n",
       " 'Apple Golden 3/99_100.jpg',\n",
       " 'Apple Golden 3/9_100.jpg',\n",
       " 'Apple Golden 3/r_321_100.jpg',\n",
       " 'Apple Golden 3/r_322_100.jpg',\n",
       " 'Apple Golden 3/r_323_100.jpg',\n",
       " 'Apple Golden 3/r_324_100.jpg',\n",
       " 'Apple Golden 3/r_325_100.jpg',\n",
       " 'Apple Golden 3/r_326_100.jpg',\n",
       " 'Apple Golden 3/r_327_100.jpg',\n",
       " 'Apple Golden 3/r_32_100.jpg',\n",
       " 'Apple Golden 3/r_33_100.jpg',\n",
       " 'Apple Golden 3/r_34_100.jpg',\n",
       " 'Apple Golden 3/r_35_100.jpg',\n",
       " 'Apple Golden 3/r_36_100.jpg',\n",
       " 'Apple Golden 3/r_37_100.jpg',\n",
       " 'Apple Golden 3/r_38_100.jpg',\n",
       " 'Apple Golden 3/r_39_100.jpg',\n",
       " 'Apple Golden 3/r_3_100.jpg',\n",
       " 'Apple Golden 3/r_40_100.jpg',\n",
       " 'Apple Golden 3/r_41_100.jpg',\n",
       " 'Apple Golden 3/r_42_100.jpg',\n",
       " 'Apple Golden 3/r_43_100.jpg',\n",
       " 'Apple Golden 3/r_44_100.jpg',\n",
       " 'Apple Golden 3/r_45_100.jpg',\n",
       " 'Apple Golden 3/r_46_100.jpg',\n",
       " 'Apple Golden 3/r_47_100.jpg',\n",
       " 'Apple Golden 3/r_48_100.jpg',\n",
       " 'Apple Golden 3/r_49_100.jpg',\n",
       " 'Apple Golden 3/r_4_100.jpg',\n",
       " 'Apple Golden 3/r_50_100.jpg',\n",
       " 'Apple Golden 3/r_51_100.jpg',\n",
       " 'Apple Golden 3/r_52_100.jpg',\n",
       " 'Apple Golden 3/r_53_100.jpg',\n",
       " 'Apple Golden 3/r_54_100.jpg',\n",
       " 'Apple Golden 3/r_55_100.jpg',\n",
       " 'Apple Golden 3/r_56_100.jpg',\n",
       " 'Apple Golden 3/r_57_100.jpg',\n",
       " 'Apple Golden 3/r_58_100.jpg',\n",
       " 'Apple Golden 3/r_59_100.jpg',\n",
       " 'Apple Golden 3/r_5_100.jpg',\n",
       " 'Apple Golden 3/r_60_100.jpg',\n",
       " 'Apple Golden 3/r_61_100.jpg',\n",
       " 'Apple Golden 3/r_62_100.jpg',\n",
       " 'Apple Golden 3/r_63_100.jpg',\n",
       " 'Apple Golden 3/r_64_100.jpg',\n",
       " 'Apple Golden 3/r_65_100.jpg',\n",
       " 'Apple Golden 3/r_66_100.jpg',\n",
       " 'Apple Golden 3/r_67_100.jpg',\n",
       " 'Apple Golden 3/r_68_100.jpg',\n",
       " 'Apple Golden 3/r_69_100.jpg',\n",
       " 'Apple Golden 3/r_6_100.jpg',\n",
       " 'Apple Golden 3/r_70_100.jpg',\n",
       " 'Apple Golden 3/r_71_100.jpg',\n",
       " 'Apple Golden 3/r_72_100.jpg',\n",
       " 'Apple Golden 3/r_73_100.jpg',\n",
       " 'Apple Golden 3/r_74_100.jpg',\n",
       " 'Apple Golden 3/r_75_100.jpg',\n",
       " 'Apple Golden 3/r_76_100.jpg',\n",
       " 'Apple Golden 3/r_77_100.jpg',\n",
       " 'Apple Golden 3/r_78_100.jpg',\n",
       " 'Apple Golden 3/r_79_100.jpg',\n",
       " 'Apple Golden 3/r_7_100.jpg',\n",
       " 'Apple Golden 3/r_80_100.jpg',\n",
       " 'Apple Golden 3/r_81_100.jpg',\n",
       " 'Apple Golden 3/r_82_100.jpg',\n",
       " 'Apple Golden 3/r_83_100.jpg',\n",
       " 'Apple Golden 3/r_84_100.jpg',\n",
       " 'Apple Golden 3/r_85_100.jpg',\n",
       " 'Apple Golden 3/r_86_100.jpg',\n",
       " 'Apple Golden 3/r_87_100.jpg',\n",
       " 'Apple Golden 3/r_88_100.jpg',\n",
       " 'Apple Golden 3/r_89_100.jpg',\n",
       " 'Apple Golden 3/r_8_100.jpg',\n",
       " 'Apple Golden 3/r_90_100.jpg',\n",
       " 'Apple Golden 3/r_91_100.jpg',\n",
       " 'Apple Golden 3/r_92_100.jpg',\n",
       " 'Apple Golden 3/r_93_100.jpg',\n",
       " 'Apple Golden 3/r_94_100.jpg',\n",
       " 'Apple Golden 3/r_95_100.jpg',\n",
       " 'Apple Golden 3/r_96_100.jpg',\n",
       " 'Apple Golden 3/r_97_100.jpg',\n",
       " 'Apple Golden 3/r_98_100.jpg',\n",
       " 'Apple Golden 3/r_99_100.jpg',\n",
       " 'Apple Golden 3/r_9_100.jpg',\n",
       " 'Apple Granny Smith/321_100.jpg',\n",
       " 'Apple Granny Smith/322_100.jpg',\n",
       " 'Apple Granny Smith/323_100.jpg',\n",
       " 'Apple Granny Smith/324_100.jpg',\n",
       " 'Apple Granny Smith/325_100.jpg',\n",
       " 'Apple Granny Smith/326_100.jpg',\n",
       " 'Apple Granny Smith/327_100.jpg',\n",
       " 'Apple Granny Smith/32_100.jpg',\n",
       " 'Apple Granny Smith/33_100.jpg',\n",
       " 'Apple Granny Smith/34_100.jpg',\n",
       " 'Apple Granny Smith/35_100.jpg',\n",
       " 'Apple Granny Smith/36_100.jpg',\n",
       " 'Apple Granny Smith/37_100.jpg',\n",
       " 'Apple Granny Smith/38_100.jpg',\n",
       " 'Apple Granny Smith/39_100.jpg',\n",
       " 'Apple Granny Smith/3_100.jpg',\n",
       " 'Apple Granny Smith/40_100.jpg',\n",
       " 'Apple Granny Smith/41_100.jpg',\n",
       " 'Apple Granny Smith/42_100.jpg',\n",
       " 'Apple Granny Smith/43_100.jpg',\n",
       " 'Apple Granny Smith/44_100.jpg',\n",
       " 'Apple Granny Smith/45_100.jpg',\n",
       " 'Apple Granny Smith/46_100.jpg',\n",
       " 'Apple Granny Smith/47_100.jpg',\n",
       " 'Apple Granny Smith/48_100.jpg',\n",
       " 'Apple Granny Smith/49_100.jpg',\n",
       " 'Apple Granny Smith/4_100.jpg',\n",
       " 'Apple Granny Smith/50_100.jpg',\n",
       " 'Apple Granny Smith/51_100.jpg',\n",
       " 'Apple Granny Smith/52_100.jpg',\n",
       " 'Apple Granny Smith/53_100.jpg',\n",
       " 'Apple Granny Smith/54_100.jpg',\n",
       " 'Apple Granny Smith/55_100.jpg',\n",
       " 'Apple Granny Smith/56_100.jpg',\n",
       " 'Apple Granny Smith/57_100.jpg',\n",
       " 'Apple Granny Smith/58_100.jpg',\n",
       " 'Apple Granny Smith/59_100.jpg',\n",
       " 'Apple Granny Smith/5_100.jpg',\n",
       " 'Apple Granny Smith/60_100.jpg',\n",
       " 'Apple Granny Smith/61_100.jpg',\n",
       " 'Apple Granny Smith/62_100.jpg',\n",
       " 'Apple Granny Smith/63_100.jpg',\n",
       " 'Apple Granny Smith/64_100.jpg',\n",
       " 'Apple Granny Smith/65_100.jpg',\n",
       " 'Apple Granny Smith/66_100.jpg',\n",
       " 'Apple Granny Smith/67_100.jpg',\n",
       " 'Apple Granny Smith/68_100.jpg',\n",
       " 'Apple Granny Smith/69_100.jpg',\n",
       " 'Apple Granny Smith/6_100.jpg',\n",
       " 'Apple Granny Smith/70_100.jpg',\n",
       " 'Apple Granny Smith/71_100.jpg',\n",
       " 'Apple Granny Smith/72_100.jpg',\n",
       " 'Apple Granny Smith/73_100.jpg',\n",
       " 'Apple Granny Smith/74_100.jpg',\n",
       " 'Apple Granny Smith/75_100.jpg',\n",
       " 'Apple Granny Smith/76_100.jpg',\n",
       " 'Apple Granny Smith/77_100.jpg',\n",
       " 'Apple Granny Smith/78_100.jpg',\n",
       " 'Apple Granny Smith/79_100.jpg',\n",
       " 'Apple Granny Smith/7_100.jpg',\n",
       " 'Apple Granny Smith/80_100.jpg',\n",
       " 'Apple Granny Smith/81_100.jpg',\n",
       " 'Apple Granny Smith/82_100.jpg',\n",
       " 'Apple Granny Smith/83_100.jpg',\n",
       " 'Apple Granny Smith/84_100.jpg',\n",
       " 'Apple Granny Smith/85_100.jpg',\n",
       " 'Apple Granny Smith/86_100.jpg',\n",
       " 'Apple Granny Smith/87_100.jpg',\n",
       " 'Apple Granny Smith/88_100.jpg',\n",
       " 'Apple Granny Smith/89_100.jpg',\n",
       " 'Apple Granny Smith/8_100.jpg',\n",
       " 'Apple Granny Smith/90_100.jpg',\n",
       " 'Apple Granny Smith/91_100.jpg',\n",
       " 'Apple Granny Smith/92_100.jpg',\n",
       " 'Apple Granny Smith/93_100.jpg',\n",
       " 'Apple Granny Smith/94_100.jpg',\n",
       " 'Apple Granny Smith/95_100.jpg',\n",
       " 'Apple Granny Smith/96_100.jpg',\n",
       " 'Apple Granny Smith/97_100.jpg',\n",
       " 'Apple Granny Smith/98_100.jpg',\n",
       " 'Apple Granny Smith/99_100.jpg',\n",
       " 'Apple Granny Smith/9_100.jpg',\n",
       " 'Apple Granny Smith/r_321_100.jpg',\n",
       " 'Apple Granny Smith/r_322_100.jpg',\n",
       " 'Apple Granny Smith/r_323_100.jpg',\n",
       " 'Apple Granny Smith/r_324_100.jpg',\n",
       " 'Apple Granny Smith/r_325_100.jpg',\n",
       " 'Apple Granny Smith/r_326_100.jpg',\n",
       " 'Apple Granny Smith/r_327_100.jpg',\n",
       " 'Apple Granny Smith/r_32_100.jpg',\n",
       " 'Apple Granny Smith/r_33_100.jpg',\n",
       " 'Apple Granny Smith/r_34_100.jpg',\n",
       " 'Apple Granny Smith/r_35_100.jpg',\n",
       " 'Apple Granny Smith/r_36_100.jpg',\n",
       " 'Apple Granny Smith/r_37_100.jpg',\n",
       " 'Apple Granny Smith/r_38_100.jpg',\n",
       " 'Apple Granny Smith/r_39_100.jpg',\n",
       " 'Apple Granny Smith/r_3_100.jpg',\n",
       " 'Apple Granny Smith/r_40_100.jpg',\n",
       " 'Apple Granny Smith/r_41_100.jpg',\n",
       " 'Apple Granny Smith/r_42_100.jpg',\n",
       " 'Apple Granny Smith/r_43_100.jpg',\n",
       " 'Apple Granny Smith/r_44_100.jpg',\n",
       " 'Apple Granny Smith/r_45_100.jpg',\n",
       " 'Apple Granny Smith/r_46_100.jpg',\n",
       " 'Apple Granny Smith/r_47_100.jpg',\n",
       " 'Apple Granny Smith/r_48_100.jpg',\n",
       " 'Apple Granny Smith/r_49_100.jpg',\n",
       " 'Apple Granny Smith/r_4_100.jpg',\n",
       " 'Apple Granny Smith/r_50_100.jpg',\n",
       " 'Apple Granny Smith/r_51_100.jpg',\n",
       " 'Apple Granny Smith/r_52_100.jpg',\n",
       " 'Apple Granny Smith/r_53_100.jpg',\n",
       " 'Apple Granny Smith/r_54_100.jpg',\n",
       " 'Apple Granny Smith/r_55_100.jpg',\n",
       " 'Apple Granny Smith/r_56_100.jpg',\n",
       " 'Apple Granny Smith/r_57_100.jpg',\n",
       " 'Apple Granny Smith/r_58_100.jpg',\n",
       " 'Apple Granny Smith/r_59_100.jpg',\n",
       " 'Apple Granny Smith/r_5_100.jpg',\n",
       " 'Apple Granny Smith/r_60_100.jpg',\n",
       " 'Apple Granny Smith/r_61_100.jpg',\n",
       " 'Apple Granny Smith/r_62_100.jpg',\n",
       " 'Apple Granny Smith/r_63_100.jpg',\n",
       " 'Apple Granny Smith/r_64_100.jpg',\n",
       " 'Apple Granny Smith/r_65_100.jpg',\n",
       " 'Apple Granny Smith/r_66_100.jpg',\n",
       " 'Apple Granny Smith/r_67_100.jpg',\n",
       " 'Apple Granny Smith/r_68_100.jpg',\n",
       " 'Apple Granny Smith/r_69_100.jpg',\n",
       " 'Apple Granny Smith/r_6_100.jpg',\n",
       " 'Apple Granny Smith/r_70_100.jpg',\n",
       " 'Apple Granny Smith/r_71_100.jpg',\n",
       " 'Apple Granny Smith/r_72_100.jpg',\n",
       " 'Apple Granny Smith/r_73_100.jpg',\n",
       " 'Apple Granny Smith/r_74_100.jpg',\n",
       " 'Apple Granny Smith/r_75_100.jpg',\n",
       " 'Apple Granny Smith/r_76_100.jpg',\n",
       " 'Apple Granny Smith/r_77_100.jpg',\n",
       " 'Apple Granny Smith/r_78_100.jpg',\n",
       " 'Apple Granny Smith/r_79_100.jpg',\n",
       " 'Apple Granny Smith/r_7_100.jpg',\n",
       " 'Apple Granny Smith/r_80_100.jpg',\n",
       " 'Apple Granny Smith/r_81_100.jpg',\n",
       " 'Apple Granny Smith/r_82_100.jpg',\n",
       " 'Apple Granny Smith/r_83_100.jpg',\n",
       " 'Apple Granny Smith/r_84_100.jpg',\n",
       " 'Apple Granny Smith/r_85_100.jpg',\n",
       " 'Apple Granny Smith/r_86_100.jpg',\n",
       " 'Apple Granny Smith/r_87_100.jpg',\n",
       " 'Apple Granny Smith/r_88_100.jpg',\n",
       " 'Apple Granny Smith/r_89_100.jpg',\n",
       " 'Apple Granny Smith/r_8_100.jpg',\n",
       " 'Apple Granny Smith/r_90_100.jpg',\n",
       " 'Apple Granny Smith/r_91_100.jpg',\n",
       " 'Apple Granny Smith/r_92_100.jpg',\n",
       " 'Apple Granny Smith/r_93_100.jpg',\n",
       " 'Apple Granny Smith/r_94_100.jpg',\n",
       " 'Apple Granny Smith/r_95_100.jpg',\n",
       " 'Apple Granny Smith/r_96_100.jpg',\n",
       " 'Apple Granny Smith/r_97_100.jpg',\n",
       " 'Apple Granny Smith/r_98_100.jpg',\n",
       " 'Apple Granny Smith/r_99_100.jpg',\n",
       " 'Apple Granny Smith/r_9_100.jpg',\n",
       " 'Apple Pink Lady/219_100.jpg',\n",
       " 'Apple Pink Lady/220_100.jpg',\n",
       " 'Apple Pink Lady/225_100.jpg',\n",
       " 'Apple Pink Lady/226_100.jpg',\n",
       " 'Apple Pink Lady/228_100.jpg',\n",
       " 'Apple Pink Lady/229_100.jpg',\n",
       " 'Apple Pink Lady/230_100.jpg',\n",
       " 'Apple Pink Lady/232_100.jpg',\n",
       " 'Apple Pink Lady/234_100.jpg',\n",
       " 'Apple Pink Lady/236_100.jpg',\n",
       " 'Apple Pink Lady/237_100.jpg',\n",
       " 'Apple Pink Lady/238_100.jpg',\n",
       " 'Apple Pink Lady/239_100.jpg',\n",
       " 'Apple Pink Lady/240_100.jpg',\n",
       " 'Apple Pink Lady/241_100.jpg',\n",
       " 'Apple Pink Lady/242_100.jpg',\n",
       " 'Apple Pink Lady/243_100.jpg',\n",
       " 'Apple Pink Lady/244_100.jpg',\n",
       " 'Apple Pink Lady/245_100.jpg',\n",
       " 'Apple Pink Lady/246_100.jpg',\n",
       " 'Apple Pink Lady/247_100.jpg',\n",
       " 'Apple Pink Lady/248_100.jpg',\n",
       " 'Apple Pink Lady/249_100.jpg',\n",
       " 'Apple Pink Lady/250_100.jpg',\n",
       " 'Apple Pink Lady/251_100.jpg',\n",
       " 'Apple Pink Lady/252_100.jpg',\n",
       " 'Apple Pink Lady/253_100.jpg',\n",
       " 'Apple Pink Lady/254_100.jpg',\n",
       " 'Apple Pink Lady/255_100.jpg',\n",
       " 'Apple Pink Lady/256_100.jpg',\n",
       " 'Apple Pink Lady/257_100.jpg',\n",
       " 'Apple Pink Lady/258_100.jpg',\n",
       " 'Apple Pink Lady/259_100.jpg',\n",
       " 'Apple Pink Lady/260_100.jpg',\n",
       " 'Apple Pink Lady/261_100.jpg',\n",
       " 'Apple Pink Lady/262_100.jpg',\n",
       " 'Apple Pink Lady/263_100.jpg',\n",
       " 'Apple Pink Lady/264_100.jpg',\n",
       " 'Apple Pink Lady/265_100.jpg',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pepper.env import get_project_dir\n",
    "from pepper.utils import _get_filenames_glob\n",
    "import os\n",
    "#C:\\Users\\franc\\Projects\\pepper_cloud_based_model\\dataset\\fruits-360_dataset\\Test\n",
    "raw_src_im_dir = os.path.join(get_project_dir(), r\"dataset\\fruits-360_dataset\\Test\")\n",
    "display(raw_src_im_dir)\n",
    "filenames = _get_filenames_glob(raw_src_im_dir, recursive=True)\n",
    "display(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22688\n"
     ]
    }
   ],
   "source": [
    "print(len(filenames))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚧 Sampling des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pepper.utils import create_if_not_exist\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def sample_images(source_dir: str, target_dir: str, n_samples: int):\n",
    "    # Récupérer la liste des sous-dossiers\n",
    "    subdirs = [\n",
    "        subdir for subdir in os.listdir(source_dir)\n",
    "        if os.path.isdir(os.path.join(source_dir, subdir))\n",
    "    ]\n",
    "\n",
    "    n_images_per_folder, remainder = divmod(n_samples, len(subdirs))\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(source_dir, subdir)\n",
    "\n",
    "        # Récupérer la liste des images dans le dossier\n",
    "        images = [\n",
    "            image for image in os.listdir(subdir_path)\n",
    "            if os.path.isfile(os.path.join(subdir_path, image))\n",
    "        ]\n",
    "\n",
    "        # Sélectionner les images aléatoirement\n",
    "        n_images = n_images_per_folder + (remainder > 0) \n",
    "        if len(images) <= n_images:\n",
    "            selected_images = images\n",
    "        else:\n",
    "            selected_images = random.sample(images, n_images)\n",
    "        \n",
    "        if n_images == len(selected_images):\n",
    "            remainder -= 1\n",
    "        else:\n",
    "            remainder += n_images - len(selected_images)\n",
    "\n",
    "        if len(selected_images) > 0:\n",
    "            create_if_not_exist(os.path.join(target_dir, subdir))\n",
    "        \n",
    "        # Copier les images sélectionnées vers le dossier cible\n",
    "        for image in selected_images:\n",
    "            source_path = os.path.join(subdir_path, image)\n",
    "            target_path = os.path.join(target_dir, subdir, image)\n",
    "            shutil.copyfile(source_path, target_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pepper.env import get_project_dir\n",
    "from pepper.utils import create_if_not_exist\n",
    "from fruits.storage_utils import sample_images\n",
    "project_dir = get_project_dir()\n",
    "raw_src_im_dir = os.path.join(project_dir, r\"dataset\\fruits-360_dataset\\Test\")\n",
    "sample_300_im_dir = os.path.join(project_dir, r\"data\\im\\sample_300\")\n",
    "create_if_not_exist(sample_300_im_dir)\n",
    "n_images_per_folder = sample_images(raw_src_im_dir, sample_300_im_dir, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = sum(n_images for subdir, n_images in n_images_per_folder.items())\n",
    "display(n_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚧 Local vers S3, S3 vers S3\n",
    "\n",
    "Compartiment `s3://pepper-labs-fruits`\n",
    "\n",
    "On va d'abord effectuer une copie de la sélection depuis les dossiers locaux directement vers un compartiment S3.\n",
    "\n",
    "Puis, par challenge, le faire directement entre le dépôt S3 en ligne des images et notre dépôt.\n",
    "\n",
    "Enfin, il sera intéressant de ne même pas effectuer cette copie, mais d'aller prélever directement notre échantillon à la source (EMR utilisant le S3 officiel des images Fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.26.133-py3-none-any.whl (135 kB)\n",
      "                                              0.0/135.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 135.6/135.6 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting botocore<1.30.0,>=1.29.133 (from boto3)\n",
      "  Downloading botocore-1.29.133-py3-none-any.whl (10.7 MB)\n",
      "                                              0.0/10.7 MB ? eta -:--:--\n",
      "     ---                                      1.1/10.7 MB 34.0 MB/s eta 0:00:01\n",
      "     ----------                               2.7/10.7 MB 34.9 MB/s eta 0:00:01\n",
      "     ----------------                         4.5/10.7 MB 35.6 MB/s eta 0:00:01\n",
      "     -----------------------                  6.4/10.7 MB 37.2 MB/s eta 0:00:01\n",
      "     --------------------------------         8.7/10.7 MB 39.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  10.7/10.7 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 10.7/10.7 MB 36.3 MB/s eta 0:00:00\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3)\n",
      "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "                                              0.0/79.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 79.8/79.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\franc\\appdata\\roaming\\python\\python311\\site-packages (from botocore<1.30.0,>=1.29.133->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\franc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from botocore<1.30.0,>=1.29.133->boto3) (1.26.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\franc\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.133->boto3) (1.16.0)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.26.133 botocore-1.29.133 jmespath-1.0.1 s3transfer-0.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pepper-bucket\n",
      "pepper-labs-fruits\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource(\"s3\")\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old version de l'alternant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH:        c:\\Users\\franc\\Projects\\pepper_cloud_based_model\\notebooks\n",
      "PATH_Data:   c:\\Users\\franc\\Projects\\pepper_cloud_based_model\\notebooks/data/Test1\n",
      "PATH_Result_Local: c:\\Users\\franc\\Projects\\pepper_cloud_based_model\\notebooks/data/Results_Local\n"
     ]
    }
   ],
   "source": [
    "PATH = os.getcwd()\n",
    "PATH_Data = PATH+'/data/Test1'\n",
    "PATH_Result = PATH+'/data/Results_Local'\n",
    "print('PATH:        '+\\\n",
    "      PATH+'\\nPATH_Data:   '+\\\n",
    "      PATH_Data+'\\nPATH_Result_Local: '+PATH_Result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Création de la SparkSession\n",
    "\n",
    "L’application Spark est contrôlée grâce à un processus de pilotage (driver process) appelé **SparkSession**.\n",
    "\n",
    "<u>Une instance de **SparkSession** est la façon dont Spark exécute les fonctions définies par l’utilisateur dans l’ensemble du cluster</u>. <u>Une SparkSession correspond toujours à une application Spark</u>.\n",
    "\n",
    "<u>Ici nous créons une session spark en spécifiant dans l'ordre</u> :\n",
    "\n",
    "1. un **nom pour l'application**, qui sera affichée dans l'interface utilisateur Web Spark \"**P8**\"\n",
    "2. que l'application doit s'exécuter **localement**.\n",
    "    * Nous ne définissons pas le nombre de cœurs à utiliser (comme `.master('local[4])` pour 4 cœurs à utiliser),\n",
    "    * nous utiliserons donc tous les cœurs disponibles dans notre processeur.\n",
    "3. une option de configuration supplémentaire permettant d'utiliser le **format \"parquet\"** que nous utiliserons pour enregistrer et charger le résultat de notre travail.\n",
    "4. vouloir **obtenir une session spark** existante ou si aucune n'existe, en créer une nouvelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Fruits\")\n",
    "    .master(\"local\")\n",
    "    .config(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.app.submitTime', '1684166786102')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.sql.parquet.writeLegacyFormat', 'true')\n",
      "('spark.app.name', 'Fruits')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.sql.caseSensitive', 'True')\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.sql.caseSensitive\", \"True\")\n",
    "\n",
    "# Obtention des paramètres de configuration\n",
    "configurations = conf.getAll()\n",
    "\n",
    "# Affichage des paramètres de configuration\n",
    "for config in configurations:\n",
    "    print(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous affectons à la variable **`sc`** le **`SparkContext`** attaché à l'objet **`spark`** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Affichage des informations de Spark en cours d'execution</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Fruits</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2717f545810>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Traitement des données\n",
    "\n",
    "<u>Dans la suite de notre flux de travail, nous allons successivement</u> :\n",
    "1. Préparer nos données\n",
    "    1. Importer les images dans un dataframe **pandas UDF**\n",
    "    2. Associer aux images leur **label**\n",
    "    3. Préprocesser en **redimensionnant nos images pour qu'elles soient compatibles avec notre modèle**\n",
    "2. Préparer notre modèle\n",
    "    1. Importer le modèle **MobileNetV2**\n",
    "    2. Créer un **nouveau modèle** dépourvu de la dernière couche de MobileNetV2\n",
    "3. Définir le processus de chargement des images et l'application de leur featurisation à travers l'utilisation de pandas UDF\n",
    "3. Exécuter les actions d'extraction de features\n",
    "4. Enregistrer le résultat de nos actions\n",
    "5. Tester le bon fonctionnement en chargeant les données enregistrées"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.1 Chargement des données\n",
    "\n",
    "Les images sont chargées au format binaire pour en faciliter le pré-traitement.\n",
    "\n",
    "\n",
    "\n",
    "Avant de charger les images, nous spécifions que nous voulons charger uniquement les fichiers dont l'extension est **jpg**.\n",
    "\n",
    "Nous indiquons également de charger tous les objets possibles contenus dans les sous-dossiers du dossier communiqué.\n",
    "\n",
    "Code d'origine :\n",
    "```python\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Affichage des 5 premières images contenant</u> :\n",
    " - le path de l'image\n",
    " - la date et heure de sa dernière modification\n",
    " - sa longueur\n",
    " - son contenu encodé en valeur hexadécimal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Je ne conserve que le **path** de l'image et j'ajoute une colonne contenant les **labels** de chaque image</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n",
      "+---------------------------------------------------------------------------------------------------------------+----------+\n",
      "|path                                                                                                           |label     |\n",
      "+---------------------------------------------------------------------------------------------------------------+----------+\n",
      "|file:/C:/Users/franc/Projects/pepper_cloud_based_model/dataset/fruits-360_dataset/Test/Watermelon/r_106_100.jpg|Watermelon|\n",
      "|file:/C:/Users/franc/Projects/pepper_cloud_based_model/dataset/fruits-360_dataset/Test/Watermelon/r_109_100.jpg|Watermelon|\n",
      "|file:/C:/Users/franc/Projects/pepper_cloud_based_model/dataset/fruits-360_dataset/Test/Watermelon/r_108_100.jpg|Watermelon|\n",
      "|file:/C:/Users/franc/Projects/pepper_cloud_based_model/dataset/fruits-360_dataset/Test/Watermelon/r_107_100.jpg|Watermelon|\n",
      "|file:/C:/Users/franc/Projects/pepper_cloud_based_model/dataset/fruits-360_dataset/Test/Watermelon/r_95_100.jpg |Watermelon|\n",
      "+---------------------------------------------------------------------------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fruits.driver import init_spark_session, load_images\n",
    "spark = init_spark_session()\n",
    "images = load_images(spark)\n",
    "images.printSchema()\n",
    "images.select(\"path\", \"label\").show(5, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.2 Préparation du modèle\n",
    "\n",
    "Je vais utiliser la technique du **transfert learning** pour extraire les features des images.\n",
    "\n",
    "J'ai choisi d'utiliser le modèle **MobileNetV2** pour sa rapidité d'exécution comparée à d'autres modèles comme *VGG16* par exemple.\n",
    "\n",
    "Pour en savoir plus sur la conception et le fonctionnement de MobileNetV2, je vous invite à lire [cet article](https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c).\n",
    "\n",
    "<u>Voici le schéma de son architecture globale</u> : \n",
    "\n",
    "![Architecture de MobileNetV2](../baseline/img/mobilenetv2_architecture.png)\n",
    "\n",
    "Il existe une dernière couche qui sert à classer les images selon 1000 catégories que nous ne voulons pas utiliser.\n",
    "\n",
    "L'idée dans ce projet est de récupérer le **vecteur de caractéristiques de dimensions `(1, 1, 1280)`** qui servira, plus tard, au travers d'un moteur de classification à reconnaître les différents fruits du jeu de données.\n",
    "\n",
    "Comme d'autres modèles similaires, **MobileNetV2**, lorsqu'on l'utilise en incluant toutes ses couches, attend obligatoirement des images de dimension `(224, 224, 3)`. Nos images étant toutes de dimension `(100, 100, 3)`, nous devrons simplement les **redimensionner** avant de les confier au modèle.\n",
    "\n",
    "<u>Dans l'odre</u> :\n",
    " 1. Nous chargeons le modèle **MobileNetV2** avec les poids **précalculés** issus d'**imagenet** et en spécifiant le format de nos images en entrée\n",
    " 2. Nous créons un nouveau modèle avec:\n",
    "  - <u>en entrée</u> : l'entrée du modèle MobileNetV2\n",
    "  - <u>en sortie</u> : l'avant dernière couche du modèle MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "model = MobileNetV2(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=True,\n",
    "    input_shape=(224, 224, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "new_model = Model(\n",
    "    inputs=model.input,\n",
    "    outputs=model.layers[-2].output\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage du résumé de notre nouveau modèle où nous constatons que <u>nous récupérons bien en sortie un vecteur de dimension (1, 1, 1280)</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)                 (None, 112, 112, 32  864         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalization)  (None, 112, 112, 32  128         ['Conv1[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)              (None, 112, 112, 32  0           ['bn_Conv1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (Depth  (None, 112, 112, 32  288        ['Conv1_relu[0][0]']             \n",
      " wiseConv2D)                    )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN (Ba  (None, 112, 112, 32  128        ['expanded_conv_depthwise[0][0]']\n",
      " tchNormalization)              )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_relu (  (None, 112, 112, 32  0          ['expanded_conv_depthwise_BN[0][0\n",
      " ReLU)                          )                                ]']                              \n",
      "                                                                                                  \n",
      " expanded_conv_project (Conv2D)  (None, 112, 112, 16  512        ['expanded_conv_depthwise_relu[0]\n",
      "                                )                                [0]']                            \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (Batc  (None, 112, 112, 16  64         ['expanded_conv_project[0][0]']  \n",
      " hNormalization)                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)        (None, 112, 112, 96  1536        ['expanded_conv_project_BN[0][0]'\n",
      "                                )                                ]                                \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNormal  (None, 112, 112, 96  384        ['block_1_expand[0][0]']         \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)     (None, 112, 112, 96  0           ['block_1_expand_BN[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D)    (None, 113, 113, 96  0           ['block_1_expand_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_depthwise (DepthwiseCo  (None, 56, 56, 96)  864         ['block_1_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (BatchNor  (None, 56, 56, 96)  384         ['block_1_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (ReLU)  (None, 56, 56, 96)   0           ['block_1_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)       (None, 56, 56, 24)   2304        ['block_1_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_1_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_1_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_2_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_2_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_2_depthwise (DepthwiseCo  (None, 56, 56, 144)  1296       ['block_2_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (BatchNor  (None, 56, 56, 144)  576        ['block_2_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (ReLU)  (None, 56, 56, 144)  0           ['block_2_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)       (None, 56, 56, 24)   3456        ['block_2_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_2_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_add (Add)              (None, 56, 56, 24)   0           ['block_1_project_BN[0][0]',     \n",
      "                                                                  'block_2_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_2_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_3_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_3_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D)    (None, 57, 57, 144)  0           ['block_3_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_3_depthwise (DepthwiseCo  (None, 28, 28, 144)  1296       ['block_3_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (BatchNor  (None, 28, 28, 144)  576        ['block_3_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (ReLU)  (None, 28, 28, 144)  0           ['block_3_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)       (None, 28, 28, 32)   4608        ['block_3_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_3_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_3_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_4_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_4_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_4_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_4_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_4_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_4_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_4_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_4_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_add (Add)              (None, 28, 28, 32)   0           ['block_3_project_BN[0][0]',     \n",
      "                                                                  'block_4_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_4_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_5_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_5_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_5_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_5_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_5_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_5_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_5_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_5_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_add (Add)              (None, 28, 28, 32)   0           ['block_4_add[0][0]',            \n",
      "                                                                  'block_5_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_5_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_6_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_6_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D)    (None, 29, 29, 192)  0           ['block_6_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_6_depthwise (DepthwiseCo  (None, 14, 14, 192)  1728       ['block_6_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (BatchNor  (None, 14, 14, 192)  768        ['block_6_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (ReLU)  (None, 14, 14, 192)  0           ['block_6_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)       (None, 14, 14, 64)   12288       ['block_6_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_6_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_6_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_7_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_7_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_7_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_7_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_7_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_7_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_7_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_7_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_add (Add)              (None, 14, 14, 64)   0           ['block_6_project_BN[0][0]',     \n",
      "                                                                  'block_7_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_7_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_8_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_8_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_8_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_8_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_8_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_8_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_8_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_8_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_add (Add)              (None, 14, 14, 64)   0           ['block_7_add[0][0]',            \n",
      "                                                                  'block_8_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_8_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_9_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_9_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_9_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_9_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_9_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_9_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_9_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_9_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_add (Add)              (None, 14, 14, 64)   0           ['block_8_add[0][0]',            \n",
      "                                                                  'block_9_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)       (None, 14, 14, 384)  24576       ['block_9_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchNorma  (None, 14, 14, 384)  1536       ['block_10_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU)    (None, 14, 14, 384)  0           ['block_10_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_depthwise (DepthwiseC  (None, 14, 14, 384)  3456       ['block_10_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (BatchNo  (None, 14, 14, 384)  1536       ['block_10_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0          ['block_10_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)      (None, 14, 14, 96)   36864       ['block_10_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_10_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_10_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_10_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_11_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_11_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_11_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_11_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_11_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_11_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_11_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_11_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_11_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_add (Add)             (None, 14, 14, 96)   0           ['block_10_project_BN[0][0]',    \n",
      "                                                                  'block_11_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_11_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_12_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_12_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_12_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_12_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_12_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_12_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_12_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_12_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_12_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_add (Add)             (None, 14, 14, 96)   0           ['block_11_add[0][0]',           \n",
      "                                                                  'block_12_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_12_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_13_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_13_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2D)   (None, 15, 15, 576)  0           ['block_13_expand_relu[0][0]']   \n",
      "                                                                                                  \n",
      " block_13_depthwise (DepthwiseC  (None, 7, 7, 576)   5184        ['block_13_pad[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (BatchNo  (None, 7, 7, 576)   2304        ['block_13_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)   0           ['block_13_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)      (None, 7, 7, 160)    92160       ['block_13_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_13_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_13_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_13_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_14_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_14_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_14_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_14_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_14_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_14_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_14_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_14_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_14_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_add (Add)             (None, 7, 7, 160)    0           ['block_13_project_BN[0][0]',    \n",
      "                                                                  'block_14_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_14_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_15_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_15_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_15_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_15_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_15_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_15_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_15_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_15_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_15_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_add (Add)             (None, 7, 7, 160)    0           ['block_14_add[0][0]',           \n",
      "                                                                  'block_15_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_15_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_16_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_16_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_16_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_16_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_16_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_16_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)      (None, 7, 7, 320)    307200      ['block_16_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_16_project_BN (BatchNorm  (None, 7, 7, 320)   1280        ['block_16_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)                (None, 7, 7, 1280)   409600      ['block_16_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)  5120        ['Conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " out_relu (ReLU)                (None, 7, 7, 1280)   0           ['Conv_1_bn[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1280)        0           ['out_relu[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 2,223,872\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Affichage dégradé que je ne m'explique pas, par rapport à l'original :\n",
    "# pas de lignes blanches séparatrices\n",
    "# retour à la ligne en seconde colonne\n",
    "# Vu la pression que j'ai au temps, pas le temps de regarder cela, mais je n'aime pas\n",
    "display(new_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 112, 112, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 112, 112, 32) 128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, 112, 112, 32) 0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 112, 112, 32) 288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 112, 112, 32) 128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, 112, 112, 32) 0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 112, 112, 16) 512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 112, 112, 16) 64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 112, 112, 96) 1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 112, 112, 96) 384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, 112, 112, 96) 0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_pad (ZeroPadding2D)     (None, 113, 113, 96) 0           block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 56, 56, 96)   864         block_1_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 56, 56, 96)   384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, 56, 56, 96)   0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 56, 56, 24)   2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 56, 56, 24)   96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 56, 56, 144)  3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 56, 56, 144)  1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 56, 56, 144)  576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, 56, 56, 144)  0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 56, 56, 24)   3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 56, 56, 24)   96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 56, 56, 24)   0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 56, 56, 144)  3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_pad (ZeroPadding2D)     (None, 57, 57, 144)  0           block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 28, 28, 144)  1296        block_3_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 28, 28, 144)  576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, 28, 28, 144)  0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 28, 28, 32)   4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 28, 28, 32)   128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 28, 28, 192)  6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 28, 28, 32)   6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 28, 28, 32)   128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 28, 28, 32)   0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 28, 28, 192)  6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 28, 28, 32)   6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 28, 28, 32)   128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 28, 28, 32)   0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, 28, 28, 192)  6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_pad (ZeroPadding2D)     (None, 29, 29, 192)  0           block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, 14, 14, 192)  1728        block_6_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, 14, 14, 192)  768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, 14, 14, 192)  0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, 14, 14, 64)   12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, 14, 14, 64)   256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, 14, 14, 384)  24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, 14, 14, 64)   24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, 14, 14, 64)   256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, 14, 14, 64)   0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, 14, 14, 384)  24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, 14, 14, 64)   24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, 14, 14, 64)   256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, 14, 14, 64)   0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, 14, 14, 384)  24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, 14, 14, 64)   24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, 14, 14, 64)   256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, 14, 14, 64)   0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, 14, 14, 384)  24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, 14, 14, 384)  1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, 14, 14, 384)  0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, 14, 14, 384)  3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, 14, 14, 384)  1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, 14, 14, 96)   36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, 14, 14, 96)   384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, 14, 14, 576)  55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, 14, 14, 96)   55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, 14, 14, 96)   384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, 14, 14, 96)   0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, 14, 14, 576)  55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, 14, 14, 96)   55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, 14, 14, 96)   384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, 14, 14, 96)   0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, 14, 14, 576)  55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_pad (ZeroPadding2D)    (None, 15, 15, 576)  0           block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, 7, 7, 576)    5184        block_13_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, 7, 7, 576)    2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)    0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, 7, 7, 160)    92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, 7, 7, 160)    640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, 7, 7, 960)    153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, 7, 7, 160)    153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, 7, 7, 160)    640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, 7, 7, 160)    0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, 7, 7, 960)    153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, 7, 7, 160)    153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, 7, 7, 160)    640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, 7, 7, 160)    0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, 7, 7, 960)    153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, 7, 7, 320)    307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, 7, 7, 320)    1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 7, 7, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1280)         0           out_relu[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 2,223,872\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les workeurs doivent pouvoir accéder au modèle ainsi qu'à ses poids. Une bonne pratique consiste à charger le modèle sur le driver puis à diffuser ensuite les poids aux différents workeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_weights = sc.broadcast(new_model.get_weights())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Mettons cela sous forme de fonctions</u> :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚧 **TODO** Pourquoi ça ne fonctionne pas sous cette forme ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fruits.model import init_keras_model\n",
    "from fruits.driver import (\n",
    "    init_spark_session,\n",
    "    broadcast_model_weights\n",
    ")\n",
    "\n",
    "spark = init_spark_session()\n",
    "model = init_keras_model()\n",
    "broadcast_model_weights(spark, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version d'origine\n",
    "\n",
    "```python\n",
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=True,\n",
    "        input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(\n",
    "        inputs=model.input,\n",
    "        outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(broadcast_weights.value)\n",
    "    return new_model\n",
    "```\n",
    "\n",
    "Avec juste quelques renommages.\n",
    "\n",
    "Notons que broadcast est passé comme variable globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_keras_model() -> Model:\n",
    "    \"\"\"Returns a MobileNetV2 model with the top layer removed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Model\n",
    "        The initialized model.\n",
    "    \"\"\"\n",
    "    # Create a MobileNetV2 model with pre-trained weights\n",
    "    model = MobileNetV2(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=True,\n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    \n",
    "    # Set all layers in the model as non-trainable\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Create a new model without the top layer\n",
    "    new_model = Model(\n",
    "        inputs=model.input,\n",
    "        outputs=model.layers[-2].output\n",
    "    )\n",
    "    \n",
    "    # Broadcast the model weights\n",
    "    new_model.set_weights(broadcast_weights.value)\n",
    "    \n",
    "    # Return the model\n",
    "    return new_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.3 Définition du processus de chargement des images et application <br/>de leur featurisation à travers l'utilisation de pandas UDF\n",
    "\n",
    "Ce notebook définit la logique par étapes, jusqu'à Pandas UDF.\n",
    "\n",
    "<u>L'empilement des appels est la suivante</u> :\n",
    "\n",
    "* Pandas UDF\n",
    "    * extraire les caractéristiques de la collection d'images (`pandas.Series`) \n",
    "    * pré-traiter une image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚧 **TODO** Pourquoi ça ne fonctionne pas sous cette forme ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idem, ces fonctions sont retravaillées, renommées et déplacées dans `fruits.executor` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fruits.executor import (\n",
    "    preprocess_img,\n",
    "    extract_image_features,\n",
    "    extract_image_features_udf\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version d'origine\n",
    "\n",
    "```python\n",
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)\n",
    "```\n",
    "\n",
    "Avec quelques renommages :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Any, Iterator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import pandas_udf  # , PandasUDFType : Spark 3.4.0\n",
    "\n",
    "def preprocess_img(content: Union[bytes, bytearray]) -> Any:\n",
    "    \"\"\"Pre-processes raw image bytes for prediction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    content : Union[bytes, bytearray]\n",
    "        Raw image bytes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Any\n",
    "        Pre-processed image data.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the provided content is not valid image bytes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the image from raw bytes and resize it\n",
    "        img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "        # Convert the image to an array\n",
    "        arr = img_to_array(img)\n",
    "        return preprocess_input(arr)\n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "            \"Invalid image content. Please provide valid image bytes.\"\n",
    "        ) from e\n",
    "\n",
    "\n",
    "def extract_image_features(\n",
    "    model: Model,\n",
    "    content_series: pd.Series\n",
    ") -> pd.Series:\n",
    "    \"\"\"Extracts image features from a pd.Series of raw images using the input\n",
    "    model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Model\n",
    "        The pre-trained model used for feature extraction.\n",
    "    content_series : pd.Series\n",
    "        The pd.Series containing raw image data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        The pd.Series containing the extracted image features.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    The function assumes that the `preprocess_img` function is defined separately.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the model is not a valid TensorFlow Keras model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocess the images in the content_series\n",
    "        prep_imgs = np.stack(content_series.map(preprocess_img))\n",
    "        # Extract the features using the model\n",
    "        feats = model.predict(prep_imgs)\n",
    "        # Flatten the feature tensors to vectors\n",
    "        flat_feats = [f.flatten() for f in feats]\n",
    "        return pd.Series(flat_feats)\n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "            \"Invalid model. Please provide a valid TensorFlow Keras model.\"\n",
    "        ) from e\n",
    "\n",
    "\n",
    "# See : https://www.databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html\n",
    "@pandas_udf(\"array<float>\")  # , PandasUDFType.SCALAR_ITER) = warning\n",
    "def extract_image_features_udf(\n",
    "    content_series_iter: Iterator[pd.Series]\n",
    ") -> Iterator[pd.Series]:\n",
    "    \"\"\"This method is a Scalar Iterator pandas UDF wrapping our\n",
    "    `extract_image_features` function. The decorator specifies that this\n",
    "    returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    content_series_iter : Iterator[pd.Series]\n",
    "        An iterator over batches of data, where each batch is a pandas Series\n",
    "        of image data.\n",
    "    Yields\n",
    "    ------\n",
    "    Iterator[pd.Series]\n",
    "        An iterator over the extracted image features for each batch of data.\n",
    "    \"\"\"\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then\n",
    "    # re-use it for multiple data batches. This amortizes the overhead of\n",
    "    # loading big models.\n",
    "    # model_weights = spark.sparkContext.broadcast(broadcast_weights.value)\n",
    "    model = init_keras_model()\n",
    "    # model.set_weights(model_weights.values)\n",
    "    for content_series in content_series_iter:\n",
    "        yield extract_image_features(model, content_series)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.4 Exécution des actions d'extraction de features\n",
    "\n",
    "Les Pandas UDF, sur de grands enregistrements (par exemple, de très grandes images), peuvent rencontrer des erreurs de type `Out Of Memory` (OOM). Si vous rencontrez de telles erreurs dans la cellule ci-dessous, essayez de réduire la taille du lot Arrow via `maxRecordsPerBatch`\n",
    "\n",
    "Je n'utiliserai pas cette commande dans ce projet et je laisse donc la commande en commentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant exécuter la featurisation sur l'ensemble de notre DataFrame Spark.\n",
    "\n",
    "<u>REMARQUE</u> : Cela peut prendre beaucoup de temps, tout dépend du volume de données à traiter.\n",
    "\n",
    "Notre jeu de données de **Test** contient **22 819 images**.\n",
    "\n",
    "Cependant, dans l'exécution en mode **local**, nous <u>traiterons un ensemble réduit de **330 images**</u>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚧 **TODO** Pourquoi ça ne fonctionne pas sous cette forme ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fruits.driver import init_spark_session, load_images\n",
    "\n",
    "spark = init_spark_session()\n",
    "images = load_images(spark)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version d'origine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fruits.executor import extract_image_features_udf\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#features_df = images.repartition(20).select(\n",
    "im_feats = images.repartition(20).select(\n",
    "    col(\"path\"), col(\"label\"),\n",
    "    extract_image_features_udf(\"content\").alias(\"features\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappel du chemin du répertoire où seront enregistrés nos résultats en plusieurs fichiers au format **`parquet`**.\n",
    "\n",
    "Nos résultats se présentent sous la forme d'un DataFrame à 3 colonnes :\n",
    " 1. `path` le chemin de l'image\n",
    " 2. `label` la classe de l'image\n",
    " 3. `features`, le vecteur de caractéristiques de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/walduch/Documents/P8/data/Results\n"
     ]
    }
   ],
   "source": [
    "print(PATH_Result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Enregistrement des données traitées au format \"**parquet**\"</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, label: string, features: array<float>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(im_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\franc\\\\Projects\\\\pepper_cloud_based_model\\\\tmp\\\\im_feats.pqt'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pepper.env import get_tmp_dir\n",
    "import os\n",
    "im_feats_pqt_path = os.path.join(get_tmp_dir(), \"im_feats.pqt\")\n",
    "display(im_feats_pqt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Fruits</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2717f545810>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o98.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 841) (host.docker.internal executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/franc/Projects/pepper_cloud_based_model/tmp/im_feats.pqt.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\franc\\Projects\\pepper_cloud_based_model\\modules\\fruits\\executor.py\", line 93, in <module>\n    @pandas_udf(\"array<float>\")  # , PandasUDFType.SCALAR_ITER) = warning\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\functions.py\", line 461, in _create_pandas_udf\n    return _create_udf(f, returnType, evalType)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 82, in _create_udf\n    return udf_obj._wrapped()\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 431, in _wrapped\n    wrapper.returnType = self.returnType  # type: ignore[attr-defined]\n                         ^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 236, in returnType\n    self._returnType_placeholder = _parse_datatype_string(self._returnType)\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1211, in _parse_datatype_string\n    sc = get_active_spark_context()\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 201, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n\t... 15 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/franc/Projects/pepper_cloud_based_model/tmp/im_feats.pqt.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\franc\\Projects\\pepper_cloud_based_model\\modules\\fruits\\executor.py\", line 93, in <module>\n    @pandas_udf(\"array<float>\")  # , PandasUDFType.SCALAR_ITER) = warning\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\functions.py\", line 461, in _create_pandas_udf\n    return _create_udf(f, returnType, evalType)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 82, in _create_udf\n    return udf_obj._wrapped()\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 431, in _wrapped\n    wrapper.returnType = self.returnType  # type: ignore[attr-defined]\n                         ^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 236, in returnType\n    self._returnType_placeholder = _parse_datatype_string(self._returnType)\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1211, in _parse_datatype_string\n    sc = get_active_spark_context()\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 201, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n\t... 15 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m im_feats\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mmode(\u001b[39m\"\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mparquet(im_feats_pqt_path)\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1656\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1654\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   1655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[1;32m-> 1656\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o98.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 841) (host.docker.internal executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/franc/Projects/pepper_cloud_based_model/tmp/im_feats.pqt.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\franc\\Projects\\pepper_cloud_based_model\\modules\\fruits\\executor.py\", line 93, in <module>\n    @pandas_udf(\"array<float>\")  # , PandasUDFType.SCALAR_ITER) = warning\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\functions.py\", line 461, in _create_pandas_udf\n    return _create_udf(f, returnType, evalType)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 82, in _create_udf\n    return udf_obj._wrapped()\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 431, in _wrapped\n    wrapper.returnType = self.returnType  # type: ignore[attr-defined]\n                         ^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 236, in returnType\n    self._returnType_placeholder = _parse_datatype_string(self._returnType)\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1211, in _parse_datatype_string\n    sc = get_active_spark_context()\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 201, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n\t... 15 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/franc/Projects/pepper_cloud_based_model/tmp/im_feats.pqt.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\franc\\Projects\\pepper_cloud_based_model\\modules\\fruits\\executor.py\", line 93, in <module>\n    @pandas_udf(\"array<float>\")  # , PandasUDFType.SCALAR_ITER) = warning\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\functions.py\", line 461, in _create_pandas_udf\n    return _create_udf(f, returnType, evalType)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 82, in _create_udf\n    return udf_obj._wrapped()\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 431, in _wrapped\n    wrapper.returnType = self.returnType  # type: ignore[attr-defined]\n                         ^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 236, in returnType\n    self._returnType_placeholder = _parse_datatype_string(self._returnType)\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1211, in _parse_datatype_string\n    sc = get_active_spark_context()\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 201, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n\t... 15 more\r\n"
     ]
    }
   ],
   "source": [
    "im_feats.write.mode(\"overwrite\").parquet(im_feats_pqt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.write.mode(\"overwrite\").parquet(PATH_Result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ICI UN POINT TRES DOULOUREUX**\n",
    "\n",
    "Passé 6h30 sans avancer, à tourner en rond, rien ne fonctionne et surtout, je ne comprends pas le broadcasting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Chargement des données enregistrées et validation du résultat\n",
    "\n",
    "<u>On charge les données fraichement enregistrées dans un **DataFrame Pandas**</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(PATH_Result, engine=\"pyarrow\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>On affiche les 5 premières lignes du DataFrame</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/home/walduch/Documents/P8/data/Test1/App...</td>\n",
       "      <td>Apple Braeburn</td>\n",
       "      <td>[0.86105645, 0.16019525, 0.0, 0.0, 0.0, 1.0233...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/home/walduch/Documents/P8/data/Test1/Cle...</td>\n",
       "      <td>Clementine</td>\n",
       "      <td>[0.45963708, 0.0, 0.0, 0.0, 0.036376934, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/home/walduch/Documents/P8/data/Test1/Cle...</td>\n",
       "      <td>Clementine</td>\n",
       "      <td>[1.3859445, 0.04571251, 0.0, 0.0, 0.9309062, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/home/walduch/Documents/P8/data/Test1/App...</td>\n",
       "      <td>Apple Braeburn</td>\n",
       "      <td>[1.7865905, 0.20313944, 0.0, 0.0, 0.41594356, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/home/walduch/Documents/P8/data/Test1/App...</td>\n",
       "      <td>Apple Braeburn</td>\n",
       "      <td>[0.81415516, 0.18681705, 0.0, 0.0, 0.0, 0.3806...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path           label  \\\n",
       "0  file:/home/walduch/Documents/P8/data/Test1/App...  Apple Braeburn   \n",
       "1  file:/home/walduch/Documents/P8/data/Test1/Cle...      Clementine   \n",
       "2  file:/home/walduch/Documents/P8/data/Test1/Cle...      Clementine   \n",
       "3  file:/home/walduch/Documents/P8/data/Test1/App...  Apple Braeburn   \n",
       "4  file:/home/walduch/Documents/P8/data/Test1/App...  Apple Braeburn   \n",
       "\n",
       "                                            features  \n",
       "0  [0.86105645, 0.16019525, 0.0, 0.0, 0.0, 1.0233...  \n",
       "1  [0.45963708, 0.0, 0.0, 0.0, 0.036376934, 0.0, ...  \n",
       "2  [1.3859445, 0.04571251, 0.0, 0.0, 0.9309062, 0...  \n",
       "3  [1.7865905, 0.20313944, 0.0, 0.0, 0.41594356, ...  \n",
       "4  [0.81415516, 0.18681705, 0.0, 0.0, 0.0, 0.3806...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>On valide que la dimension du vecteur de caractéristiques des images est bien de dimension 1280</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.loc[0, \"features\"].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous venons de valider le processus sur un jeu de données allégé en local où nous avons simulé un cluster de machines en répartissant la charge de travail sur différents cœurs de processeur au sein d'une même machine.\n",
    "\n",
    "Nous allons maintenant généraliser le processus en déployant notre solution sur un réel cluster de machines et nous travaillerons désormais sur la totalité des $22\\,819$ images de notre dossier `Test`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4\\. Déploiement de la solution sur le cloud\n",
    "\n",
    "Nous avons pu valider le fonctionnement de notre solution sur un cluster local. Il s'agit à présent de le déployer sur une infrastructure *élastique*, c'est-à-dire capable de se redimensionner pour accompagner la montée en puissance et donc en charge de l'application.\n",
    "\n",
    "**Attention**, *je travaille sous Linux avec une version Ubuntu, les commandes décrites ci-dessous sont donc réalisées  exclusivement dans cet environnement.*\n",
    "\n",
    "Pour arrêter un choix d'architecture technique, répondons à ces 4 questions :\n",
    "1. Quel prestataire de Cloud choisir ?\n",
    "2. Quelles solutions de ce prestataire adopter ?\n",
    "3. Où stocker nos données ?\n",
    "4. Comment configurer nos outils dans ce nouvel environnement ?\n",
    "\n",
    "## 4.1 Choix du prestataire cloud : AWS\n",
    "\n",
    "Le leader incontesté du marché du cloud computing en son précurseur historique. L'offre **Amazon Web Services** (AWS) d'Amazon demeure à ce jour la plus large et la plus complète dans le domaine du cloud computing, même si ses challengers (Google, Microsoft, etc) n'ont pas à rougir de leur offre, en particulier sur le segment du Big Data.\n",
    "\n",
    "D'autres acteurs proposent des offres plus spécialisées ou plus compétitives qu'il faut pouvoir envisager selon les spécificités de chaque projet. Mais en l'occurrence, notre projet est une application classique simple basée sur quelques librairies standard bien établies qui ne nécessite donc pas d'explorer de telles voies et peut se satisfaire de micro-services standards.\n",
    "\n",
    "Nous choisissons donc AWS, pour cette raison pratique, pour la facilité et la rapidité de mise en oeuvre que nous lui connaissons déjà, ainsi que pour les possibilités avancées de contrôle des coûts et de modulations tarifaires offertes par ce prestataire (possibilité de choisir les modalités tarifaires les plus avantageuses selon le profil analytique de charge).\n",
    "\n",
    "L'objectif immédiat est de pouvoir louer de la puissance de calcul à la demande (donc sans immobilisation), de déployer facilement en se déchargeant de la responsabilité de maintenance évolutive de l'infrastructure, de disposer d'un dimensionnement automatique transparent des ressources de service pour maintenir une continuité de service à mesure qu'augmente la base d'utilisateurs de l'application, de maîtriser les coûts induits à l'aide de tableaux de bord analytiques.\n",
    "\n",
    "## 4.2 Choix de la solution technique : EMR\n",
    "\n",
    "Classiquement (depuis le modèle OSI des années 70), le marketing IT segmente les offres par niveaux d'abstraction informatique et remanie le corpus pour renforcer l'idée de nouveauté.\n",
    "\n",
    "Dans le domaine du *cloud computing*, où les ressources sont louées *as a Service* utilisé [*à la demande* (années 2000)](https://www.csoonline.com/article/2115856/ibm-s-on-demand-strategy.html), nous distinguerons essentiellement les deux niveaux auxquels nous pouvons nous déployer en tant que :\n",
    "1. Sur une offre de solution **IaaS** (Infrastructure as a Service) : hébergement classique avec location de l'infrastructure.\n",
    "2. Sur une offre de solution **PaaS** (Plateforme as a Service) : approche orientée micro-services, avec location des seuls espace de stockage et puissance de calcul, et donc avec une virtualisation (abstraction et découplage) totale des couches matérielles.\n",
    "\n",
    "### Solution **IaaS**\n",
    "\n",
    "Dans cette configuration **AWS** met à notre disposition des serveurs vierges (*instances EC2*) que nous pouvons directement administrer.\n",
    "\n",
    "Avec une telle solution, nous pouvons reproduire pratiquement à l'identique la solution mise en œuvre localement sur notre machine.\n",
    "\n",
    "Nous installons nous-mêmes l'ensemble des outils et dépendances dont nous avons besoin, puis nous soumettons notre script :\n",
    "* Installation de **Spark**, **Java**, **Python**, **Jupyter Notebook**, et des **librairies complémentaires**\n",
    "* Il nous faudra notamment veiller à configurer **chacune des machines (workers) du cluster**\n",
    "\n",
    "|**Avantages**|**Inconvénients**|\n",
    "|-|-|\n",
    "|- **Liberté totale** de mise en œuvre de la solution<br/>- **Facilité de mise en œuvre** à partir d'un modèle qui s'exécute en local sur une machine Linux|- **Chronophage** : il est nécessité d'installer et de configurer toute la solution<br/>- Possible **problèmes d'installation** des outils (des problématiques qui n'existaient pas en local sur notre machine peuvent apparaître sur le serveur EC2)<br/>- Solution **non pérenne**, il faudra veiller à la mise à jour des outils et éventuellement devoir réinstaller Spark, Java etc.|\n",
    "\n",
    "### Solution **PaaS**\n",
    "\n",
    "La galaxie des micro-services **AWS** est l'une des plus riches de l'univers du *cloud*.\n",
    "\n",
    "En particulier, Amazon nous propose une offre de plate-forme Big Data, son offre [EMR (*Elastic MapReduce*)](https://aws.amazon.com/fr/emr/) qui prend en charge Apache Spark, Hive, Presto et autres applications Big Data.\n",
    "\n",
    "\n",
    "fournit énormément de services différents, dans l'un de ceux-là il existe une offre qui permet de louer des **instances EC2** avec des applications préinstallées et configurées : il s'agit du **service EMR**.\n",
    "* **Spark** y sera déjà pré-installé\n",
    "* Il est possible de demander l'installation de :\n",
    "    * **Tensorflow** et **JupyterHub**\n",
    "    * des **packages complémentaires**\n",
    "* .. **sur l'ensemble des workers du cluster**.\n",
    "\n",
    "\n",
    "|**Avantages**|**Inconvénients**|\n",
    "|-|-|\n",
    "|- **Facilité de mise en œuvre** : Il suffit de très peu de configuration pour obtenir un environnement parfaitement opérationnel<br/>- **Rapidité de mise en œuvre** : Une fois la première configuration réalisée, il est très facile et très rapide de recréer des clusters à l'identique qui seront disponibles presque instantanément (le temps d'instancier les serveurs soit environ 15/20 minutes)<br/>- **Solutions matérielles et logicielles optimisées** par les ingénieurs d'AWS : On sait que les versions installées vont fonctionner et que l'architecture proposée est optimisée<br/>- **Stabilité de la solution**<br/>- **Solution évolutive** : Il est facile d’obtenir à chaque nouvelle instanciation une version à jour de chaque package, en étant garanti de leur compatibilité avec le reste de l’environnement<br/>- **Maintenance de sécurisé** : Les éventuels patchs de sécurité seront automatiquement mis à jour à chaque nouvelle instanciation du cluster EMR.|<br/>- Peut-être un certain **manque de liberté** sur la version des packages disponibles ? Même si je n'ai pas constaté ce problème.|\n",
    "\n",
    "### Choix retenu\n",
    "\n",
    "Nous sommes actuellement en phase d'amorçage du déploiement de l'application sur une infrastructure capable de monter en charge.\n",
    "\n",
    "La bonne solution est évidemment de nous focaliser sur un déploiement rapide sur une plate-forme qui nous assure la **continuité de service** et la *scalabilité* sans nous faire courir les risques et les coûts induits par une approche *IaaS*.\n",
    "\n",
    "Il sera toujours possible d'opter, après une première phase d'observation de la production, pour une architecture technique plus *à façon* et éventuellement basée sur l'offre *IaaS*, mais bien plus probablement basée sur une intégration horizontale de micro-services *PaaS* alternatifs ou complémentaires à *EMR*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Choix de la solution de stockage des données : Amazon S3\n",
    "\n",
    "Nous pourrions être tentés de stocker nos données d'application sur l'espace de nos serveurs **EC2** sous-jacents. En effet, un principe fondamental des approches Big Data est d'assurer autant que possible la *collocalisation* des données et des traitements qui s'y rapportent.\n",
    "\n",
    "Mais cela pose trois problèmes que nous préférons éviter :\n",
    "1. Ce mode de stockage des données est **plus onéreux** que l'utilisation de services de stockage spécialisés.\n",
    "2. En cas de résiliation d'une instance EMR, pour cause d'inactivité (un EMR coûte, même s'il n'est pas utilisé), ou de simple bascule d'une instance sur une autre, à défaut d'une solution supplémentaire de *backup*, les données seraient perdues.\n",
    "3. Nous nous exposons à des **risques de ralentissements voire dysfonctionnements de service** en raison du risque de saturation de l'espace disponible, limité. \n",
    "\n",
    "**saturer** l'espace disponible de nos serveurs (ralentissements, dysfonctionnements).\n",
    "\n",
    "Amazon propose un service de stockage des données, [**S3** (*Simple Storage Service*)](https://aws.amazon.com/fr/s3/), qui nous permet de nous affranchir de ces 3 problématiques :\n",
    "* Coût de stockage compétitif.\n",
    "* Persistence découplée du cycle de vie des instances EMR.\n",
    "* Espace **illimité** (certes c'est une vue de l'esprit, mais disons que le risque de saturation est proche de 0).\n",
    "\n",
    "En outre, en faisant le choix d'un service de stockage AWS, et en prenant notamment soin de choisir la même région (c'est-à-dire le même Data Center ou bien des Data Centers géographiquement proches) pour nos serveurs **EC2** et **S3**, nous nous assurons de minimiser la latence dans l'accès aux données depuis l'EMR.\n",
    "\n",
    "<mark>?? De plus, comme nous le verrons <u>il est possible d'accéder aux données sur **S3** de la même manière que l'on **accède aux données sur un disque local**</u> -> ?? accès séquentiel, non ?</mark>\n",
    "\n",
    "L'ensemble des données sera stocké dans le compartiment sécurisé (un *bucket* non accessible de l'Internet) **`s3://pepper-labs-fruits`**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Configuration de l'environnement de travail\n",
    "\n",
    "La première étape est de créer un utilisateur distinct du compte racine (Root), pour des raisons évidentes de sécurité.\n",
    "\n",
    "Cet utilisateur doit disposer de droits suffisants pour opérer comme développeur et comme administrateur des services visés, donc disposer d'un contrôle total sur les services EMR et S3.\n",
    "\n",
    "La création des utilisateurs réguliers, des groupes (par exemple *Developers*), la définition et l'attribution des droits, s'effectue depuis un service incontournable d'AWS, [**IAM** (Identity and Access Management)](https://aws.amazon.com/fr/iam/).\n",
    "\n",
    "Nous choisissons, pour l'utilisateur *Root*, comme pour l'utilisateur régulier, *Pepper*, la mise en place d'une authentification [**MFA** (Multi-Factor Authentication)](https://aws.amazon.com/fr/iam/features/mfa/) comme barrière à l'entrée de la console AWS, ce qui nous assure le meilleur niveau de sécurité. En effet, dans le cadre d'une architecture cloud, le détournement d'un compte, en particulier un compte qui dispose des droits les plus étendus, pourrait avoir des conséquences dramatiques.\n",
    "\n",
    "Pour accéder ensuite à distance, en ligne de commande, ou encore programmatiquement via les APIs d'AWS (boto3) aux services AWS sans passer par l'interface Web de la console AWS, il est nécessaire de générer des paires de clés publique/privée (RSA), chaque utilisateur devant prendre le plus grand soin à conserver sa clé privée secrète et donc à ne la communiquer sous aucun prétexte à qui que ce soit (sûreté).\n",
    "\n",
    "Cette clé privée sera le sésame pour ouvrir les différentes serrures (commande AWS, accès SSH si nécessaires (par exemple à nos instances EC2), déchiffrement de mot de passe administrateur (par exemple si nous installions un serveur Windows), interactions programmatiques via l'API, etc). Notons qu'il est possible de multiplier ces clés, et même de les attribuer pour un usage unique, afin de renforcer la sécurité, mais nous laissons à l'équipe des Administrateurs Système le soin d'aligner la configuration d'IAM avec l'annuaire et les politiques de l'entreprise.\n",
    "\n",
    "Pour ce qui les manipulations que nous allons effectuer dans la suite, il sera pratique d'utiliser [**AWS CLI**](https://aws.amazon.com/fr/cli/), l'interface en ligne de commande d'AWS, qui permet d'interagir directement depuis un terminal (Linux ou PowerShell) avec les différents services d'AWS**. Cela nous permettra par exemple de lister le contenu de notre compartiment S3, d'en télécharger ou d'y téléverser des fichiers, avec la même facilité que si ces fichiers étaient locaux."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Upload de nos données sur S3\n",
    "\n",
    "Nos outils sont configurés.\n",
    "\n",
    "Il faut maintenant de téléverser nos données de travail sur Amazon S3.\n",
    "\n",
    "Ces données sont les images contenues dans le répertoire `Test` du jeu de données téléchargé sur [**Kaggle**](https://www.kaggle.com/moltean/fruits/download).\n",
    "\n",
    "Créons le compartiment S3 :\n",
    "\n",
    "```sh\n",
    "$ aws s3 mb s3://pepper-labs-fruits\n",
    "```\n",
    "\n",
    "Vérifions qu'il a bien été créé :\n",
    "\n",
    "```sh\n",
    "$ aws s3 ls\n",
    "2023-05-15 20:31:10 pepper-labs-fruits\n",
    "```\n",
    "\n",
    "Copions à présent le contenu du dossier `Test` dans un répertoire `Test` sur notre bucket `p8-data` :\n",
    "\n",
    "On se place à l'intérieur du répertoire `Test` et on synchronise les contenus local et distant à l'aide de la commande `sync` :\n",
    "\n",
    "```sh\n",
    "$ cd <fruits-360_dataset_path>/Test\n",
    "$ aws sync . s3://pepper-labs-fruits/Test\n",
    "```\n",
    "\n",
    "Nos données de projet sont à présent disponibles sur Amazon S3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Configuration du serveur EMR\n",
    "\n",
    "Une fois encore, le cours [Réalisez des calculs distribués sur des données massives / Déployez un cluster de calculs distribués](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues) <br /> détaille l'essentiel des étapes pour lancer un cluster avec **EMR**.\n",
    "\n",
    "Réécrire les références 1/ en plus dense 2/ en plus 'à la source'\n",
    "\n",
    "----\n",
    "\n",
    "Je détaillerai ici les étapes particulières qui nous permettent de configurer le serveur selon nos besoins :\n",
    "1. Cliquez sur `Créer un cluster` :\n",
    "![Créer un cluster](../baseline/img/EMR_creer.png)\n",
    "\n",
    "2. Cliquez sur `Accéder aux options avancées` :\n",
    "![Créer un cluster](../baseline/img/EMR_options_avancees.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.1 Étape 1 : Logiciels et étapes\n",
    "\n",
    "#### 4.6.1.1 Configuration des logiciels\n",
    "\n",
    "Sélectionnez les packages dont nous aurons besoin comme dans la capture d'écran :\n",
    "1. Nous sélectionnons la dernière version d'`EMR`, soit la version `6.3.0` au moment où je rédige ce document\n",
    "2. Nous cochons bien évidement `Hadoop` et `Spark` qui seront préinstallés dans leur version la plus récente\n",
    "3. Nous aurons également besoin de `TensorFlow` pour importer notre modèle et réaliser le *transfert learning*\n",
    "4. Nous travaillerons enfin avec un *notebook `Jupyter`* via l'application `JupyterHub`\n",
    "  * Comme nous le verrons dans un instant nous allons <u>paramétrer l'application afin que les notebooks</u>,\n",
    "  * comme le reste de nos données de travail, <u>soient enregistrés directement sur S3</u>.\n",
    "  \n",
    "![Créer un cluster](../baseline/img/EMR_configuration_logiciels.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1.2 Modifier les paramètres du logiciel\n",
    "\n",
    "Paramétrez la persistance des notebooks créés et ouvert via JupyterHub :\n",
    "\n",
    "On peut à cette étape effectuer des demandes de paramétrage particulières sur nos applications.\n",
    "\n",
    "L'objectif est, comme pour le reste de nos données de travail, d'éviter toutes les problématiques évoquées précédemment.\n",
    "\n",
    "C'est l'objectif à cette étape, <u>nous allons enregistrer et ouvrir les notebooks</u> non pas sur l'espace disque de  l'instance EC2 (comme ce serait le cas dans la configuration par défaut de JupyterHub) mais <u>directement sur **Amazon S3**</u>.\n",
    "\n",
    "Deux solutions sont possibles pour réaliser cela :\n",
    "1. Créer un **fichier de configuration JSON** que l'on **upload sur S3** et on indique ensuite le chemin d’accès au fichier JSON\n",
    "2. Rentrez directement la configuration au format JSON\n",
    " \n",
    "J'ai personnellement créé un fichier JSON lors de la création de ma première instance EMR, puis lorsqu'on décide de cloner notre serveur pour en recréer un facilement à l'identique, la configuration du fichier JSON se retrouve directement copié comme dans la capture ci-dessous.\n",
    "\n",
    "<u>Voici le contenu de mon fichier JSON</u> :\n",
    "\n",
    "```sh\n",
    "[\n",
    "    {\n",
    "        \"classification\": \"jupyter-s3-conf\",\n",
    "        \"properties\": {\n",
    "            \"s3.persistence.bucket\": \"pepper-labs-fruits\",\n",
    "            \"s3.persistence.enabled\": \"true\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "Appuyez ensuite sur \"**Suivant**\"\n",
    "\n",
    "![Modifier les paramètres du logiciel](../baseline/img/EMR_parametres_logiciel.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.2 Étape 2 : Matériel\n",
    "\n",
    "A cette étape, laissez les choix par défaut.\n",
    "\n",
    "L'important ici est la sélection des instances :\n",
    "\n",
    "1. Nous choisissons des instances de type `M5` qui sont des **instances de type équilibrés**\n",
    "2. Nous choisissons le type `xlarge` qui est l'instance la **moins onéreuse disponible** \n",
    "3. Nous sélectionnons 1 instance **Maître** (le *pilote*) et 2 instances **Principales** (les *travailleurs*) soit 3 instances EC2.\n",
    "\n",
    "Références :\n",
    "* [Instances M5 Amazon EC2](https://aws.amazon.com/fr/ec2/instance-types/m5/)\n",
    "\n",
    "![Choix du materiel](../baseline/img/EMR_materiel.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.3 Étape 3 : Paramètres de cluster généraux\n",
    "\n",
    "#### 4.6.3.1 Options générales\n",
    "\n",
    "La première chose à faire est de donner un nom au cluster.\n",
    "\n",
    "Pour des raisons pratiques, j'ai également décoché `Protection de la résiliation`.\n",
    "    \n",
    "![Nom du Cluster](../baseline/img/EMR_nom_cluster.png)\n",
    "\n",
    "#### 4.6.3.2 Actions d'amorçage\n",
    "\n",
    "Nous allons à cette étape **choisir les packages manquants à installer** qui sont indispensables pour exécuter notre notebook.\n",
    "\n",
    "L'avantage de réaliser cette étape maintenant est que les packages installés le seront sur l'ensemble des machines du cluster.\n",
    "\n",
    "La procédure pour créer le fichier **bootstrap** qui contient l'ensemble des instructions permettant d'installer tous les packages dont nous aurons besoin est expliqué dans le cours [Réalisez des calculs distribués sur des données massives / Bootstrapping](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues#/id/r-4356490)\n",
    "\n",
    "Nous créons donc un fichier nommé `bootstrap-emr.sh` que nous téléversons sur S3 (à la racine du compartiment `pepper-bucket`) et nous l'ajoutons comme indiqué dans la capture d'écran ci-dessous:\n",
    "\n",
    "```sh\n",
    "$ aws s3 cp .\\bootstrap-emr.sh s3://pepper-bucket\n",
    "upload: .\\bootstrap-emr.sh to s3://pepper-bucket/bootstrap-emr.sh\n",
    "```\n",
    "\n",
    "![Actions d’amorçage](../baseline/img/EMR_amorcage.png)\n",
    "\n",
    "Voici le contenu du fichier `bootstrap-emr.sh`\n",
    "\n",
    "Il s'agit d'une séquence de commandes `pip install` pour installer les bibliothèques manquantes comme réalisé en local.\n",
    "\n",
    "Il est nécessaire de réaliser ces actions à cette étape pour que les packages soient installés sur l'ensemble des machines du cluster et non pas uniquement sur le driver, comme cela serait le cas si nous exécutions ces commandes directement dans le notebook JupyterHub ou dans la console EMR (connecté au driver).\n",
    "\n",
    "Ces actions d'amorçage sont exécutées avant l'installation des applications et avant qu'Amazon EMR ne commence à traiter les données. En cas d'ajout de nouveaux nœuds à un cluster en cours d'exécution, ces actions d'amorçage seront également exécutées sur ces nouveaux nœuds.\n",
    "\n",
    "En revanche, l'action d'amorçage n'est effectuée qu'une fois sur chaque nœud. Pour modifier la configuration d'amorçage, il faut donc résilier l'instance EMR et lancer un nouveau cluster. \n",
    "\n",
    "```sh\n",
    "#!bin/bash\n",
    "sudo python3 -m pip install -U setuptools\n",
    "sudo python3 -m pip install -U pip\n",
    "sudo python3 -m pip install wheel\n",
    "sudo python3 -m pip install pillow\n",
    "sudo python3 -m pip install pandas\n",
    "sudo python3 -m pip install pyarrow\n",
    "sudo python3 -m pip install boto3\n",
    "sudo python3 -m pip install s3fs\n",
    "sudo python3 -m pip install fsspec\n",
    "```\n",
    "\n",
    "\n",
    "* **`setuptools`** et **`pip`** doivent être mis à jour pour éviter un problème avec l'installation de **`pyarrow`**.\n",
    "* **`Pandas`** a eu droit à une mise à jour majeure (`1.3.0`) il y a moins d'une semaine au moment de la rédaction de ce notebook, et cette nouvelle version de **`Pandas`** dépend d'une version plus récente de **`Numpy`** que la version installée par défaut (`1.16.5`) lors de l'initialisation des instances **EC2**. <u>Il ne semble pas possible d'imposer une autre version de Numpy que celle installé par défaut</u> même si on force l'installation d'une version récente de **Numpy** (en tout cas, ni simplement ni intuitivement).\n",
    "\n",
    "La mise à jour étant très récente <u>la version de **Numpy** n'est pas encore mise à jour sur **EC2**</u> mais on peut imaginer que ce sera le cas très rapidement et il ne sera plus nécessaire d'imposer une version spécifique de **Pandas**.\n",
    "\n",
    "En attendant, je demande <u>l'installation de l'avant dernière version de **Pandas (1.2.5)**</u>\n",
    "\n",
    "On clique ensuite sur `Suivant`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.4 Étape 4 : Sécurité\n",
    "\n",
    "#### 4.6.4.1 Options de sécurité\n",
    "\n",
    "A cette étape nous sélectionnons la **paire de clés EC2** créée précédemment.\n",
    "\n",
    "Elle nous permettra de nous connecter en `ssh` à nos **instances EC2** sans avoir à saisir nos login et mot de passe.\n",
    "\n",
    "Laissons les autres paramètres à leur valeur par défaut.\n",
    "\n",
    "Cliquons enfin sur `Créer un cluster`.\n",
    " \n",
    "![EMR Sécurité](../baseline/img/EMR_securite.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Instanciation du serveur\n",
    "\n",
    "Il ne nous reste plus qu'à attendre que le serveur soit prêt.\n",
    "\n",
    "Cette étape peut prendre entre **15 et 20 minutes**.\n",
    "\n",
    "Plusieurs étapes s'enchaînent et on peut suivre l'évolution du statut du **cluster EMR** :\n",
    "\n",
    "![Instanciation étape 1](../baseline/img/EMR_instanciation_01.png)\n",
    "![Instanciation étape 2](../baseline/img/EMR_instanciation_02.png)\n",
    "![Instanciation étape 3](../baseline/img/EMR_instanciation_03.png)\n",
    "\n",
    "Lorsque le statut affiche en vert `\"En attente\"` cela signifie que l'instanciation s'est bien déroulée et que notre serveur est prêt à être utilisé. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Création du tunnel SSH à l'instance EC2 (Maître)\n",
    "\n",
    "### 4.8.1 Création des autorisations sur les connexions entrantes\n",
    "\n",
    "Nous souhaitons maintenant pouvoir accéder aux applications :\n",
    "* **JupyterHub**, pour l'exécution de notre notebook ;\n",
    "* **Serveur d'historique Spark**, pour le suivi de l'exécution des tâches de notre script lorsqu'il sera lancé.\n",
    " \n",
    "Ces applications ne sont accessibles que depuis le réseau local du pilote.\n",
    "\n",
    "Pour y accéder nous devons donc **créer un tunnel SSH vers le pilote**.\n",
    "\n",
    "Par défaut, ce driver se situe derrière un pare-feu qui bloque l'accès SSH.\n",
    "\n",
    "Pour ouvrir le port `22` sur lequel écoute le serveur SSH, il faut modifier le **groupe de sécurité EC2 du driver**.\n",
    "\n",
    "Cette étape est décrite dans le cours [Réalisez des calculs distribués sur des données massives / Lancement d'une application à partir du driver](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues#/id/r-4356512): \n",
    "\n",
    "Sur la page de la console consacrée à EC2, dans l'onglet \"Réseau et sécurité\", cliquez sur \"Groupes de sécurité\".\n",
    "Vous allez devoir modifier le groupe de sécurité d’ElasticMapReduce-Master. \n",
    "\n",
    "Dans l'onglet \"Entrant\", ajoutez une règle SSH dont la source est \"N'importe où\" (ou \"Mon IP\" si vous disposez d'une adresse IP fixe).\n",
    "\n",
    "![Configuration autorisation ports entrants pour ssh](../baseline/img/EMR_config_ssh_01.png)\n",
    "\n",
    "Une fois cette étape réalisée vous devriez avoir une configuration semblable à la mienne :\n",
    "\n",
    "![Configuration ssh terminée](../baseline/img/EMR_config_ssh_02.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.2 Création du tunnel ssh vers le Driver\n",
    "\n",
    "On peut maintenant établir le **tunnel SSH** vers le **Pilote**.\n",
    "\n",
    "Pour cela on récupère les informations de connexion fournis par Amazon depuis la page du service EMR / Cluster / onglet Récapitulatif en cliquant sur \"**Activer la connexion Web**\".\n",
    "\n",
    "![Activer la connexion Web](../baseline/img/EMR_tunnel_ssh_01.png)\n",
    "\n",
    "<u>On récupère ensuite la commande fournie par Amazon pour **établir le tunnel SSH**</u> :\n",
    "\n",
    "![Récupérer la commande pour établir le tunnel ssh](../baseline/img/EMR_tunnel_ssh_02.png)\n",
    "\n",
    "<u>Dans mon cas, la commande ne fonctionne pas telle</u> quelle et j'ai du **l'adapter à ma configuration**. <br />\n",
    "\n",
    "La **clé ssh** se situe dans un dossier `.ssh` elle-même située dans mon **répertoire personnel** dont le symbole est, sous Linux, identifié par un tilde `~`.\n",
    "\n",
    "Ayant suivi le cours [Réalisez des calculs distribués sur des données massives / Lancement d'une application à partir du driver](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives)\n",
    "\n",
    "* j'ai choisi d'utiliser le port `5555` au lieu du `8157`, même si le choix n'est pas très important.\n",
    "* j'ai également rencontré un <u>problème de compatibilité</u> avec l'argument `-N` (la liste des arguments et leur significations disponibles [ici](https://explainshell.com/explain?cmd=ssh+-L+-N+-f+-l+-D)) et j'ai décidé de simplement le supprimer.\n",
    "\n",
    "<u>Finalement, j'utilise la commande suivante dans un terminal pour établir mon tunnel SSH (seule l'URL change d'une instance à une autre)</u> :\n",
    "\n",
    "```sh\n",
    "$ ssh -i ~/.ssh/p8-ec2.pem -D 5555 hadoop@ec2-35-180-91-39.eu-west-3.compute.amazonaws.com\n",
    "```\n",
    "\n",
    "<u>On inscrit `yes` pour valider la connexion et si la connexion est établie on obtient le résultat suivant</u> :\n",
    "\n",
    "![Création du tunnel SSH](../baseline/img/EMR_connexion_ssh_01.png)\n",
    "\n",
    "Nous avons **correctement établi le tunnel ssh avec le driver** sur le port `5555`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.3 Configuration de FoxyProxy\n",
    "\n",
    "Une dernière étape est nécessaire pour accéder à nos applications, en demandant à notre navigateur d'emprunter le tunnel ssh.\n",
    "\n",
    "J'utilise pour cela **FoxyProxy**.\n",
    "\n",
    "[Une fois encore, vous pouvez utiliser le cours pour le configurer](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308701-realisez-la-maintenance-dun-cluster#/id/r-4356554).\n",
    "\n",
    "Sinon, ouvrez la configuration de **FoxyProxy** et <u>cliquez sur **Ajouter**</u> en haut à gauche puis renseigner les éléments comme dans la capture ci-dessous :\n",
    "\n",
    "![Configuration FoxyProxy Etape 1](../baseline/img/EMR_foxyproxy_config_01.png)\n",
    "\n",
    "<u>On obtient le résultat ci-dessous</u> :\n",
    "\n",
    "![Configuration FoxyProxy Etape 2](../baseline/img/EMR_foxyproxy_config_02.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.4 Accès aux applications du serveur EMR via le tunnel ssh\n",
    "\n",
    "<u>Avant d'établir notre **tunnel ssh** nous avions ça</u> :\n",
    "\n",
    "![avant tunnel ssh](../baseline/img/EMR_tunnel_ssh_avant.png)\n",
    "\n",
    "<u>On active le **tunnel ssh** comme vu précédemment puis on demande à notre navigateur de l'utiliser avec **FoxyProxy**</u> :\n",
    "\n",
    "![FoxyProxy activation](../baseline/img/EMR_foxyproxy_activation.png)\n",
    "\n",
    "<u>On peut maintenant s'apercevoir que plusieurs applications nous sont accessibles</u> :\n",
    "\n",
    "![avant tunnel ssh](../baseline/img/EMR_tunnel_ssh_apres.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Connexion au notebook JupyterHub\n",
    "\n",
    "Pour se connecter à **JupyterHub** en vue d'exécuter notre **notebook**, il faut commencer par <u>cliquer sur l'application **JupyterHub**</u> apparue depuis que nous avons configuré le **tunnel ssh** et **foxyproxy** sur notre navigateur (actualisez la page si ce n’est pas le cas).\n",
    "\n",
    "![Démarrage de JupyterHub](../baseline/img/EMR_jupyterhub_connexion_01.png)\n",
    "\n",
    "On passe les éventuels avertissements de sécurité puis nous arrivons sur une page de connexion.\n",
    "\n",
    "<u>On se connecte avec les informations par défaut</u> :\n",
    " - <u>login</u>: **jovyan**\n",
    " - <u>password</u>: **jupyter**\n",
    " \n",
    "![Connexion à JupyterHub](../baseline/img/EMR_jupyterhub_connexion_02.png)\n",
    "\n",
    "Nous arrivons ensuite dans un dossier vierge de notebook.\n",
    "\n",
    "Il suffit d'en créer un en cliquant sur \"**New**\" en haut à droite.\n",
    "\n",
    "![Liste et création des notebook](../baseline/img/EMR_jupyterhub_creer_notebooks.png)\n",
    "\n",
    "Il est également possible d'en <u>uploader un directement dans notre **bucket S3**</u>.\n",
    "\n",
    "Grace à la <u>**persistance** paramétrée à l'instanciation du cluster nous sommes actuellement dans l'arborescence de notre **bucket S3**</u>\n",
    "\n",
    "![Notebook stockés sur S3](../baseline/img/EMR_jupyterhub_S3.png)\n",
    "\n",
    "Je décide d'**importer un notebook déjà rédigé en local directement sur S3** et je l'ouvre depuis **l'interface JupyterHub**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 Exécution du code\n",
    "\n",
    "Je décide d'exécuter cette partie du code depuis **JupyterHub hébergé sur notre cluster EMR**.\n",
    "\n",
    "Pour ne pas alourdir inutilement les explications du **notebook**, je ne réexpliquerai pas les étapes communes que nous avons déjà vues dans la première partie où l'on a exécuté le code localement sur notre machine virtuelle Ubuntu.\n",
    "\n",
    "<u>Avant de commencer</u>, il faut s'assurer d'utiliser le **kernel pyspark**.\n",
    "\n",
    "**En utilisant ce kernel, une session spark est créée à l'exécution de la première cellule**.\n",
    "\n",
    "Il n'est donc **plus nécessaire d'exécuter le code `spark = (SparkSession ...`** comme lors de l'exécution de notre notebook en local sur notre VM Ubuntu."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10.1 Démarrage de la session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1626050279029_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-10-195.eu-west-3.compute.internal:20888/proxy/application_1626050279029_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-2-58.eu-west-3.compute.internal:8042/node/containerlogs/container_1626050279029_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# L'exécution de cette cellule démarre l'application Spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Affichage des informations sur la session en cours et liens vers Spark UI</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1000M', 'executorCores': 2, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1626050279029_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-10-195.eu-west-3.compute.internal:20888/proxy/application_1626050279029_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-2-58.eu-west-3.compute.internal:8042/node/containerlogs/container_1626050279029_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10.2 Installation des packages\n",
    "\n",
    "Les packages nécessaires ont été installé via l'étape de **bootstrap** à l'instanciation du serveur.\n",
    "\n",
    "### 4.10.3 Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10.4 Définition des PATH pour charger les images et enregistrer les résultats\n",
    "\n",
    "Nous accédons directement à nos **données sur S3** comme si elles étaient **stockées localement**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version d'origine de l'alternant\n",
    "\n",
    "```python\n",
    ">>> PATH = 's3://p8-data'\n",
    ">>> PATH_Data = PATH+'/Test'\n",
    ">>> PATH_Result = PATH+'/Results'\n",
    ">>> print('PATH:        '+\\\n",
    ">>>       PATH+'\\nPATH_Data:   '+\\\n",
    ">>>       PATH_Data+'\\nPATH_Result: '+PATH_Result)\n",
    "PATH:        s3://p8-data\n",
    "PATH_Data:   s3://p8-data/Test\n",
    "PATH_Result: s3://p8-data/Results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket_path: s3://p8-data\n",
      " input_path: s3://p8-data/Test\n",
      "output_path: s3://p8-data/Results\n"
     ]
    }
   ],
   "source": [
    "bucket_path = \"s3://p8-data\"\n",
    "input_path = f\"{bucket_path}/Test\"\n",
    "output_path = f\"{bucket_path}/Results\"\n",
    "print(\"bucket_path:\", bucket_path)\n",
    "print(\" input_path:\", input_path)\n",
    "print(\"output_path:\", output_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10.5 Traitement des données"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.10.5.1 Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+\n",
      "|                path|   modificationTime|length|             content|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "|s3://p8-data/Test...|2021-07-03 09:00:08|  7353|[FF D8 FF E0 00 1...|\n",
      "|s3://p8-data/Test...|2021-07-03 09:00:08|  7350|[FF D8 FF E0 00 1...|\n",
      "|s3://p8-data/Test...|2021-07-03 09:00:08|  7349|[FF D8 FF E0 00 1...|\n",
      "|s3://p8-data/Test...|2021-07-03 09:00:08|  7348|[FF D8 FF E0 00 1...|\n",
      "|s3://p8-data/Test...|2021-07-03 09:00:09|  7328|[FF D8 FF E0 00 1...|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "images.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Je ne conserve que le **path** de l'image et j'ajoute <br />\n",
    "    une colonne contenant les **labels** de chaque image</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n",
      "None\n",
      "+------------------------------------------+----------+\n",
      "|path                                      |label     |\n",
      "+------------------------------------------+----------+\n",
      "|s3://p8-data/Test/Watermelon/r_106_100.jpg|Watermelon|\n",
      "|s3://p8-data/Test/Watermelon/r_109_100.jpg|Watermelon|\n",
      "|s3://p8-data/Test/Watermelon/r_108_100.jpg|Watermelon|\n",
      "|s3://p8-data/Test/Watermelon/r_107_100.jpg|Watermelon|\n",
      "|s3://p8-data/Test/Watermelon/r_95_100.jpg |Watermelon|\n",
      "+------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None"
     ]
    }
   ],
   "source": [
    "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n",
    "print(images.printSchema())\n",
    "print(images.select('path','label').show(5,False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.10.5.2 Préparation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
      "\r\n",
      "    8192/14536120 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      " 4202496/14536120 [=======>......................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      "14540800/14536120 [==============================] - 0s 0us/step"
     ]
    }
   ],
   "source": [
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "broadcast_weights = sc.broadcast(new_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 112, 112, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 112, 112, 32) 128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, 112, 112, 32) 0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 112, 112, 32) 288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 112, 112, 32) 128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, 112, 112, 32) 0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 112, 112, 16) 512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 112, 112, 16) 64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 112, 112, 96) 1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 112, 112, 96) 384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, 112, 112, 96) 0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_pad (ZeroPadding2D)     (None, 113, 113, 96) 0           block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 56, 56, 96)   864         block_1_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 56, 56, 96)   384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, 56, 56, 96)   0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 56, 56, 24)   2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 56, 56, 24)   96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 56, 56, 144)  3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 56, 56, 144)  1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 56, 56, 144)  576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, 56, 56, 144)  0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 56, 56, 24)   3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 56, 56, 24)   96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 56, 56, 24)   0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 56, 56, 144)  3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_pad (ZeroPadding2D)     (None, 57, 57, 144)  0           block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 28, 28, 144)  1296        block_3_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 28, 28, 144)  576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, 28, 28, 144)  0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 28, 28, 32)   4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 28, 28, 32)   128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 28, 28, 192)  6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 28, 28, 32)   6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 28, 28, 32)   128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 28, 28, 32)   0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 28, 28, 192)  6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 28, 28, 32)   6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 28, 28, 32)   128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 28, 28, 32)   0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, 28, 28, 192)  6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_pad (ZeroPadding2D)     (None, 29, 29, 192)  0           block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, 14, 14, 192)  1728        block_6_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, 14, 14, 192)  768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, 14, 14, 192)  0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, 14, 14, 64)   12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, 14, 14, 64)   256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, 14, 14, 384)  24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, 14, 14, 64)   24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, 14, 14, 64)   256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, 14, 14, 64)   0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, 14, 14, 384)  24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, 14, 14, 64)   24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, 14, 14, 64)   256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, 14, 14, 64)   0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, 14, 14, 384)  24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, 14, 14, 64)   24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, 14, 14, 64)   256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, 14, 14, 64)   0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, 14, 14, 384)  24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, 14, 14, 384)  1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, 14, 14, 384)  0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, 14, 14, 384)  3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, 14, 14, 384)  1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, 14, 14, 96)   36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, 14, 14, 96)   384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, 14, 14, 576)  55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, 14, 14, 96)   55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, 14, 14, 96)   384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, 14, 14, 96)   0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, 14, 14, 576)  55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, 14, 14, 96)   55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, 14, 14, 96)   384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, 14, 14, 96)   0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, 14, 14, 576)  55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_pad (ZeroPadding2D)    (None, 15, 15, 576)  0           block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, 7, 7, 576)    5184        block_13_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, 7, 7, 576)    2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)    0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, 7, 7, 160)    92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, 7, 7, 160)    640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, 7, 7, 960)    153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, 7, 7, 160)    153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, 7, 7, 160)    640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, 7, 7, 160)    0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, 7, 7, 960)    153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, 7, 7, 160)    153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, 7, 7, 160)    640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, 7, 7, 160)    0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, 7, 7, 960)    153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, 7, 7, 320)    307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, 7, 7, 320)    1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 7, 7, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1280)         0           out_relu[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 2,223,872\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(broadcast_weights.value)\n",
    "    return new_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.10.5.3 Définition du processus de chargement des images <br/> et application de leur featurisation à travers l'utilisation de pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/functions.py:392: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details."
     ]
    }
   ],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.10.5.4 Exécutions des actions d'extractions de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_df = images.repartition(24).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\")\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://p8-data/Results"
     ]
    }
   ],
   "source": [
    "print(PATH_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_df.write.mode(\"overwrite\").parquet(PATH_Result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10.6 Chargement des données enregistrées et validation du résultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_parquet(PATH_Result, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           path  ...                                           features\n",
      "0    s3://p8-data/Test/Watermelon/r_174_100.jpg  ...  [0.0059991637, 0.44703647, 0.0, 0.0, 3.3713572...\n",
      "1  s3://p8-data/Test/Pineapple Mini/128_100.jpg  ...  [0.0146466885, 4.080593, 0.055877004, 0.0, 0.0...\n",
      "2  s3://p8-data/Test/Pineapple Mini/137_100.jpg  ...  [0.0, 4.9659867, 0.0, 0.0, 0.0, 0.0, 0.5144821...\n",
      "3      s3://p8-data/Test/Watermelon/275_100.jpg  ...  [0.22511952, 0.07235509, 0.0, 0.0, 1.690149, 0...\n",
      "4      s3://p8-data/Test/Watermelon/271_100.jpg  ...  [0.3286234, 0.18830013, 0.0, 0.0, 1.9123534, 0...\n",
      "\n",
      "[5 rows x 3 columns]"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280,)"
     ]
    }
   ],
   "source": [
    "df.loc[0,'features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22688, 3)"
     ]
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut également constater la présence des fichiers au format \"**parquet**\" sur le **serveur S3** :\n",
    "\n",
    "![Affichage des résultats sur S3](../baseline/img/S3_Results.png)\n",
    "\n",
    "## 4.11 Suivi de l'avancement des tâches avec le Serveur d'Historique Spark\n",
    "\n",
    "Il est possible de voir l'avancement des tâches en cours avec le **serveur d'historique Spark**.\n",
    "\n",
    "![Accès au serveur d'historique spark](../baseline/img/EMR_serveur_historique_spark_acces.png)\n",
    "\n",
    "**Il est également possible de revenir et d'étudier les tâches qui ont été réalisé, afin de debugger, optimiser les futurs tâches à réaliser.**\n",
    "\n",
    "Lorsque la commande `features_df.write.mode(\"overwrite\").parquet(PATH_Result)` était en cours, nous pouvions observer son état d'avancement :\n",
    "\n",
    "![Progression execution script](../baseline/img/EMR_jupyterhub_avancement.png)\n",
    "\n",
    "Le **serveur d'historique Spark** nous permet une vision beaucoup plus précise de l'exécution des différentes tâche sur les différentes machines du cluster :\n",
    "\n",
    "![Suivi des tâches spark](../baseline/img/EMR_SHSpark_01.png)\n",
    "\n",
    "On peut également constater que notre cluster de calcul a mis un tout petit peu **moins de 8 minutes** pour traiter les **22 688 images**.\n",
    "\n",
    "![Temps de traitement](../baseline/img/EMR_SHSpark_02.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.12 Résiliation de l'instance EMR\n",
    "\n",
    "Notre travail est maintenant terminé.\n",
    "\n",
    "Le cluster de machines EMR est **facturé à la demande**, et nous continuons d'être facturé même lorsque les machines sont au repos.\n",
    "\n",
    "Pour **optimiser la facturation**, il nous faut maintenant **résilier le cluster**.\n",
    "\n",
    "Je réalise cette commande depuis l'interface AWS :\n",
    "\n",
    "1. Commencez par **désactiver le tunnel ssh dans FoxyProxy** pour éviter des problèmes de **timeout**.\n",
    "![Désactivation de FoxyProxy](../baseline/img/EMR_foxyproxy_desactivation.png)\n",
    "2. Cliquez sur \"**Résilier**\"\n",
    "![Cliquez sur Résilier](../baseline/img/EMR_resiliation_01.png)\n",
    "3. Confirmez la résiliation\n",
    "![Confirmez la résiliation](../baseline/img/EMR_resiliation_02.png)\n",
    "4. La résiliation prend environ **1 minute**\n",
    "![Résiliation en cours](../baseline/img/EMR_resiliation_03.png)\n",
    "5. La résiliation est effectuée\n",
    "![Résiliation terminée](../baseline/img/EMR_resiliation_04.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.13 Cloner le serveur EMR (si besoin)\n",
    "\n",
    "Si nous devons de nouveau exécuter notre notebook dans les mêmes conditions, il nous suffit de **cloner notre cluster** et ainsi en obtenir une copie fonctionnelle sous 15/20 minutes, le temps de son instanciation.\n",
    "\n",
    "<u>Pour cela deux solutions</u> :\n",
    "1. <u>Depuis l'interface AWS</u> :\n",
    "    1. Cliquez sur \"**Cloner**\"\n",
    "    ![Cloner un cluster](../baseline/img/EMR_cloner_01.png)\n",
    "    2. Dans notre cas nous ne souhaitons pas inclure d'étapes\n",
    "    ![Ne pas inclure d'étapes](../baseline/img/EMR_cloner_02.png)\n",
    "    3. La configuration du cluster est recréée à l’identique.\n",
    "        * On peut revenir sur les différentes étapes si on souhaite apporter des modifications\n",
    "        * Quand tout est prêt, cliquez sur \"**Créer un cluster**\"\n",
    "    ![Vérification/Modification/Créer un cluster](../baseline/img/EMR_cloner_03.png)\n",
    "2. <u>En ligne de commande</u> (avec AWS CLI d'installé et de configuré et en s'assurant de s'attribuer les droits nécessaires sur le compte AMI utilisé)\n",
    "    1. Cliquez sur \"**Exporter AWS CLI**\"\n",
    "    ![Exporter AWS CLI](../baseline/img/EMR_cloner_cli_01.png)\n",
    "    2. Copier/Coller la commande **depuis un terminal**\n",
    "    ![Copier Coller Commande](../baseline/img/EMR_cloner_cli_02.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.14 Arborescence du serveur S3 à la fin du projet\n",
    "\n",
    "Pour information, voici **l'arborescence complète de mon bucket S3 p8-data** à la fin du projet :\n",
    "\n",
    "*Par soucis de lisibilité, nous ne listons pas les 131 sous dossiers du répertoire \"Test\"*\n",
    "\n",
    "```sh\n",
    "1. Results/_SUCCESS\n",
    "1. Results/part-00000-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00001-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00002-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00003-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00004-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00005-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00006-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00007-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00008-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00009-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00010-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00011-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00012-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00013-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00014-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00015-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00016-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00017-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00018-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00019-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00020-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00021-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00022-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00023-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Test/\n",
    "1. bootstrap-emr.sh\n",
    "1. jupyter-s3-conf.json\n",
    "1. jupyter/jovyan/.s3keep\n",
    "1. jupyter/jovyan/P8_01_Notebook.ipynb\n",
    "1. jupyter/jovyan/_metadata\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/file-perm.sqlite\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/html/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/latex/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbsignatures.db\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/notebook_secret\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/Untitled-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/Untitled1-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/test3-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/Untitled.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/Untitled1.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/test3.ipynb\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5\\. Conclusion\n",
    "\n",
    "Nous avons réalisé ce projet **en deux temps** en tenant compte des contraintes qui nous ont été imposées.\n",
    "\n",
    "Nous avons **dans un premier temps développé notre solution en local** sur une machine virtuelle dans un environnement Linux Ubuntu.\n",
    "\n",
    "La <u>première phase</u> a consisté à **installer l'environnement de travail Spark**.\n",
    "\n",
    "**Spark** a un paramètre qui nous permet de travailler en local et nous permet ainsi de **simuler du calcul partagé** en considérant **chaque cœur d'un processeur comme un worker indépendant**.\n",
    "\n",
    "Nous avons travaillé sur un plus **petit jeu de donnée**, l'idée était simplement de **valider le bon fonctionnement de la solution**.\n",
    "\n",
    "Nous avons fait le choix de réaliser du **transfert learning** à partir du model **MobileNetV2**.\n",
    "\n",
    "Ce modèle a été retenu pour sa **légèreté** et sa **rapidité d'exécution** ainsi que pour la **faible dimension de son vecteur en sortie**.\n",
    "\n",
    "Les résultats ont été enregistrés sur disque en plusieurs partitions au format \"**parquet**\".\n",
    "\n",
    "**La solution a parfaitement fonctionné en mode local**.\n",
    "\n",
    "La <u>deuxième phase</u> a consisté à créer un **réel cluster de calculs**.\n",
    "\n",
    "L'objectif était de pouvoir **anticiper une future augmentation de la charge de travail**.\n",
    "\n",
    "Le meilleur choix retenu a été l'utilisation du prestataire de services **Amazon Web Services** qui nous permet de **louer à la demande de la puissance de calculs**, pour un **coût tout à fait acceptable**.\n",
    "\n",
    "Ce service se nomme **EC2** et se classe parmi les offres **Infrastructure as a Service** (IaaS).\n",
    "\n",
    "Nous sommes allez plus loin en utilisant un service de plus haut niveau (**Plateforme as a Service** PaaS) en utilisant le service **`EMR`** qui nous permet d'un seul coup d'**instancier plusieurs serveurs (un cluster)** sur lesquels nous avons pu demander l'installation et la configuration de plusieurs programmes et librairies nécessaires à notre projet comme **`Spark`**, **`Hadoop`**, **`JupyterHub`** ainsi que la librairie **`TensorFlow`**.\n",
    "\n",
    "En plus d'être plus **rapide et efficace à mettre en place**, nous avons la **certitude du bon fonctionnement de la solution**, celle-ci ayant été préalablement validé par les ingénieurs d'Amazon.\n",
    "\n",
    "Nous avons également pu installer, sans difficulté, **les packages nécessaires sur l'ensembles des machines du cluster**.\n",
    "\n",
    "Enfin, avec très peu de modification, et plus simplement encore, nous avons pu **exécuter notre notebook comme nous l'avions fait localement**.\n",
    "\n",
    "Nous avons cette fois-ci exécuté le traitement sur **l'ensemble des images de notre dossier \"Test\"**.\n",
    "\n",
    "Nous avons opté pour le service **Amazon S3** pour **stocker les données de notre projet**.\n",
    "\n",
    "S3 offre, pour un faible coût, toutes les conditions dont nous avons besoin pour stocker et exploiter de manière efficace nos données.\n",
    "\n",
    "L'espace alloué est potentiellement **illimité**, mais les coûts seront fonction de l'espace utilisé.\n",
    "\n",
    "Il nous sera **facile de faire face à une montée de la charge de travail** en **redimensionnant** simplement notre cluster de machines (horizontalement et/ou verticalement au besoin), les coûts augmenteront en conséquence mais resteront nettement inférieurs aux coûts engendrés par l'achat de matériels ou par la location de serveurs dédiés."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
