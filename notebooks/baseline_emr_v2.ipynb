{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **D√©ployez un mod√®le dans le cloud**\n",
    "\n",
    "# Sommaire\n",
    "    \n",
    "* **‚úî 1. [Pr√©ambule](#introduction)**\n",
    "    * ‚úî 1.1 [Probl√©matique](#problem-statement)\n",
    "    * ‚úî 1.2 Objectifs dans ce projet\n",
    "    * ‚úî 1.3 D√©roulement des √©tapes du projet\n",
    "* **2. Choix techniques g√©n√©raux retenus**\n",
    "    * 2.1 Calcul distribu√©\n",
    "    * 2.2 Transfert Learning\n",
    "* **3. D√©ploiement de la solution en local**\n",
    "    * 3.1 Environnement de travail\n",
    "    * 3.2 Installation de Spark\n",
    "    * 3.3 Installation des packages\n",
    "    * 3.4 Import des librairies\n",
    "    * 3.5 D√©finition des PATH pour charger les images et enregistrer les r√©sultats\n",
    "    * 3.6 Cr√©ation de la SparkSession\n",
    "    * 3.7 Traitement des donn√©es\n",
    "        * 3.7.1 Chargement des donn√©es\n",
    "        * 3.7.2 Pr√©paration du mod√®le\n",
    "        * 3.7.3 D√©finition du processus de chargement des images et application de leur featurisation √† travers l'utilisation de pandas UDF\n",
    "        * 3.7.4 Ex√©cution des actions d'extractions de features\n",
    "    * 3.8 Chargement des donn√©es enregistr√©es et validation du r√©sultat\n",
    "* **4. D√©ploiement de la solution sur le cloud**\n",
    "    * 4.1 Choix du prestataire cloud : AWS\n",
    "    * 4.2 Choix de la solution technique : EMR\n",
    "    * 4.3 Choix de la solution de stockage des donn√©es : Amazon S3\n",
    "    * 4.4 Configuration de l'environnement de travail\n",
    "    * 4.5 Upload de nos donn√©es sur S3\n",
    "    * 4.6 Configuration du serveur EMR\n",
    "        * 4.6.1 √âtape 1 : Logiciels et √©tapes\n",
    "            * 4.6.1.1 Configuration des logiciels\n",
    "            * 4.6.1.2 Modifier les param√®tres du logiciel\n",
    "        * 4.6.2 √âtape 2 : Mat√©riel\n",
    "        * 4.6.3 √âtape 3 : Param√®tres de cluster g√©n√©raux\n",
    "            * 4.6.3.1 Options g√©n√©rales\n",
    "            * 4.6.3.2 Actions d'amor√ßage\n",
    "        * 4.6.4 √âtape 4 : S√©curit√©\n",
    "            * 4.6.4.1 Options de s√©curit√©\n",
    "    * 4.7 Instanciation du serveur\n",
    "    * 4.8 Cr√©ation du tunnel SSH √† l'instance EC2 (Ma√Ætre)\n",
    "        * 4.8.1 Cr√©ation des autorisations sur les connexions entrantes\n",
    "        * 4.8.2 Cr√©ation du tunnel ssh vers le Driver\n",
    "        * 4.8.3 Configuration de FoxyProxy\n",
    "        * 4.8.4 Acc√®s aux applications du serveur EMR via le tunnel ssh\n",
    "    * 4.9 Connexion au notebook JupyterHub\n",
    "    * 4.10 Ex√©cution du code\n",
    "        * 4.10.1 D√©marrage de la session Spark\n",
    "        * 4.10.2 Installation des packages\n",
    "        * 4.10.3 Import des librairies\n",
    "        * 4.10.4 D√©finition des PATH pour charger les images et enregistrer les r√©sultats\n",
    "        * 4.10.5 Traitement des donn√©es\n",
    "            * 4.10.5.1 Chargement des donn√©es\n",
    "            * 4.10.5.2 Pr√©paration du mod√®le\n",
    "            * 4.10.5.3 D√©finition du processus de chargement des images et application de leur featurisation √† travers l'utilisation de pandas UDF\n",
    "            * 4.10.5.4 Ex√©cutions des actions d'extractions de features\n",
    "        * 4.10.6 Chargement des donn√©es enregistr√©es et validation du r√©sultat\n",
    "    * 4.11 Suivi de l'avancement des t√¢ches avec le Serveur d'Historique Spark\n",
    "    * 4.12 R√©siliation de l'instance EMR\n",
    "    * 4.13 Cloner le serveur EMR (si besoin)\n",
    "    * 4.14 Arborescence du serveur S3 √† la fin du projet\n",
    "* **5. Conclusion**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"introduction\"></a> 1. **Pr√©ambule**\n",
    "\n",
    "## <a id=\"problem-statement\"></a> 1.1 Probl√©matique\n",
    "\n",
    "La tr√®s jeune start-up de l'AgriTech, nomm√©e \"**Fruits**!\", cherche √† proposer des solutions innovantes pour la r√©colte des fruits.\n",
    "\n",
    "La volont√© de l‚Äôentreprise est de **pr√©server la biodiversit√© des fruits** en permettant des traitements sp√©cifiques pour chaque esp√®ce de fruits en d√©veloppant des robots cueilleurs intelligents.\n",
    "\n",
    "La start-up souhaite dans un premier temps se faire conna√Ætre en mettant √† disposition du grand public une application mobile qui permettrait aux utilisateurs de prendre en photo un fruit et d'obtenir des informations sur ce fruit.\n",
    "\n",
    "Pour la start-up, cette application permettrait de **sensibiliser le grand public** √† la biodiversit√© des fruits et de mettre en place une premi√®re version du moteur de classification des images de fruits.\n",
    "\n",
    "De plus, le d√©veloppement de l‚Äô**application mobile** permettra de construire une premi√®re version de l'architecture **Big Data** n√©cessaire.\n",
    "\n",
    "## 1.2 Objectifs dans ce projet\n",
    "\n",
    "1. D√©velopper une premi√®re cha√Æne de traitement des donn√©es qui comprendra :\n",
    "   * le **preprocessing**,\n",
    "   * et une √©tape de **r√©duction de dimension**.\n",
    "2. Tenir compte du fait que le volume de donn√©es va augmenter tr√®s rapidement apr√®s la livraison de ce projet, ce qui implique de:\n",
    "   * D√©ployer le traitement des donn√©es dans un **environnement Big Data**\n",
    "   * D√©velopper les scripts en **pyspark** pour effectuer du **calcul distribu√©**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 D√©roulement des √©tapes du projet\n",
    "\n",
    "Le projet va √™tre r√©alis√© en 2 temps, dans deux environnements diff√©rents.\n",
    "\n",
    "Nous allons dans un premier temps d√©velopper et ex√©cuter notre code en local, en travaillant sur un nombre limit√© d'images √† traiter.\n",
    "\n",
    "Une fois les choix techniques valid√©s, nous d√©ploierons notre solution dans un environnement Big Data en mode distribu√©.\n",
    "\n",
    "<u>Pour cette raison, ce projet sera divis√© en 3 parties</u>:\n",
    "1. Liste des choix techniques g√©n√©raux retenus\n",
    "2. D√©ploiement de la solution en local\n",
    "3. D√©ploiement de la solution dans le cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\\. **Choix techniques g√©n√©raux retenus**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Calcul distribu√©\n",
    "\n",
    "L‚Äô√©nonc√© du projet nous impose de d√©velopper des scripts en **pyspark** afin de <u>prendre en compte l‚Äôaugmentation tr√®s rapide du volume de donn√© apr√®s la livraison du projet</u>.\n",
    "\n",
    "Pour comprendre rapidement et simplement ce qu‚Äôest **pyspark** et son principe de fonctionnement, nous vous conseillons de lire cet article : [PySpark : Tout savoir sur la librairie Python](https://datascientest.com/pyspark)\n",
    "\n",
    "<u>Le d√©but de l‚Äôarticle nous dit ceci </u>:\n",
    "\n",
    "¬´ *Lorsque l‚Äôon parle de traitement de bases de donn√©es sur python, on pense imm√©diatement √† la librairie pandas. Cependant, lorsqu‚Äôon a affaire √† des bases de donn√©es trop massives, les calculs deviennent trop lents. Heureusement, il existe une autre librairie python, assez proche de pandas, qui permet de traiter des tr√®s grandes quantit√©s de donn√©es : PySpark. Apache Spark est un framework open-source d√©velopp√© par l‚ÄôAMPLab de UC Berkeley permettant de traiter des bases de donn√©es massives en utilisant le calcul distribu√©, technique qui consiste √† exploiter plusieurs unit√©s de calcul r√©parties en clusters au profit d‚Äôun seul projet afin de diviser le temps d‚Äôex√©cution d‚Äôune requ√™te. \n",
    "Spark a √©t√© d√©velopp√© en Scala et est au meilleur de ses capacit√©s dans son langage natif. Cependant, la librairie PySpark propose de l‚Äôutiliser avec le langage Python, en gardant des performances similaires √† des impl√©mentations en Scala. Pyspark est donc une bonne alternative √† la librairie pandas lorsqu‚Äôon cherche √† traiter des jeux de donn√©es trop volumineux qui entra√Ænent des calculs trop chronophages.* ¬ª\n",
    "\n",
    "Comme nous le constatons, **pySpark** est un moyen de communiquer avec **Spark** via le langage **Python**.\n",
    "\n",
    "**Spark**, quant √† lui, est un outil qui permet de g√©rer et de coordonner l'ex√©cution de t√¢ches sur des donn√©es √† travers un groupe d'ordinateurs.\n",
    "\n",
    "<u>Spark (ou Apache Spark) est un framework open source de calcul distribu√© in-memory pour le traitement et l'analyse de donn√©es massives</u>.\n",
    "\n",
    "Un autre [article tr√®s int√©ressant et beaucoup plus complet pour comprendre le **fonctionnement de Spark**](https://www.veonum.com/apache-spark-pour-les-nuls/), ainsi que le r√¥le des **Spark Session** que nous utiliserons dans ce projet.\n",
    "\n",
    "<u>Voici √©galement un extrait</u>:\n",
    "\n",
    "*Les applications Spark se composent d‚Äôun pilote (¬´‚ÄØdriver process‚ÄØ¬ª) et de plusieurs ex√©cuteurs (¬´‚ÄØexecutor processes‚ÄØ¬ª). Il peut √™tre configur√© pour √™tre lui-m√™me l‚Äôex√©cuteur (local mode) ou en utiliser autant que n√©cessaire pour traiter l‚Äôapplication, Spark prenant en charge la mise √† l‚Äô√©chelle automatique par une configuration d‚Äôun nombre minimum et maximum d‚Äôex√©cuteurs.*\n",
    "\n",
    "![Sch√©ma de Spark](../baseline/img/spark-schema.png)\n",
    "\n",
    "*Le driver (parfois appel√© ¬´‚ÄØSpark Session‚ÄØ¬ª) distribue et planifie les t√¢ches entre les diff√©rents ex√©cuteurs qui les ex√©cutent et permettent un traitement r√©parti. Il est le responsable de l‚Äôex√©cution du code sur les diff√©rentes machines.<br />\n",
    "Chaque ex√©cuteur est un processus Java Virtual Machine (JVM) distinct dont il est possible de configurer le nombre de CPU et la quantit√© de m√©moire qui lui est allou√©.<br />\n",
    "Une seule t√¢che peut traiter un fractionnement de donn√©es √† la fois.*\n",
    "\n",
    "Dans les deux environnements (Local et Cloud) nous utiliserons donc **Spark** et nous l‚Äôexploiterons √† travers des scripts python gr√¢ce √† **PySpark**.\n",
    "\n",
    "Dans la <u>version locale</u> de notre script nous **simulerons le calcul distribu√©** afin de valider que notre solution fonctionne.\n",
    "\n",
    "Dans la <u>version cloud</u> nous **r√©aliserons les op√©rations sur un cluster de machines**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Transfert Learning\n",
    "\n",
    "L'√©nonc√© du projet nous demande √©galement de r√©aliser une premi√®re cha√Æne de traitement des donn√©es qui comprendra la **pr√©paration des donn√©es** et une √©tape de **r√©duction de dimensionnalit√©**.\n",
    "\n",
    "Il est √©galement pr√©cis√© qu'il n'est pas n√©cessaire d'entra√Æner un mod√®le pour le moment.\n",
    "\n",
    "Nous d√©cidons de partir sur une solution de **transfert learning**, qui consiste √† utiliser un mod√®le pr√©-entra√Æn√©  (ici **MobileNetV2**) pour r√©soudre un probl√®me similaire plus g√©n√©ral et √† l'adapter √† notre probl√©matique sp√©cifique.\n",
    "\n",
    "Nous allons fournir au mod√®le nos images, et nous allons <u>r√©cup√©rer l'avant derni√®re couche</u> du mod√®le.\n",
    "\n",
    "En effet la derni√®re couche de mod√®le est une couche **softmax** qui permet la classification des images ce que nous ne souhaitons pas dans ce projet.\n",
    "\n",
    "L'avant derni√®re couche correspond √† un **vecteur r√©duit** de dimension (1, 1, 1280).\n",
    "\n",
    "Cela permettra de r√©aliser une premi√®re version du moteur pour la classification des images des fruits.\n",
    "\n",
    "**MobileNetV2** a √©t√© retenu pour sa <u>rapidit√© d'ex√©cution</u>, particuli√®rement adapt√©e pour le traitement d'un gros volume de donn√©es ainsi que la <u>faible dimensionnalit√© du vecteur de caract√©ristique en sortie</u> (1, 1, 1280)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\\. D√©ploiement de la solution en local\n",
    "\n",
    "## 3.1 Environnement de travail\n",
    "\n",
    "Pour des raisons de simplicit√©, nous d√©veloppons dans un environnement Linux Ubuntu (ex√©cut√© depuis une machine Windows dans une machine virtuelle)\n",
    "* Pour installer une machine virtuelle :  https://www.malekal.com/meilleurs-logiciels-de-machine-virtuelle-gratuits-ou-payants/\n",
    "\n",
    "\n",
    "Compte tenu de l'usage dans l'entreprise de d√©velopper dans un environnement int√©gr√© de d√©veloppement (IDE) VS Code sur Windows, nous avons profit√© de ce projet pilote pour comparer 3 solutions pour le d√©veloppement et le test en local avant le d√©ploiement dans le cloud :\n",
    "1. Installation de Spark directement sur Windows, ce qui peux se faire avec la version 3.4.0 de Spark sans devoir n√©cessairement installer Winutils, l'interface Hadoop (HDFS) vers les fonctionnalit√©s natives de Windows.\n",
    "2. Installation de Spark sur un sous-syst√®me WSL Ubuntu 20.04, et mise en place d'une communication client-serveur en r√©seau local entre les deux environnements : d√©ploiement depuis les notebooks Jupyter/VS Code/Windows de scripts PySpark vers le master Spark h√©berg√© sur le syst√®me Ubuntu.\n",
    "3. R√©seau local physique avec cette fois-ci une communication en r√©seau priv√© avec une seconde machine √©quip√© du seul syst√®me Ubuntu.\n",
    "4. Utilisation des fonctionnalit√©s Remote Development de VS Code, avec docker / ssh / ... \n",
    "\n",
    "\n",
    "## 3.2 Installation de Spark\n",
    "\n",
    "[La premi√®re √©tape consiste √† installer Spark ](https://computingforgeeks.com/how-to-install-apache-spark-on-ubuntu-debian/)\n",
    "\n",
    "## 3.3 Installation des packages\n",
    "\n",
    "On installe ensuite √† l'aide du gestionnaire de paquets **pip** les packages qui nous seront n√©cessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Pandas pillow tensorflow pyspark pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pandas\n",
      "Version: 2.0.0\n",
      "Summary: Powerful data structures for data analysis, time series, and statistics\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: The Pandas Development Team <pandas-dev@python.org>\n",
      "License: BSD 3-Clause License\n",
      "\n",
      "Copyright (c) 2008-2011, AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team\n",
      "All rights reserved.\n",
      "\n",
      "Copyright (c) 2011-2023, Open source contributors.\n",
      "\n",
      "Redistribution and use in source and binary forms, with or without\n",
      "modification, are permitted provided that the following conditions are met:\n",
      "\n",
      "* Redistributions of source code must retain the above copyright notice, this\n",
      "  list of conditions and the following disclaimer.\n",
      "\n",
      "* Redistributions in binary form must reproduce the above copyright notice,\n",
      "  this list of conditions and the following disclaimer in the documentation\n",
      "  and/or other materials provided with the distribution.\n",
      "\n",
      "* Neither the name of the copyright holder nor the names of its\n",
      "  contributors may be used to endorse or promote products derived from\n",
      "  this software without specific prior written permission.\n",
      "\n",
      "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "\n",
      "Location: c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\n",
      "Requires: numpy, numpy, python-dateutil, pytz, tzdata\n",
      "Required-by: evidently, fastparquet, gspread-pandas, seaborn, statsmodels, xarray\n",
      "---\n",
      "Name: Pillow\n",
      "Version: 9.5.0\n",
      "Summary: Python Imaging Library (Fork)\n",
      "Home-page: https://python-pillow.org\n",
      "Author: Jeffrey A. Clark (Alex)\n",
      "Author-email: aclark@aclark.net\n",
      "License: HPND\n",
      "Location: c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\n",
      "Requires: \n",
      "Required-by: bokeh, matplotlib\n",
      "---\n",
      "Name: tensorflow\n",
      "Version: 2.12.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\n",
      "Requires: tensorflow-intel\n",
      "Required-by: \n",
      "---\n",
      "Name: pyspark\n",
      "Version: 3.4.0\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n",
      "---\n",
      "Name: pyarrow\n",
      "Version: 11.0.0\n",
      "Summary: Python library for Apache Arrow\n",
      "Home-page: https://arrow.apache.org/\n",
      "Author: \n",
      "Author-email: \n",
      "License: Apache License, Version 2.0\n",
      "Location: c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\n",
      "Requires: numpy\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show Pandas pillow tensorflow pyspark pyarrow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from keras.utils import img_to_array\n",
    "from keras import Model\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå Description des librairies et fonctions utilis√©es\n",
    "\n",
    "üìå **UDF** est l'acronyme de *User-Defined Function*.\n",
    "\n",
    "Description en fran√ßais :\n",
    "* **`pandas`** : biblioth√®que Python pour la manipulation et l'analyse de donn√©es.\n",
    "* **`PIL (Python Imaging Library)`** : biblioth√®que Python pour l'ouverture et la manipulation d'images.\n",
    "* **`numpy`** : biblioth√®que Python pour la manipulation de tableaux multidimensionnels.\n",
    "* **`io`** : biblioth√®que Python pour la gestion des entr√©es/sorties.\n",
    "* **`os`** : biblioth√®que Python pour l'interaction avec le syst√®me d'exploitation.\n",
    "* **`tensorflow`** : biblioth√®que open-source d'apprentissage automatique pour les donn√©es num√©riques et les r√©seaux de neurones.\n",
    "* **`MobileNetV2`** : mod√®le de r√©seau de neurones pr√©-entra√Æn√© pour la classification d'images.\n",
    "* **`preprocess_input`** : fonction de pr√©traitement pour les images avant l'utilisation de MobileNetV2.\n",
    "* **`img_to_array`** : fonction pour convertir une image PIL en tableau numpy.\n",
    "* **`Model`** : classe Keras pour la d√©finition de mod√®les d'apprentissage en profondeur.\n",
    "* **`pyspark`** : framework open-source pour le traitement de donn√©es en masse distribu√©es sur des clusters.\n",
    "* **`col`** : fonction pour s√©lectionner une colonne dans un DataFrame Spark.\n",
    "* **`pandas_udf`** : fonction pour ex√©cuter une fonction Pandas sur un DataFrame Spark.\n",
    "* **`PandasUDFType`** : enum pour sp√©cifier le type de la fonction PandasUDF.\n",
    "* **`element_at`** : fonction pour extraire l'√©l√©ment d'une liste √† une position donn√©e dans un DataFrame Spark.\n",
    "* **`split`** : fonction pour diviser une cha√Æne en une liste de sous-cha√Ænes en utilisant un d√©limiteur sp√©cifi√© dans un DataFrame Spark.\n",
    "* **`SparkSession`** : point d'entr√©e pour l'interaction avec les donn√©es dans Spark.\n",
    "\n",
    "Description en anglais :\n",
    "* **`pandas`**: Python library for data manipulation and analysis.\n",
    "* **`PIL (Python Imaging Library)`**: Python library for opening and manipulating images.\n",
    "* **`numpy`**: Python library for multidimensional array manipulation.\n",
    "* **`io`**: Python library for managing input/output operations.\n",
    "* **`os`**: Python library for interacting with the operating system.\n",
    "* **`tensorflow`**: open-source machine learning library for numerical data and neural networks.\n",
    "* **`MobileNetV2`**: pre-trained neural network model for image classification.\n",
    "* **`preprocess_input`**: pre-processing function for images before using MobileNetV2.\n",
    "* **`img_to_array`**: function to convert a PIL image to a numpy array.\n",
    "* **`Model`**: Keras class for defining deep learning models.\n",
    "* **`pyspark`**: open-source framework for processing distributed data on clusters.\n",
    "* **`col`**: function to select a column in a Spark DataFrame.\n",
    "* **`pandas_udf`**: function to execute a Pandas function on a Spark DataFrame.\n",
    "* **`PandasUDFType`**: enum to specify the type of PandasUDF function.\n",
    "* **`element_at`**: function to extract the element of a list at a given position in a Spark DataFrame.\n",
    "* **`split`**: function to split a string into a list of sub-strings using a specified delimiter in a Spark DataFrame.\n",
    "* **`SparkSession`**: entry point for interacting with data in Spark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 D√©finition des PATH pour charger les images <br /> et enregistrer les r√©sultats\n",
    "\n",
    "Dans cette version locale nous partons du principe que les donn√©es sont stock√©es dans le m√™me r√©pertoire que le notebook.\n",
    "\n",
    "Nous n'utilisons qu'un extrait de **300 images** √† traiter dans cette premi√®re version en local.\n",
    "\n",
    "L'extrait des images √† charger est stock√©e dans le dossier `data/im/sample_300`. <mark>**Test1**</mark>\n",
    "\n",
    "Nous enregistrerons le r√©sultat de notre traitement dans le dossier `tmp/prep/im/sample_300`. <mark>\"**Results_Local**\"</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöß **TODO** 2 choses √† faire ici :\n",
    "1. Former l'extrait\n",
    "2. Mettre en place mon env habituel (pepper.env)\n",
    "\n",
    "Pour extraire le sample 300 ?\n",
    "\n",
    "A priori, il faut prendre √©quitablement sur les 131 esp√®ces (de fruits ET l√©gumes). Donc, dans le cadre strict du projet, nous devrions ne prendre que les images de fruits. Mais clairement, c'est le type d'attention et de scrupule dont ils se f...\n",
    "\n",
    "Il y a environ 150 exemplaires par cat√©gorie.\n",
    "\n",
    "L'apprentissage n'est pas supervis√©, c'est du transfer learning.\n",
    "\n",
    "Donc on n'a pas √† conserver les labels donn√©s par les noms de dossiers.\n",
    "\n",
    "Faisons une premi√®re version, il sera toujours temps d'am√©liorer ensuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\Projects\\pepper_cloud_based_model\\data\\im\\sample_300\n",
      "C:\\Users\\franc\\Projects\\pepper_cloud_based_model\\tmp\\prep\\im\\sample_300\n"
     ]
    }
   ],
   "source": [
    "from fruits.env import get_data_im_sample_300_dir, get_prep_im_sample_300_dir\n",
    "\n",
    "print(get_data_im_sample_300_dir())\n",
    "print(get_prep_im_sample_300_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\franc\\\\Projects\\\\pepper_cloud_based_model\\\\dataset\\\\fruits-360_dataset\\\\Test'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Apple Braeburn/321_100.jpg',\n",
       " 'Apple Braeburn/322_100.jpg',\n",
       " 'Apple Braeburn/323_100.jpg',\n",
       " 'Apple Braeburn/324_100.jpg',\n",
       " 'Apple Braeburn/325_100.jpg',\n",
       " 'Apple Braeburn/326_100.jpg',\n",
       " 'Apple Braeburn/327_100.jpg',\n",
       " 'Apple Braeburn/32_100.jpg',\n",
       " 'Apple Braeburn/33_100.jpg',\n",
       " 'Apple Braeburn/34_100.jpg',\n",
       " 'Apple Braeburn/35_100.jpg',\n",
       " 'Apple Braeburn/36_100.jpg',\n",
       " 'Apple Braeburn/37_100.jpg',\n",
       " 'Apple Braeburn/38_100.jpg',\n",
       " 'Apple Braeburn/39_100.jpg',\n",
       " 'Apple Braeburn/3_100.jpg',\n",
       " 'Apple Braeburn/40_100.jpg',\n",
       " 'Apple Braeburn/41_100.jpg',\n",
       " 'Apple Braeburn/42_100.jpg',\n",
       " 'Apple Braeburn/43_100.jpg',\n",
       " 'Apple Braeburn/44_100.jpg',\n",
       " 'Apple Braeburn/45_100.jpg',\n",
       " 'Apple Braeburn/46_100.jpg',\n",
       " 'Apple Braeburn/47_100.jpg',\n",
       " 'Apple Braeburn/48_100.jpg',\n",
       " 'Apple Braeburn/49_100.jpg',\n",
       " 'Apple Braeburn/4_100.jpg',\n",
       " 'Apple Braeburn/50_100.jpg',\n",
       " 'Apple Braeburn/51_100.jpg',\n",
       " 'Apple Braeburn/52_100.jpg',\n",
       " 'Apple Braeburn/53_100.jpg',\n",
       " 'Apple Braeburn/54_100.jpg',\n",
       " 'Apple Braeburn/55_100.jpg',\n",
       " 'Apple Braeburn/56_100.jpg',\n",
       " 'Apple Braeburn/57_100.jpg',\n",
       " 'Apple Braeburn/58_100.jpg',\n",
       " 'Apple Braeburn/59_100.jpg',\n",
       " 'Apple Braeburn/5_100.jpg',\n",
       " 'Apple Braeburn/60_100.jpg',\n",
       " 'Apple Braeburn/61_100.jpg',\n",
       " 'Apple Braeburn/62_100.jpg',\n",
       " 'Apple Braeburn/63_100.jpg',\n",
       " 'Apple Braeburn/64_100.jpg',\n",
       " 'Apple Braeburn/65_100.jpg',\n",
       " 'Apple Braeburn/66_100.jpg',\n",
       " 'Apple Braeburn/67_100.jpg',\n",
       " 'Apple Braeburn/68_100.jpg',\n",
       " 'Apple Braeburn/69_100.jpg',\n",
       " 'Apple Braeburn/6_100.jpg',\n",
       " 'Apple Braeburn/70_100.jpg',\n",
       " 'Apple Braeburn/71_100.jpg',\n",
       " 'Apple Braeburn/72_100.jpg',\n",
       " 'Apple Braeburn/73_100.jpg',\n",
       " 'Apple Braeburn/74_100.jpg',\n",
       " 'Apple Braeburn/75_100.jpg',\n",
       " 'Apple Braeburn/76_100.jpg',\n",
       " 'Apple Braeburn/77_100.jpg',\n",
       " 'Apple Braeburn/78_100.jpg',\n",
       " 'Apple Braeburn/79_100.jpg',\n",
       " 'Apple Braeburn/7_100.jpg',\n",
       " 'Apple Braeburn/80_100.jpg',\n",
       " 'Apple Braeburn/81_100.jpg',\n",
       " 'Apple Braeburn/82_100.jpg',\n",
       " 'Apple Braeburn/83_100.jpg',\n",
       " 'Apple Braeburn/84_100.jpg',\n",
       " 'Apple Braeburn/85_100.jpg',\n",
       " 'Apple Braeburn/86_100.jpg',\n",
       " 'Apple Braeburn/87_100.jpg',\n",
       " 'Apple Braeburn/88_100.jpg',\n",
       " 'Apple Braeburn/89_100.jpg',\n",
       " 'Apple Braeburn/8_100.jpg',\n",
       " 'Apple Braeburn/90_100.jpg',\n",
       " 'Apple Braeburn/91_100.jpg',\n",
       " 'Apple Braeburn/92_100.jpg',\n",
       " 'Apple Braeburn/93_100.jpg',\n",
       " 'Apple Braeburn/94_100.jpg',\n",
       " 'Apple Braeburn/95_100.jpg',\n",
       " 'Apple Braeburn/96_100.jpg',\n",
       " 'Apple Braeburn/97_100.jpg',\n",
       " 'Apple Braeburn/98_100.jpg',\n",
       " 'Apple Braeburn/99_100.jpg',\n",
       " 'Apple Braeburn/9_100.jpg',\n",
       " 'Apple Braeburn/r_321_100.jpg',\n",
       " 'Apple Braeburn/r_322_100.jpg',\n",
       " 'Apple Braeburn/r_323_100.jpg',\n",
       " 'Apple Braeburn/r_324_100.jpg',\n",
       " 'Apple Braeburn/r_325_100.jpg',\n",
       " 'Apple Braeburn/r_326_100.jpg',\n",
       " 'Apple Braeburn/r_327_100.jpg',\n",
       " 'Apple Braeburn/r_32_100.jpg',\n",
       " 'Apple Braeburn/r_33_100.jpg',\n",
       " 'Apple Braeburn/r_34_100.jpg',\n",
       " 'Apple Braeburn/r_35_100.jpg',\n",
       " 'Apple Braeburn/r_36_100.jpg',\n",
       " 'Apple Braeburn/r_37_100.jpg',\n",
       " 'Apple Braeburn/r_38_100.jpg',\n",
       " 'Apple Braeburn/r_39_100.jpg',\n",
       " 'Apple Braeburn/r_3_100.jpg',\n",
       " 'Apple Braeburn/r_40_100.jpg',\n",
       " 'Apple Braeburn/r_41_100.jpg',\n",
       " 'Apple Braeburn/r_42_100.jpg',\n",
       " 'Apple Braeburn/r_43_100.jpg',\n",
       " 'Apple Braeburn/r_44_100.jpg',\n",
       " 'Apple Braeburn/r_45_100.jpg',\n",
       " 'Apple Braeburn/r_46_100.jpg',\n",
       " 'Apple Braeburn/r_47_100.jpg',\n",
       " 'Apple Braeburn/r_48_100.jpg',\n",
       " 'Apple Braeburn/r_49_100.jpg',\n",
       " 'Apple Braeburn/r_4_100.jpg',\n",
       " 'Apple Braeburn/r_50_100.jpg',\n",
       " 'Apple Braeburn/r_51_100.jpg',\n",
       " 'Apple Braeburn/r_52_100.jpg',\n",
       " 'Apple Braeburn/r_53_100.jpg',\n",
       " 'Apple Braeburn/r_54_100.jpg',\n",
       " 'Apple Braeburn/r_55_100.jpg',\n",
       " 'Apple Braeburn/r_56_100.jpg',\n",
       " 'Apple Braeburn/r_57_100.jpg',\n",
       " 'Apple Braeburn/r_58_100.jpg',\n",
       " 'Apple Braeburn/r_59_100.jpg',\n",
       " 'Apple Braeburn/r_5_100.jpg',\n",
       " 'Apple Braeburn/r_60_100.jpg',\n",
       " 'Apple Braeburn/r_61_100.jpg',\n",
       " 'Apple Braeburn/r_62_100.jpg',\n",
       " 'Apple Braeburn/r_63_100.jpg',\n",
       " 'Apple Braeburn/r_64_100.jpg',\n",
       " 'Apple Braeburn/r_65_100.jpg',\n",
       " 'Apple Braeburn/r_66_100.jpg',\n",
       " 'Apple Braeburn/r_67_100.jpg',\n",
       " 'Apple Braeburn/r_68_100.jpg',\n",
       " 'Apple Braeburn/r_69_100.jpg',\n",
       " 'Apple Braeburn/r_6_100.jpg',\n",
       " 'Apple Braeburn/r_70_100.jpg',\n",
       " 'Apple Braeburn/r_71_100.jpg',\n",
       " 'Apple Braeburn/r_72_100.jpg',\n",
       " 'Apple Braeburn/r_73_100.jpg',\n",
       " 'Apple Braeburn/r_74_100.jpg',\n",
       " 'Apple Braeburn/r_75_100.jpg',\n",
       " 'Apple Braeburn/r_76_100.jpg',\n",
       " 'Apple Braeburn/r_77_100.jpg',\n",
       " 'Apple Braeburn/r_78_100.jpg',\n",
       " 'Apple Braeburn/r_79_100.jpg',\n",
       " 'Apple Braeburn/r_7_100.jpg',\n",
       " 'Apple Braeburn/r_80_100.jpg',\n",
       " 'Apple Braeburn/r_81_100.jpg',\n",
       " 'Apple Braeburn/r_82_100.jpg',\n",
       " 'Apple Braeburn/r_83_100.jpg',\n",
       " 'Apple Braeburn/r_84_100.jpg',\n",
       " 'Apple Braeburn/r_85_100.jpg',\n",
       " 'Apple Braeburn/r_86_100.jpg',\n",
       " 'Apple Braeburn/r_87_100.jpg',\n",
       " 'Apple Braeburn/r_88_100.jpg',\n",
       " 'Apple Braeburn/r_89_100.jpg',\n",
       " 'Apple Braeburn/r_8_100.jpg',\n",
       " 'Apple Braeburn/r_90_100.jpg',\n",
       " 'Apple Braeburn/r_91_100.jpg',\n",
       " 'Apple Braeburn/r_92_100.jpg',\n",
       " 'Apple Braeburn/r_93_100.jpg',\n",
       " 'Apple Braeburn/r_94_100.jpg',\n",
       " 'Apple Braeburn/r_95_100.jpg',\n",
       " 'Apple Braeburn/r_96_100.jpg',\n",
       " 'Apple Braeburn/r_97_100.jpg',\n",
       " 'Apple Braeburn/r_98_100.jpg',\n",
       " 'Apple Braeburn/r_99_100.jpg',\n",
       " 'Apple Braeburn/r_9_100.jpg',\n",
       " 'Apple Crimson Snow/100_100.jpg',\n",
       " 'Apple Crimson Snow/101_100.jpg',\n",
       " 'Apple Crimson Snow/102_100.jpg',\n",
       " 'Apple Crimson Snow/103_100.jpg',\n",
       " 'Apple Crimson Snow/104_100.jpg',\n",
       " 'Apple Crimson Snow/105_100.jpg',\n",
       " 'Apple Crimson Snow/106_100.jpg',\n",
       " 'Apple Crimson Snow/107_100.jpg',\n",
       " 'Apple Crimson Snow/108_100.jpg',\n",
       " 'Apple Crimson Snow/109_100.jpg',\n",
       " 'Apple Crimson Snow/110_100.jpg',\n",
       " 'Apple Crimson Snow/112_100.jpg',\n",
       " 'Apple Crimson Snow/113_100.jpg',\n",
       " 'Apple Crimson Snow/114_100.jpg',\n",
       " 'Apple Crimson Snow/115_100.jpg',\n",
       " 'Apple Crimson Snow/116_100.jpg',\n",
       " 'Apple Crimson Snow/117_100.jpg',\n",
       " 'Apple Crimson Snow/118_100.jpg',\n",
       " 'Apple Crimson Snow/119_100.jpg',\n",
       " 'Apple Crimson Snow/120_100.jpg',\n",
       " 'Apple Crimson Snow/121_100.jpg',\n",
       " 'Apple Crimson Snow/122_100.jpg',\n",
       " 'Apple Crimson Snow/123_100.jpg',\n",
       " 'Apple Crimson Snow/124_100.jpg',\n",
       " 'Apple Crimson Snow/125_100.jpg',\n",
       " 'Apple Crimson Snow/126_100.jpg',\n",
       " 'Apple Crimson Snow/127_100.jpg',\n",
       " 'Apple Crimson Snow/128_100.jpg',\n",
       " 'Apple Crimson Snow/129_100.jpg',\n",
       " 'Apple Crimson Snow/130_100.jpg',\n",
       " 'Apple Crimson Snow/131_100.jpg',\n",
       " 'Apple Crimson Snow/132_100.jpg',\n",
       " 'Apple Crimson Snow/133_100.jpg',\n",
       " 'Apple Crimson Snow/134_100.jpg',\n",
       " 'Apple Crimson Snow/135_100.jpg',\n",
       " 'Apple Crimson Snow/136_100.jpg',\n",
       " 'Apple Crimson Snow/137_100.jpg',\n",
       " 'Apple Crimson Snow/138_100.jpg',\n",
       " 'Apple Crimson Snow/139_100.jpg',\n",
       " 'Apple Crimson Snow/140_100.jpg',\n",
       " 'Apple Crimson Snow/141_100.jpg',\n",
       " 'Apple Crimson Snow/142_100.jpg',\n",
       " 'Apple Crimson Snow/143_100.jpg',\n",
       " 'Apple Crimson Snow/144_100.jpg',\n",
       " 'Apple Crimson Snow/145_100.jpg',\n",
       " 'Apple Crimson Snow/146_100.jpg',\n",
       " 'Apple Crimson Snow/147_100.jpg',\n",
       " 'Apple Crimson Snow/148_100.jpg',\n",
       " 'Apple Crimson Snow/149_100.jpg',\n",
       " 'Apple Crimson Snow/150_100.jpg',\n",
       " 'Apple Crimson Snow/151_100.jpg',\n",
       " 'Apple Crimson Snow/152_100.jpg',\n",
       " 'Apple Crimson Snow/153_100.jpg',\n",
       " 'Apple Crimson Snow/154_100.jpg',\n",
       " 'Apple Crimson Snow/155_100.jpg',\n",
       " 'Apple Crimson Snow/80_100.jpg',\n",
       " 'Apple Crimson Snow/81_100.jpg',\n",
       " 'Apple Crimson Snow/83_100.jpg',\n",
       " 'Apple Crimson Snow/84_100.jpg',\n",
       " 'Apple Crimson Snow/85_100.jpg',\n",
       " 'Apple Crimson Snow/86_100.jpg',\n",
       " 'Apple Crimson Snow/87_100.jpg',\n",
       " 'Apple Crimson Snow/88_100.jpg',\n",
       " 'Apple Crimson Snow/89_100.jpg',\n",
       " 'Apple Crimson Snow/90_100.jpg',\n",
       " 'Apple Crimson Snow/91_100.jpg',\n",
       " 'Apple Crimson Snow/92_100.jpg',\n",
       " 'Apple Crimson Snow/93_100.jpg',\n",
       " 'Apple Crimson Snow/94_100.jpg',\n",
       " 'Apple Crimson Snow/95_100.jpg',\n",
       " 'Apple Crimson Snow/96_100.jpg',\n",
       " 'Apple Crimson Snow/97_100.jpg',\n",
       " 'Apple Crimson Snow/98_100.jpg',\n",
       " 'Apple Crimson Snow/99_100.jpg',\n",
       " 'Apple Crimson Snow/r_105_100.jpg',\n",
       " 'Apple Crimson Snow/r_106_100.jpg',\n",
       " 'Apple Crimson Snow/r_111_100.jpg',\n",
       " 'Apple Crimson Snow/r_112_100.jpg',\n",
       " 'Apple Crimson Snow/r_113_100.jpg',\n",
       " 'Apple Crimson Snow/r_114_100.jpg',\n",
       " 'Apple Crimson Snow/r_115_100.jpg',\n",
       " 'Apple Crimson Snow/r_116_100.jpg',\n",
       " 'Apple Crimson Snow/r_117_100.jpg',\n",
       " 'Apple Crimson Snow/r_118_100.jpg',\n",
       " 'Apple Crimson Snow/r_120_100.jpg',\n",
       " 'Apple Crimson Snow/r_121_100.jpg',\n",
       " 'Apple Crimson Snow/r_122_100.jpg',\n",
       " 'Apple Crimson Snow/r_123_100.jpg',\n",
       " 'Apple Crimson Snow/r_124_100.jpg',\n",
       " 'Apple Crimson Snow/r_13_100.jpg',\n",
       " 'Apple Crimson Snow/r_14_100.jpg',\n",
       " 'Apple Crimson Snow/r_15_100.jpg',\n",
       " 'Apple Crimson Snow/r_16_100.jpg',\n",
       " 'Apple Crimson Snow/r_17_100.jpg',\n",
       " 'Apple Crimson Snow/r_183_100.jpg',\n",
       " 'Apple Crimson Snow/r_184_100.jpg',\n",
       " 'Apple Crimson Snow/r_18_100.jpg',\n",
       " 'Apple Crimson Snow/r_19_100.jpg',\n",
       " 'Apple Crimson Snow/r_20_100.jpg',\n",
       " 'Apple Crimson Snow/r_21_100.jpg',\n",
       " 'Apple Crimson Snow/r_22_100.jpg',\n",
       " 'Apple Crimson Snow/r_23_100.jpg',\n",
       " 'Apple Crimson Snow/r_24_100.jpg',\n",
       " 'Apple Crimson Snow/r_25_100.jpg',\n",
       " 'Apple Crimson Snow/r_26_100.jpg',\n",
       " 'Apple Crimson Snow/r_27_100.jpg',\n",
       " 'Apple Crimson Snow/r_28_100.jpg',\n",
       " 'Apple Crimson Snow/r_29_100.jpg',\n",
       " 'Apple Crimson Snow/r_30_100.jpg',\n",
       " 'Apple Crimson Snow/r_31_100.jpg',\n",
       " 'Apple Crimson Snow/r_32_100.jpg',\n",
       " 'Apple Crimson Snow/r_33_100.jpg',\n",
       " 'Apple Crimson Snow/r_34_100.jpg',\n",
       " 'Apple Crimson Snow/r_35_100.jpg',\n",
       " 'Apple Crimson Snow/r_36_100.jpg',\n",
       " 'Apple Crimson Snow/r_37_100.jpg',\n",
       " 'Apple Crimson Snow/r_38_100.jpg',\n",
       " 'Apple Crimson Snow/r_39_100.jpg',\n",
       " 'Apple Crimson Snow/r_40_100.jpg',\n",
       " 'Apple Crimson Snow/r_41_100.jpg',\n",
       " 'Apple Crimson Snow/r_42_100.jpg',\n",
       " 'Apple Crimson Snow/r_43_100.jpg',\n",
       " 'Apple Crimson Snow/r_44_100.jpg',\n",
       " 'Apple Crimson Snow/r_45_100.jpg',\n",
       " 'Apple Crimson Snow/r_46_100.jpg',\n",
       " 'Apple Crimson Snow/r_47_100.jpg',\n",
       " 'Apple Crimson Snow/r_48_100.jpg',\n",
       " 'Apple Crimson Snow/r_49_100.jpg',\n",
       " 'Apple Crimson Snow/r_50_100.jpg',\n",
       " 'Apple Crimson Snow/r_51_100.jpg',\n",
       " 'Apple Crimson Snow/r_52_100.jpg',\n",
       " 'Apple Crimson Snow/r_53_100.jpg',\n",
       " 'Apple Crimson Snow/r_54_100.jpg',\n",
       " 'Apple Crimson Snow/r_55_100.jpg',\n",
       " 'Apple Crimson Snow/r_56_100.jpg',\n",
       " 'Apple Crimson Snow/r_57_100.jpg',\n",
       " 'Apple Crimson Snow/r_58_100.jpg',\n",
       " 'Apple Crimson Snow/r_59_100.jpg',\n",
       " 'Apple Crimson Snow/r_60_100.jpg',\n",
       " 'Apple Crimson Snow/r_61_100.jpg',\n",
       " 'Apple Crimson Snow/r_62_100.jpg',\n",
       " 'Apple Crimson Snow/r_63_100.jpg',\n",
       " 'Apple Crimson Snow/r_64_100.jpg',\n",
       " 'Apple Crimson Snow/r_65_100.jpg',\n",
       " 'Apple Crimson Snow/r_66_100.jpg',\n",
       " 'Apple Crimson Snow/r_67_100.jpg',\n",
       " 'Apple Crimson Snow/r_69_100.jpg',\n",
       " 'Apple Crimson Snow/r_71_100.jpg',\n",
       " 'Apple Golden 1/100_100.jpg',\n",
       " 'Apple Golden 1/101_100.jpg',\n",
       " 'Apple Golden 1/102_100.jpg',\n",
       " 'Apple Golden 1/103_100.jpg',\n",
       " 'Apple Golden 1/104_100.jpg',\n",
       " 'Apple Golden 1/105_100.jpg',\n",
       " 'Apple Golden 1/106_100.jpg',\n",
       " 'Apple Golden 1/107_100.jpg',\n",
       " 'Apple Golden 1/108_100.jpg',\n",
       " 'Apple Golden 1/109_100.jpg',\n",
       " 'Apple Golden 1/110_100.jpg',\n",
       " 'Apple Golden 1/111_100.jpg',\n",
       " 'Apple Golden 1/112_100.jpg',\n",
       " 'Apple Golden 1/113_100.jpg',\n",
       " 'Apple Golden 1/114_100.jpg',\n",
       " 'Apple Golden 1/115_100.jpg',\n",
       " 'Apple Golden 1/116_100.jpg',\n",
       " 'Apple Golden 1/117_100.jpg',\n",
       " 'Apple Golden 1/118_100.jpg',\n",
       " 'Apple Golden 1/119_100.jpg',\n",
       " 'Apple Golden 1/120_100.jpg',\n",
       " 'Apple Golden 1/121_100.jpg',\n",
       " 'Apple Golden 1/122_100.jpg',\n",
       " 'Apple Golden 1/123_100.jpg',\n",
       " 'Apple Golden 1/124_100.jpg',\n",
       " 'Apple Golden 1/125_100.jpg',\n",
       " 'Apple Golden 1/126_100.jpg',\n",
       " 'Apple Golden 1/127_100.jpg',\n",
       " 'Apple Golden 1/128_100.jpg',\n",
       " 'Apple Golden 1/129_100.jpg',\n",
       " 'Apple Golden 1/130_100.jpg',\n",
       " 'Apple Golden 1/131_100.jpg',\n",
       " 'Apple Golden 1/132_100.jpg',\n",
       " 'Apple Golden 1/133_100.jpg',\n",
       " 'Apple Golden 1/134_100.jpg',\n",
       " 'Apple Golden 1/135_100.jpg',\n",
       " 'Apple Golden 1/136_100.jpg',\n",
       " 'Apple Golden 1/137_100.jpg',\n",
       " 'Apple Golden 1/138_100.jpg',\n",
       " 'Apple Golden 1/139_100.jpg',\n",
       " 'Apple Golden 1/140_100.jpg',\n",
       " 'Apple Golden 1/141_100.jpg',\n",
       " 'Apple Golden 1/142_100.jpg',\n",
       " 'Apple Golden 1/143_100.jpg',\n",
       " 'Apple Golden 1/144_100.jpg',\n",
       " 'Apple Golden 1/145_100.jpg',\n",
       " 'Apple Golden 1/146_100.jpg',\n",
       " 'Apple Golden 1/149_100.jpg',\n",
       " 'Apple Golden 1/150_100.jpg',\n",
       " 'Apple Golden 1/63_100.jpg',\n",
       " 'Apple Golden 1/72_100.jpg',\n",
       " 'Apple Golden 1/73_100.jpg',\n",
       " 'Apple Golden 1/74_100.jpg',\n",
       " 'Apple Golden 1/75_100.jpg',\n",
       " 'Apple Golden 1/76_100.jpg',\n",
       " 'Apple Golden 1/77_100.jpg',\n",
       " 'Apple Golden 1/78_100.jpg',\n",
       " 'Apple Golden 1/79_100.jpg',\n",
       " 'Apple Golden 1/80_100.jpg',\n",
       " 'Apple Golden 1/81_100.jpg',\n",
       " 'Apple Golden 1/82_100.jpg',\n",
       " 'Apple Golden 1/83_100.jpg',\n",
       " 'Apple Golden 1/84_100.jpg',\n",
       " 'Apple Golden 1/85_100.jpg',\n",
       " 'Apple Golden 1/86_100.jpg',\n",
       " 'Apple Golden 1/87_100.jpg',\n",
       " 'Apple Golden 1/88_100.jpg',\n",
       " 'Apple Golden 1/89_100.jpg',\n",
       " 'Apple Golden 1/90_100.jpg',\n",
       " 'Apple Golden 1/91_100.jpg',\n",
       " 'Apple Golden 1/92_100.jpg',\n",
       " 'Apple Golden 1/93_100.jpg',\n",
       " 'Apple Golden 1/94_100.jpg',\n",
       " 'Apple Golden 1/95_100.jpg',\n",
       " 'Apple Golden 1/96_100.jpg',\n",
       " 'Apple Golden 1/97_100.jpg',\n",
       " 'Apple Golden 1/98_100.jpg',\n",
       " 'Apple Golden 1/99_100.jpg',\n",
       " 'Apple Golden 1/r_321_100.jpg',\n",
       " 'Apple Golden 1/r_322_100.jpg',\n",
       " 'Apple Golden 1/r_323_100.jpg',\n",
       " 'Apple Golden 1/r_324_100.jpg',\n",
       " 'Apple Golden 1/r_325_100.jpg',\n",
       " 'Apple Golden 1/r_326_100.jpg',\n",
       " 'Apple Golden 1/r_327_100.jpg',\n",
       " 'Apple Golden 1/r_32_100.jpg',\n",
       " 'Apple Golden 1/r_33_100.jpg',\n",
       " 'Apple Golden 1/r_34_100.jpg',\n",
       " 'Apple Golden 1/r_35_100.jpg',\n",
       " 'Apple Golden 1/r_36_100.jpg',\n",
       " 'Apple Golden 1/r_37_100.jpg',\n",
       " 'Apple Golden 1/r_38_100.jpg',\n",
       " 'Apple Golden 1/r_39_100.jpg',\n",
       " 'Apple Golden 1/r_3_100.jpg',\n",
       " 'Apple Golden 1/r_40_100.jpg',\n",
       " 'Apple Golden 1/r_41_100.jpg',\n",
       " 'Apple Golden 1/r_42_100.jpg',\n",
       " 'Apple Golden 1/r_43_100.jpg',\n",
       " 'Apple Golden 1/r_44_100.jpg',\n",
       " 'Apple Golden 1/r_45_100.jpg',\n",
       " 'Apple Golden 1/r_46_100.jpg',\n",
       " 'Apple Golden 1/r_47_100.jpg',\n",
       " 'Apple Golden 1/r_48_100.jpg',\n",
       " 'Apple Golden 1/r_49_100.jpg',\n",
       " 'Apple Golden 1/r_4_100.jpg',\n",
       " 'Apple Golden 1/r_50_100.jpg',\n",
       " 'Apple Golden 1/r_51_100.jpg',\n",
       " 'Apple Golden 1/r_52_100.jpg',\n",
       " 'Apple Golden 1/r_53_100.jpg',\n",
       " 'Apple Golden 1/r_54_100.jpg',\n",
       " 'Apple Golden 1/r_55_100.jpg',\n",
       " 'Apple Golden 1/r_56_100.jpg',\n",
       " 'Apple Golden 1/r_57_100.jpg',\n",
       " 'Apple Golden 1/r_58_100.jpg',\n",
       " 'Apple Golden 1/r_59_100.jpg',\n",
       " 'Apple Golden 1/r_5_100.jpg',\n",
       " 'Apple Golden 1/r_60_100.jpg',\n",
       " 'Apple Golden 1/r_61_100.jpg',\n",
       " 'Apple Golden 1/r_62_100.jpg',\n",
       " 'Apple Golden 1/r_63_100.jpg',\n",
       " 'Apple Golden 1/r_64_100.jpg',\n",
       " 'Apple Golden 1/r_65_100.jpg',\n",
       " 'Apple Golden 1/r_66_100.jpg',\n",
       " 'Apple Golden 1/r_67_100.jpg',\n",
       " 'Apple Golden 1/r_68_100.jpg',\n",
       " 'Apple Golden 1/r_69_100.jpg',\n",
       " 'Apple Golden 1/r_6_100.jpg',\n",
       " 'Apple Golden 1/r_70_100.jpg',\n",
       " 'Apple Golden 1/r_71_100.jpg',\n",
       " 'Apple Golden 1/r_72_100.jpg',\n",
       " 'Apple Golden 1/r_73_100.jpg',\n",
       " 'Apple Golden 1/r_74_100.jpg',\n",
       " 'Apple Golden 1/r_75_100.jpg',\n",
       " 'Apple Golden 1/r_76_100.jpg',\n",
       " 'Apple Golden 1/r_77_100.jpg',\n",
       " 'Apple Golden 1/r_78_100.jpg',\n",
       " 'Apple Golden 1/r_79_100.jpg',\n",
       " 'Apple Golden 1/r_7_100.jpg',\n",
       " 'Apple Golden 1/r_80_100.jpg',\n",
       " 'Apple Golden 1/r_81_100.jpg',\n",
       " 'Apple Golden 1/r_82_100.jpg',\n",
       " 'Apple Golden 1/r_83_100.jpg',\n",
       " 'Apple Golden 1/r_84_100.jpg',\n",
       " 'Apple Golden 1/r_85_100.jpg',\n",
       " 'Apple Golden 1/r_86_100.jpg',\n",
       " 'Apple Golden 1/r_87_100.jpg',\n",
       " 'Apple Golden 1/r_88_100.jpg',\n",
       " 'Apple Golden 1/r_89_100.jpg',\n",
       " 'Apple Golden 1/r_8_100.jpg',\n",
       " 'Apple Golden 1/r_90_100.jpg',\n",
       " 'Apple Golden 1/r_91_100.jpg',\n",
       " 'Apple Golden 1/r_92_100.jpg',\n",
       " 'Apple Golden 1/r_93_100.jpg',\n",
       " 'Apple Golden 1/r_94_100.jpg',\n",
       " 'Apple Golden 1/r_95_100.jpg',\n",
       " 'Apple Golden 1/r_96_100.jpg',\n",
       " 'Apple Golden 1/r_97_100.jpg',\n",
       " 'Apple Golden 1/r_98_100.jpg',\n",
       " 'Apple Golden 1/r_99_100.jpg',\n",
       " 'Apple Golden 1/r_9_100.jpg',\n",
       " 'Apple Golden 2/321_100.jpg',\n",
       " 'Apple Golden 2/322_100.jpg',\n",
       " 'Apple Golden 2/323_100.jpg',\n",
       " 'Apple Golden 2/324_100.jpg',\n",
       " 'Apple Golden 2/325_100.jpg',\n",
       " 'Apple Golden 2/326_100.jpg',\n",
       " 'Apple Golden 2/327_100.jpg',\n",
       " 'Apple Golden 2/32_100.jpg',\n",
       " 'Apple Golden 2/33_100.jpg',\n",
       " 'Apple Golden 2/34_100.jpg',\n",
       " 'Apple Golden 2/35_100.jpg',\n",
       " 'Apple Golden 2/36_100.jpg',\n",
       " 'Apple Golden 2/37_100.jpg',\n",
       " 'Apple Golden 2/38_100.jpg',\n",
       " 'Apple Golden 2/39_100.jpg',\n",
       " 'Apple Golden 2/3_100.jpg',\n",
       " 'Apple Golden 2/40_100.jpg',\n",
       " 'Apple Golden 2/41_100.jpg',\n",
       " 'Apple Golden 2/42_100.jpg',\n",
       " 'Apple Golden 2/43_100.jpg',\n",
       " 'Apple Golden 2/44_100.jpg',\n",
       " 'Apple Golden 2/45_100.jpg',\n",
       " 'Apple Golden 2/46_100.jpg',\n",
       " 'Apple Golden 2/47_100.jpg',\n",
       " 'Apple Golden 2/48_100.jpg',\n",
       " 'Apple Golden 2/49_100.jpg',\n",
       " 'Apple Golden 2/4_100.jpg',\n",
       " 'Apple Golden 2/50_100.jpg',\n",
       " 'Apple Golden 2/51_100.jpg',\n",
       " 'Apple Golden 2/52_100.jpg',\n",
       " 'Apple Golden 2/53_100.jpg',\n",
       " 'Apple Golden 2/54_100.jpg',\n",
       " 'Apple Golden 2/55_100.jpg',\n",
       " 'Apple Golden 2/56_100.jpg',\n",
       " 'Apple Golden 2/57_100.jpg',\n",
       " 'Apple Golden 2/58_100.jpg',\n",
       " 'Apple Golden 2/59_100.jpg',\n",
       " 'Apple Golden 2/5_100.jpg',\n",
       " 'Apple Golden 2/60_100.jpg',\n",
       " 'Apple Golden 2/61_100.jpg',\n",
       " 'Apple Golden 2/62_100.jpg',\n",
       " 'Apple Golden 2/63_100.jpg',\n",
       " 'Apple Golden 2/64_100.jpg',\n",
       " 'Apple Golden 2/65_100.jpg',\n",
       " 'Apple Golden 2/66_100.jpg',\n",
       " 'Apple Golden 2/67_100.jpg',\n",
       " 'Apple Golden 2/68_100.jpg',\n",
       " 'Apple Golden 2/69_100.jpg',\n",
       " 'Apple Golden 2/6_100.jpg',\n",
       " 'Apple Golden 2/70_100.jpg',\n",
       " 'Apple Golden 2/71_100.jpg',\n",
       " 'Apple Golden 2/72_100.jpg',\n",
       " 'Apple Golden 2/73_100.jpg',\n",
       " 'Apple Golden 2/74_100.jpg',\n",
       " 'Apple Golden 2/75_100.jpg',\n",
       " 'Apple Golden 2/76_100.jpg',\n",
       " 'Apple Golden 2/77_100.jpg',\n",
       " 'Apple Golden 2/78_100.jpg',\n",
       " 'Apple Golden 2/79_100.jpg',\n",
       " 'Apple Golden 2/7_100.jpg',\n",
       " 'Apple Golden 2/80_100.jpg',\n",
       " 'Apple Golden 2/81_100.jpg',\n",
       " 'Apple Golden 2/82_100.jpg',\n",
       " 'Apple Golden 2/83_100.jpg',\n",
       " 'Apple Golden 2/84_100.jpg',\n",
       " 'Apple Golden 2/85_100.jpg',\n",
       " 'Apple Golden 2/86_100.jpg',\n",
       " 'Apple Golden 2/87_100.jpg',\n",
       " 'Apple Golden 2/88_100.jpg',\n",
       " 'Apple Golden 2/89_100.jpg',\n",
       " 'Apple Golden 2/8_100.jpg',\n",
       " 'Apple Golden 2/90_100.jpg',\n",
       " 'Apple Golden 2/91_100.jpg',\n",
       " 'Apple Golden 2/92_100.jpg',\n",
       " 'Apple Golden 2/93_100.jpg',\n",
       " 'Apple Golden 2/94_100.jpg',\n",
       " 'Apple Golden 2/95_100.jpg',\n",
       " 'Apple Golden 2/96_100.jpg',\n",
       " 'Apple Golden 2/97_100.jpg',\n",
       " 'Apple Golden 2/98_100.jpg',\n",
       " 'Apple Golden 2/99_100.jpg',\n",
       " 'Apple Golden 2/9_100.jpg',\n",
       " 'Apple Golden 2/r_321_100.jpg',\n",
       " 'Apple Golden 2/r_322_100.jpg',\n",
       " 'Apple Golden 2/r_323_100.jpg',\n",
       " 'Apple Golden 2/r_324_100.jpg',\n",
       " 'Apple Golden 2/r_325_100.jpg',\n",
       " 'Apple Golden 2/r_326_100.jpg',\n",
       " 'Apple Golden 2/r_327_100.jpg',\n",
       " 'Apple Golden 2/r_32_100.jpg',\n",
       " 'Apple Golden 2/r_33_100.jpg',\n",
       " 'Apple Golden 2/r_34_100.jpg',\n",
       " 'Apple Golden 2/r_35_100.jpg',\n",
       " 'Apple Golden 2/r_36_100.jpg',\n",
       " 'Apple Golden 2/r_37_100.jpg',\n",
       " 'Apple Golden 2/r_38_100.jpg',\n",
       " 'Apple Golden 2/r_39_100.jpg',\n",
       " 'Apple Golden 2/r_3_100.jpg',\n",
       " 'Apple Golden 2/r_40_100.jpg',\n",
       " 'Apple Golden 2/r_41_100.jpg',\n",
       " 'Apple Golden 2/r_42_100.jpg',\n",
       " 'Apple Golden 2/r_43_100.jpg',\n",
       " 'Apple Golden 2/r_44_100.jpg',\n",
       " 'Apple Golden 2/r_45_100.jpg',\n",
       " 'Apple Golden 2/r_46_100.jpg',\n",
       " 'Apple Golden 2/r_47_100.jpg',\n",
       " 'Apple Golden 2/r_48_100.jpg',\n",
       " 'Apple Golden 2/r_49_100.jpg',\n",
       " 'Apple Golden 2/r_4_100.jpg',\n",
       " 'Apple Golden 2/r_50_100.jpg',\n",
       " 'Apple Golden 2/r_51_100.jpg',\n",
       " 'Apple Golden 2/r_52_100.jpg',\n",
       " 'Apple Golden 2/r_53_100.jpg',\n",
       " 'Apple Golden 2/r_54_100.jpg',\n",
       " 'Apple Golden 2/r_55_100.jpg',\n",
       " 'Apple Golden 2/r_56_100.jpg',\n",
       " 'Apple Golden 2/r_57_100.jpg',\n",
       " 'Apple Golden 2/r_58_100.jpg',\n",
       " 'Apple Golden 2/r_59_100.jpg',\n",
       " 'Apple Golden 2/r_5_100.jpg',\n",
       " 'Apple Golden 2/r_60_100.jpg',\n",
       " 'Apple Golden 2/r_61_100.jpg',\n",
       " 'Apple Golden 2/r_62_100.jpg',\n",
       " 'Apple Golden 2/r_63_100.jpg',\n",
       " 'Apple Golden 2/r_64_100.jpg',\n",
       " 'Apple Golden 2/r_65_100.jpg',\n",
       " 'Apple Golden 2/r_66_100.jpg',\n",
       " 'Apple Golden 2/r_67_100.jpg',\n",
       " 'Apple Golden 2/r_68_100.jpg',\n",
       " 'Apple Golden 2/r_69_100.jpg',\n",
       " 'Apple Golden 2/r_6_100.jpg',\n",
       " 'Apple Golden 2/r_70_100.jpg',\n",
       " 'Apple Golden 2/r_71_100.jpg',\n",
       " 'Apple Golden 2/r_72_100.jpg',\n",
       " 'Apple Golden 2/r_73_100.jpg',\n",
       " 'Apple Golden 2/r_74_100.jpg',\n",
       " 'Apple Golden 2/r_75_100.jpg',\n",
       " 'Apple Golden 2/r_76_100.jpg',\n",
       " 'Apple Golden 2/r_77_100.jpg',\n",
       " 'Apple Golden 2/r_78_100.jpg',\n",
       " 'Apple Golden 2/r_79_100.jpg',\n",
       " 'Apple Golden 2/r_7_100.jpg',\n",
       " 'Apple Golden 2/r_80_100.jpg',\n",
       " 'Apple Golden 2/r_81_100.jpg',\n",
       " 'Apple Golden 2/r_82_100.jpg',\n",
       " 'Apple Golden 2/r_83_100.jpg',\n",
       " 'Apple Golden 2/r_84_100.jpg',\n",
       " 'Apple Golden 2/r_85_100.jpg',\n",
       " 'Apple Golden 2/r_86_100.jpg',\n",
       " 'Apple Golden 2/r_87_100.jpg',\n",
       " 'Apple Golden 2/r_88_100.jpg',\n",
       " 'Apple Golden 2/r_89_100.jpg',\n",
       " 'Apple Golden 2/r_8_100.jpg',\n",
       " 'Apple Golden 2/r_90_100.jpg',\n",
       " 'Apple Golden 2/r_91_100.jpg',\n",
       " 'Apple Golden 2/r_92_100.jpg',\n",
       " 'Apple Golden 2/r_93_100.jpg',\n",
       " 'Apple Golden 2/r_94_100.jpg',\n",
       " 'Apple Golden 2/r_95_100.jpg',\n",
       " 'Apple Golden 2/r_96_100.jpg',\n",
       " 'Apple Golden 2/r_97_100.jpg',\n",
       " 'Apple Golden 2/r_98_100.jpg',\n",
       " 'Apple Golden 2/r_99_100.jpg',\n",
       " 'Apple Golden 2/r_9_100.jpg',\n",
       " 'Apple Golden 3/311_100.jpg',\n",
       " 'Apple Golden 3/312_100.jpg',\n",
       " 'Apple Golden 3/313_100.jpg',\n",
       " 'Apple Golden 3/31_100.jpg',\n",
       " 'Apple Golden 3/32_100.jpg',\n",
       " 'Apple Golden 3/33_100.jpg',\n",
       " 'Apple Golden 3/34_100.jpg',\n",
       " 'Apple Golden 3/35_100.jpg',\n",
       " 'Apple Golden 3/36_100.jpg',\n",
       " 'Apple Golden 3/37_100.jpg',\n",
       " 'Apple Golden 3/38_100.jpg',\n",
       " 'Apple Golden 3/39_100.jpg',\n",
       " 'Apple Golden 3/3_100.jpg',\n",
       " 'Apple Golden 3/40_100.jpg',\n",
       " 'Apple Golden 3/41_100.jpg',\n",
       " 'Apple Golden 3/42_100.jpg',\n",
       " 'Apple Golden 3/43_100.jpg',\n",
       " 'Apple Golden 3/44_100.jpg',\n",
       " 'Apple Golden 3/45_100.jpg',\n",
       " 'Apple Golden 3/46_100.jpg',\n",
       " 'Apple Golden 3/47_100.jpg',\n",
       " 'Apple Golden 3/48_100.jpg',\n",
       " 'Apple Golden 3/49_100.jpg',\n",
       " 'Apple Golden 3/4_100.jpg',\n",
       " 'Apple Golden 3/50_100.jpg',\n",
       " 'Apple Golden 3/51_100.jpg',\n",
       " 'Apple Golden 3/52_100.jpg',\n",
       " 'Apple Golden 3/53_100.jpg',\n",
       " 'Apple Golden 3/54_100.jpg',\n",
       " 'Apple Golden 3/55_100.jpg',\n",
       " 'Apple Golden 3/56_100.jpg',\n",
       " 'Apple Golden 3/57_100.jpg',\n",
       " 'Apple Golden 3/58_100.jpg',\n",
       " 'Apple Golden 3/59_100.jpg',\n",
       " 'Apple Golden 3/5_100.jpg',\n",
       " 'Apple Golden 3/60_100.jpg',\n",
       " 'Apple Golden 3/61_100.jpg',\n",
       " 'Apple Golden 3/62_100.jpg',\n",
       " 'Apple Golden 3/63_100.jpg',\n",
       " 'Apple Golden 3/64_100.jpg',\n",
       " 'Apple Golden 3/65_100.jpg',\n",
       " 'Apple Golden 3/66_100.jpg',\n",
       " 'Apple Golden 3/67_100.jpg',\n",
       " 'Apple Golden 3/68_100.jpg',\n",
       " 'Apple Golden 3/69_100.jpg',\n",
       " 'Apple Golden 3/6_100.jpg',\n",
       " 'Apple Golden 3/70_100.jpg',\n",
       " 'Apple Golden 3/71_100.jpg',\n",
       " 'Apple Golden 3/72_100.jpg',\n",
       " 'Apple Golden 3/73_100.jpg',\n",
       " 'Apple Golden 3/74_100.jpg',\n",
       " 'Apple Golden 3/75_100.jpg',\n",
       " 'Apple Golden 3/76_100.jpg',\n",
       " 'Apple Golden 3/77_100.jpg',\n",
       " 'Apple Golden 3/78_100.jpg',\n",
       " 'Apple Golden 3/79_100.jpg',\n",
       " 'Apple Golden 3/7_100.jpg',\n",
       " 'Apple Golden 3/80_100.jpg',\n",
       " 'Apple Golden 3/81_100.jpg',\n",
       " 'Apple Golden 3/82_100.jpg',\n",
       " 'Apple Golden 3/83_100.jpg',\n",
       " 'Apple Golden 3/84_100.jpg',\n",
       " 'Apple Golden 3/85_100.jpg',\n",
       " 'Apple Golden 3/86_100.jpg',\n",
       " 'Apple Golden 3/87_100.jpg',\n",
       " 'Apple Golden 3/88_100.jpg',\n",
       " 'Apple Golden 3/89_100.jpg',\n",
       " 'Apple Golden 3/8_100.jpg',\n",
       " 'Apple Golden 3/90_100.jpg',\n",
       " 'Apple Golden 3/91_100.jpg',\n",
       " 'Apple Golden 3/92_100.jpg',\n",
       " 'Apple Golden 3/93_100.jpg',\n",
       " 'Apple Golden 3/94_100.jpg',\n",
       " 'Apple Golden 3/95_100.jpg',\n",
       " 'Apple Golden 3/96_100.jpg',\n",
       " 'Apple Golden 3/97_100.jpg',\n",
       " 'Apple Golden 3/98_100.jpg',\n",
       " 'Apple Golden 3/99_100.jpg',\n",
       " 'Apple Golden 3/9_100.jpg',\n",
       " 'Apple Golden 3/r_321_100.jpg',\n",
       " 'Apple Golden 3/r_322_100.jpg',\n",
       " 'Apple Golden 3/r_323_100.jpg',\n",
       " 'Apple Golden 3/r_324_100.jpg',\n",
       " 'Apple Golden 3/r_325_100.jpg',\n",
       " 'Apple Golden 3/r_326_100.jpg',\n",
       " 'Apple Golden 3/r_327_100.jpg',\n",
       " 'Apple Golden 3/r_32_100.jpg',\n",
       " 'Apple Golden 3/r_33_100.jpg',\n",
       " 'Apple Golden 3/r_34_100.jpg',\n",
       " 'Apple Golden 3/r_35_100.jpg',\n",
       " 'Apple Golden 3/r_36_100.jpg',\n",
       " 'Apple Golden 3/r_37_100.jpg',\n",
       " 'Apple Golden 3/r_38_100.jpg',\n",
       " 'Apple Golden 3/r_39_100.jpg',\n",
       " 'Apple Golden 3/r_3_100.jpg',\n",
       " 'Apple Golden 3/r_40_100.jpg',\n",
       " 'Apple Golden 3/r_41_100.jpg',\n",
       " 'Apple Golden 3/r_42_100.jpg',\n",
       " 'Apple Golden 3/r_43_100.jpg',\n",
       " 'Apple Golden 3/r_44_100.jpg',\n",
       " 'Apple Golden 3/r_45_100.jpg',\n",
       " 'Apple Golden 3/r_46_100.jpg',\n",
       " 'Apple Golden 3/r_47_100.jpg',\n",
       " 'Apple Golden 3/r_48_100.jpg',\n",
       " 'Apple Golden 3/r_49_100.jpg',\n",
       " 'Apple Golden 3/r_4_100.jpg',\n",
       " 'Apple Golden 3/r_50_100.jpg',\n",
       " 'Apple Golden 3/r_51_100.jpg',\n",
       " 'Apple Golden 3/r_52_100.jpg',\n",
       " 'Apple Golden 3/r_53_100.jpg',\n",
       " 'Apple Golden 3/r_54_100.jpg',\n",
       " 'Apple Golden 3/r_55_100.jpg',\n",
       " 'Apple Golden 3/r_56_100.jpg',\n",
       " 'Apple Golden 3/r_57_100.jpg',\n",
       " 'Apple Golden 3/r_58_100.jpg',\n",
       " 'Apple Golden 3/r_59_100.jpg',\n",
       " 'Apple Golden 3/r_5_100.jpg',\n",
       " 'Apple Golden 3/r_60_100.jpg',\n",
       " 'Apple Golden 3/r_61_100.jpg',\n",
       " 'Apple Golden 3/r_62_100.jpg',\n",
       " 'Apple Golden 3/r_63_100.jpg',\n",
       " 'Apple Golden 3/r_64_100.jpg',\n",
       " 'Apple Golden 3/r_65_100.jpg',\n",
       " 'Apple Golden 3/r_66_100.jpg',\n",
       " 'Apple Golden 3/r_67_100.jpg',\n",
       " 'Apple Golden 3/r_68_100.jpg',\n",
       " 'Apple Golden 3/r_69_100.jpg',\n",
       " 'Apple Golden 3/r_6_100.jpg',\n",
       " 'Apple Golden 3/r_70_100.jpg',\n",
       " 'Apple Golden 3/r_71_100.jpg',\n",
       " 'Apple Golden 3/r_72_100.jpg',\n",
       " 'Apple Golden 3/r_73_100.jpg',\n",
       " 'Apple Golden 3/r_74_100.jpg',\n",
       " 'Apple Golden 3/r_75_100.jpg',\n",
       " 'Apple Golden 3/r_76_100.jpg',\n",
       " 'Apple Golden 3/r_77_100.jpg',\n",
       " 'Apple Golden 3/r_78_100.jpg',\n",
       " 'Apple Golden 3/r_79_100.jpg',\n",
       " 'Apple Golden 3/r_7_100.jpg',\n",
       " 'Apple Golden 3/r_80_100.jpg',\n",
       " 'Apple Golden 3/r_81_100.jpg',\n",
       " 'Apple Golden 3/r_82_100.jpg',\n",
       " 'Apple Golden 3/r_83_100.jpg',\n",
       " 'Apple Golden 3/r_84_100.jpg',\n",
       " 'Apple Golden 3/r_85_100.jpg',\n",
       " 'Apple Golden 3/r_86_100.jpg',\n",
       " 'Apple Golden 3/r_87_100.jpg',\n",
       " 'Apple Golden 3/r_88_100.jpg',\n",
       " 'Apple Golden 3/r_89_100.jpg',\n",
       " 'Apple Golden 3/r_8_100.jpg',\n",
       " 'Apple Golden 3/r_90_100.jpg',\n",
       " 'Apple Golden 3/r_91_100.jpg',\n",
       " 'Apple Golden 3/r_92_100.jpg',\n",
       " 'Apple Golden 3/r_93_100.jpg',\n",
       " 'Apple Golden 3/r_94_100.jpg',\n",
       " 'Apple Golden 3/r_95_100.jpg',\n",
       " 'Apple Golden 3/r_96_100.jpg',\n",
       " 'Apple Golden 3/r_97_100.jpg',\n",
       " 'Apple Golden 3/r_98_100.jpg',\n",
       " 'Apple Golden 3/r_99_100.jpg',\n",
       " 'Apple Golden 3/r_9_100.jpg',\n",
       " 'Apple Granny Smith/321_100.jpg',\n",
       " 'Apple Granny Smith/322_100.jpg',\n",
       " 'Apple Granny Smith/323_100.jpg',\n",
       " 'Apple Granny Smith/324_100.jpg',\n",
       " 'Apple Granny Smith/325_100.jpg',\n",
       " 'Apple Granny Smith/326_100.jpg',\n",
       " 'Apple Granny Smith/327_100.jpg',\n",
       " 'Apple Granny Smith/32_100.jpg',\n",
       " 'Apple Granny Smith/33_100.jpg',\n",
       " 'Apple Granny Smith/34_100.jpg',\n",
       " 'Apple Granny Smith/35_100.jpg',\n",
       " 'Apple Granny Smith/36_100.jpg',\n",
       " 'Apple Granny Smith/37_100.jpg',\n",
       " 'Apple Granny Smith/38_100.jpg',\n",
       " 'Apple Granny Smith/39_100.jpg',\n",
       " 'Apple Granny Smith/3_100.jpg',\n",
       " 'Apple Granny Smith/40_100.jpg',\n",
       " 'Apple Granny Smith/41_100.jpg',\n",
       " 'Apple Granny Smith/42_100.jpg',\n",
       " 'Apple Granny Smith/43_100.jpg',\n",
       " 'Apple Granny Smith/44_100.jpg',\n",
       " 'Apple Granny Smith/45_100.jpg',\n",
       " 'Apple Granny Smith/46_100.jpg',\n",
       " 'Apple Granny Smith/47_100.jpg',\n",
       " 'Apple Granny Smith/48_100.jpg',\n",
       " 'Apple Granny Smith/49_100.jpg',\n",
       " 'Apple Granny Smith/4_100.jpg',\n",
       " 'Apple Granny Smith/50_100.jpg',\n",
       " 'Apple Granny Smith/51_100.jpg',\n",
       " 'Apple Granny Smith/52_100.jpg',\n",
       " 'Apple Granny Smith/53_100.jpg',\n",
       " 'Apple Granny Smith/54_100.jpg',\n",
       " 'Apple Granny Smith/55_100.jpg',\n",
       " 'Apple Granny Smith/56_100.jpg',\n",
       " 'Apple Granny Smith/57_100.jpg',\n",
       " 'Apple Granny Smith/58_100.jpg',\n",
       " 'Apple Granny Smith/59_100.jpg',\n",
       " 'Apple Granny Smith/5_100.jpg',\n",
       " 'Apple Granny Smith/60_100.jpg',\n",
       " 'Apple Granny Smith/61_100.jpg',\n",
       " 'Apple Granny Smith/62_100.jpg',\n",
       " 'Apple Granny Smith/63_100.jpg',\n",
       " 'Apple Granny Smith/64_100.jpg',\n",
       " 'Apple Granny Smith/65_100.jpg',\n",
       " 'Apple Granny Smith/66_100.jpg',\n",
       " 'Apple Granny Smith/67_100.jpg',\n",
       " 'Apple Granny Smith/68_100.jpg',\n",
       " 'Apple Granny Smith/69_100.jpg',\n",
       " 'Apple Granny Smith/6_100.jpg',\n",
       " 'Apple Granny Smith/70_100.jpg',\n",
       " 'Apple Granny Smith/71_100.jpg',\n",
       " 'Apple Granny Smith/72_100.jpg',\n",
       " 'Apple Granny Smith/73_100.jpg',\n",
       " 'Apple Granny Smith/74_100.jpg',\n",
       " 'Apple Granny Smith/75_100.jpg',\n",
       " 'Apple Granny Smith/76_100.jpg',\n",
       " 'Apple Granny Smith/77_100.jpg',\n",
       " 'Apple Granny Smith/78_100.jpg',\n",
       " 'Apple Granny Smith/79_100.jpg',\n",
       " 'Apple Granny Smith/7_100.jpg',\n",
       " 'Apple Granny Smith/80_100.jpg',\n",
       " 'Apple Granny Smith/81_100.jpg',\n",
       " 'Apple Granny Smith/82_100.jpg',\n",
       " 'Apple Granny Smith/83_100.jpg',\n",
       " 'Apple Granny Smith/84_100.jpg',\n",
       " 'Apple Granny Smith/85_100.jpg',\n",
       " 'Apple Granny Smith/86_100.jpg',\n",
       " 'Apple Granny Smith/87_100.jpg',\n",
       " 'Apple Granny Smith/88_100.jpg',\n",
       " 'Apple Granny Smith/89_100.jpg',\n",
       " 'Apple Granny Smith/8_100.jpg',\n",
       " 'Apple Granny Smith/90_100.jpg',\n",
       " 'Apple Granny Smith/91_100.jpg',\n",
       " 'Apple Granny Smith/92_100.jpg',\n",
       " 'Apple Granny Smith/93_100.jpg',\n",
       " 'Apple Granny Smith/94_100.jpg',\n",
       " 'Apple Granny Smith/95_100.jpg',\n",
       " 'Apple Granny Smith/96_100.jpg',\n",
       " 'Apple Granny Smith/97_100.jpg',\n",
       " 'Apple Granny Smith/98_100.jpg',\n",
       " 'Apple Granny Smith/99_100.jpg',\n",
       " 'Apple Granny Smith/9_100.jpg',\n",
       " 'Apple Granny Smith/r_321_100.jpg',\n",
       " 'Apple Granny Smith/r_322_100.jpg',\n",
       " 'Apple Granny Smith/r_323_100.jpg',\n",
       " 'Apple Granny Smith/r_324_100.jpg',\n",
       " 'Apple Granny Smith/r_325_100.jpg',\n",
       " 'Apple Granny Smith/r_326_100.jpg',\n",
       " 'Apple Granny Smith/r_327_100.jpg',\n",
       " 'Apple Granny Smith/r_32_100.jpg',\n",
       " 'Apple Granny Smith/r_33_100.jpg',\n",
       " 'Apple Granny Smith/r_34_100.jpg',\n",
       " 'Apple Granny Smith/r_35_100.jpg',\n",
       " 'Apple Granny Smith/r_36_100.jpg',\n",
       " 'Apple Granny Smith/r_37_100.jpg',\n",
       " 'Apple Granny Smith/r_38_100.jpg',\n",
       " 'Apple Granny Smith/r_39_100.jpg',\n",
       " 'Apple Granny Smith/r_3_100.jpg',\n",
       " 'Apple Granny Smith/r_40_100.jpg',\n",
       " 'Apple Granny Smith/r_41_100.jpg',\n",
       " 'Apple Granny Smith/r_42_100.jpg',\n",
       " 'Apple Granny Smith/r_43_100.jpg',\n",
       " 'Apple Granny Smith/r_44_100.jpg',\n",
       " 'Apple Granny Smith/r_45_100.jpg',\n",
       " 'Apple Granny Smith/r_46_100.jpg',\n",
       " 'Apple Granny Smith/r_47_100.jpg',\n",
       " 'Apple Granny Smith/r_48_100.jpg',\n",
       " 'Apple Granny Smith/r_49_100.jpg',\n",
       " 'Apple Granny Smith/r_4_100.jpg',\n",
       " 'Apple Granny Smith/r_50_100.jpg',\n",
       " 'Apple Granny Smith/r_51_100.jpg',\n",
       " 'Apple Granny Smith/r_52_100.jpg',\n",
       " 'Apple Granny Smith/r_53_100.jpg',\n",
       " 'Apple Granny Smith/r_54_100.jpg',\n",
       " 'Apple Granny Smith/r_55_100.jpg',\n",
       " 'Apple Granny Smith/r_56_100.jpg',\n",
       " 'Apple Granny Smith/r_57_100.jpg',\n",
       " 'Apple Granny Smith/r_58_100.jpg',\n",
       " 'Apple Granny Smith/r_59_100.jpg',\n",
       " 'Apple Granny Smith/r_5_100.jpg',\n",
       " 'Apple Granny Smith/r_60_100.jpg',\n",
       " 'Apple Granny Smith/r_61_100.jpg',\n",
       " 'Apple Granny Smith/r_62_100.jpg',\n",
       " 'Apple Granny Smith/r_63_100.jpg',\n",
       " 'Apple Granny Smith/r_64_100.jpg',\n",
       " 'Apple Granny Smith/r_65_100.jpg',\n",
       " 'Apple Granny Smith/r_66_100.jpg',\n",
       " 'Apple Granny Smith/r_67_100.jpg',\n",
       " 'Apple Granny Smith/r_68_100.jpg',\n",
       " 'Apple Granny Smith/r_69_100.jpg',\n",
       " 'Apple Granny Smith/r_6_100.jpg',\n",
       " 'Apple Granny Smith/r_70_100.jpg',\n",
       " 'Apple Granny Smith/r_71_100.jpg',\n",
       " 'Apple Granny Smith/r_72_100.jpg',\n",
       " 'Apple Granny Smith/r_73_100.jpg',\n",
       " 'Apple Granny Smith/r_74_100.jpg',\n",
       " 'Apple Granny Smith/r_75_100.jpg',\n",
       " 'Apple Granny Smith/r_76_100.jpg',\n",
       " 'Apple Granny Smith/r_77_100.jpg',\n",
       " 'Apple Granny Smith/r_78_100.jpg',\n",
       " 'Apple Granny Smith/r_79_100.jpg',\n",
       " 'Apple Granny Smith/r_7_100.jpg',\n",
       " 'Apple Granny Smith/r_80_100.jpg',\n",
       " 'Apple Granny Smith/r_81_100.jpg',\n",
       " 'Apple Granny Smith/r_82_100.jpg',\n",
       " 'Apple Granny Smith/r_83_100.jpg',\n",
       " 'Apple Granny Smith/r_84_100.jpg',\n",
       " 'Apple Granny Smith/r_85_100.jpg',\n",
       " 'Apple Granny Smith/r_86_100.jpg',\n",
       " 'Apple Granny Smith/r_87_100.jpg',\n",
       " 'Apple Granny Smith/r_88_100.jpg',\n",
       " 'Apple Granny Smith/r_89_100.jpg',\n",
       " 'Apple Granny Smith/r_8_100.jpg',\n",
       " 'Apple Granny Smith/r_90_100.jpg',\n",
       " 'Apple Granny Smith/r_91_100.jpg',\n",
       " 'Apple Granny Smith/r_92_100.jpg',\n",
       " 'Apple Granny Smith/r_93_100.jpg',\n",
       " 'Apple Granny Smith/r_94_100.jpg',\n",
       " 'Apple Granny Smith/r_95_100.jpg',\n",
       " 'Apple Granny Smith/r_96_100.jpg',\n",
       " 'Apple Granny Smith/r_97_100.jpg',\n",
       " 'Apple Granny Smith/r_98_100.jpg',\n",
       " 'Apple Granny Smith/r_99_100.jpg',\n",
       " 'Apple Granny Smith/r_9_100.jpg',\n",
       " 'Apple Pink Lady/219_100.jpg',\n",
       " 'Apple Pink Lady/220_100.jpg',\n",
       " 'Apple Pink Lady/225_100.jpg',\n",
       " 'Apple Pink Lady/226_100.jpg',\n",
       " 'Apple Pink Lady/228_100.jpg',\n",
       " 'Apple Pink Lady/229_100.jpg',\n",
       " 'Apple Pink Lady/230_100.jpg',\n",
       " 'Apple Pink Lady/232_100.jpg',\n",
       " 'Apple Pink Lady/234_100.jpg',\n",
       " 'Apple Pink Lady/236_100.jpg',\n",
       " 'Apple Pink Lady/237_100.jpg',\n",
       " 'Apple Pink Lady/238_100.jpg',\n",
       " 'Apple Pink Lady/239_100.jpg',\n",
       " 'Apple Pink Lady/240_100.jpg',\n",
       " 'Apple Pink Lady/241_100.jpg',\n",
       " 'Apple Pink Lady/242_100.jpg',\n",
       " 'Apple Pink Lady/243_100.jpg',\n",
       " 'Apple Pink Lady/244_100.jpg',\n",
       " 'Apple Pink Lady/245_100.jpg',\n",
       " 'Apple Pink Lady/246_100.jpg',\n",
       " 'Apple Pink Lady/247_100.jpg',\n",
       " 'Apple Pink Lady/248_100.jpg',\n",
       " 'Apple Pink Lady/249_100.jpg',\n",
       " 'Apple Pink Lady/250_100.jpg',\n",
       " 'Apple Pink Lady/251_100.jpg',\n",
       " 'Apple Pink Lady/252_100.jpg',\n",
       " 'Apple Pink Lady/253_100.jpg',\n",
       " 'Apple Pink Lady/254_100.jpg',\n",
       " 'Apple Pink Lady/255_100.jpg',\n",
       " 'Apple Pink Lady/256_100.jpg',\n",
       " 'Apple Pink Lady/257_100.jpg',\n",
       " 'Apple Pink Lady/258_100.jpg',\n",
       " 'Apple Pink Lady/259_100.jpg',\n",
       " 'Apple Pink Lady/260_100.jpg',\n",
       " 'Apple Pink Lady/261_100.jpg',\n",
       " 'Apple Pink Lady/262_100.jpg',\n",
       " 'Apple Pink Lady/263_100.jpg',\n",
       " 'Apple Pink Lady/264_100.jpg',\n",
       " 'Apple Pink Lady/265_100.jpg',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pepper.env import get_project_dir\n",
    "from pepper.utils import _get_filenames_glob\n",
    "import os\n",
    "#C:\\Users\\franc\\Projects\\pepper_cloud_based_model\\dataset\\fruits-360_dataset\\Test\n",
    "raw_src_im_dir = os.path.join(get_project_dir(), r\"dataset\\fruits-360_dataset\\Test\")\n",
    "display(raw_src_im_dir)\n",
    "filenames = _get_filenames_glob(raw_src_im_dir, recursive=True)\n",
    "display(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22688\n"
     ]
    }
   ],
   "source": [
    "print(len(filenames))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöß Sampling des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pepper.utils import create_if_not_exist\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def sample_images(source_dir: str, target_dir: str, n_samples: int):\n",
    "    # R√©cup√©rer la liste des sous-dossiers\n",
    "    subdirs = [\n",
    "        subdir for subdir in os.listdir(source_dir)\n",
    "        if os.path.isdir(os.path.join(source_dir, subdir))\n",
    "    ]\n",
    "\n",
    "    n_images_per_folder, remainder = divmod(n_samples, len(subdirs))\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(source_dir, subdir)\n",
    "\n",
    "        # R√©cup√©rer la liste des images dans le dossier\n",
    "        images = [\n",
    "            image for image in os.listdir(subdir_path)\n",
    "            if os.path.isfile(os.path.join(subdir_path, image))\n",
    "        ]\n",
    "\n",
    "        # S√©lectionner les images al√©atoirement\n",
    "        n_images = n_images_per_folder + (remainder > 0) \n",
    "        if len(images) <= n_images:\n",
    "            selected_images = images\n",
    "        else:\n",
    "            selected_images = random.sample(images, n_images)\n",
    "        \n",
    "        if n_images == len(selected_images):\n",
    "            remainder -= 1\n",
    "        else:\n",
    "            remainder += n_images - len(selected_images)\n",
    "\n",
    "        if len(selected_images) > 0:\n",
    "            create_if_not_exist(os.path.join(target_dir, subdir))\n",
    "        \n",
    "        # Copier les images s√©lectionn√©es vers le dossier cible\n",
    "        for image in selected_images:\n",
    "            source_path = os.path.join(subdir_path, image)\n",
    "            target_path = os.path.join(target_dir, subdir, image)\n",
    "            shutil.copyfile(source_path, target_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pepper.env import get_project_dir\n",
    "from pepper.utils import create_if_not_exist\n",
    "from fruits.storage_utils import sample_images\n",
    "project_dir = get_project_dir()\n",
    "raw_src_im_dir = os.path.join(project_dir, r\"dataset\\fruits-360_dataset\\Test\")\n",
    "sample_300_im_dir = os.path.join(project_dir, r\"data\\im\\sample_300\")\n",
    "create_if_not_exist(sample_300_im_dir)\n",
    "n_images_per_folder = sample_images(raw_src_im_dir, sample_300_im_dir, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = sum(n_images for subdir, n_images in n_images_per_folder.items())\n",
    "display(n_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöß Local vers S3, S3 vers S3\n",
    "\n",
    "Compartiment `s3://pepper-labs-fruits`\n",
    "\n",
    "On va d'abord effectuer une copie de la s√©lection depuis les dossiers locaux directement vers un compartiment S3.\n",
    "\n",
    "Puis, par challenge, le faire directement entre le d√©p√¥t S3 en ligne des images et notre d√©p√¥t.\n",
    "\n",
    "Enfin, il sera int√©ressant de ne m√™me pas effectuer cette copie, mais d'aller pr√©lever directement notre √©chantillon √† la source (EMR utilisant le S3 officiel des images Fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.26.133-py3-none-any.whl (135 kB)\n",
      "                                              0.0/135.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 135.6/135.6 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting botocore<1.30.0,>=1.29.133 (from boto3)\n",
      "  Downloading botocore-1.29.133-py3-none-any.whl (10.7 MB)\n",
      "                                              0.0/10.7 MB ? eta -:--:--\n",
      "     ---                                      1.1/10.7 MB 34.0 MB/s eta 0:00:01\n",
      "     ----------                               2.7/10.7 MB 34.9 MB/s eta 0:00:01\n",
      "     ----------------                         4.5/10.7 MB 35.6 MB/s eta 0:00:01\n",
      "     -----------------------                  6.4/10.7 MB 37.2 MB/s eta 0:00:01\n",
      "     --------------------------------         8.7/10.7 MB 39.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  10.7/10.7 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 10.7/10.7 MB 36.3 MB/s eta 0:00:00\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3)\n",
      "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "                                              0.0/79.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 79.8/79.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\franc\\appdata\\roaming\\python\\python311\\site-packages (from botocore<1.30.0,>=1.29.133->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\franc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from botocore<1.30.0,>=1.29.133->boto3) (1.26.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\franc\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.133->boto3) (1.16.0)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.26.133 botocore-1.29.133 jmespath-1.0.1 s3transfer-0.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pepper-bucket\n",
      "pepper-labs-fruits\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource(\"s3\")\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old version de l'alternant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH:        c:\\Users\\franc\\Projects\\pepper_cloud_based_model\\notebooks\n",
      "PATH_Data:   c:\\Users\\franc\\Projects\\pepper_cloud_based_model\\notebooks/data/Test1\n",
      "PATH_Result_Local: c:\\Users\\franc\\Projects\\pepper_cloud_based_model\\notebooks/data/Results_Local\n"
     ]
    }
   ],
   "source": [
    "PATH = os.getcwd()\n",
    "PATH_Data = PATH+'/data/Test1'\n",
    "PATH_Result = PATH+'/data/Results_Local'\n",
    "print('PATH:        '+\\\n",
    "      PATH+'\\nPATH_Data:   '+\\\n",
    "      PATH_Data+'\\nPATH_Result_Local: '+PATH_Result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Cr√©ation de la SparkSession\n",
    "\n",
    "L‚Äôapplication Spark est contr√¥l√©e gr√¢ce √† un processus de pilotage (driver process) appel√© **SparkSession**.\n",
    "\n",
    "<u>Une instance de **SparkSession** est la fa√ßon dont Spark ex√©cute les fonctions d√©finies par l‚Äôutilisateur dans l‚Äôensemble du cluster</u>. <u>Une SparkSession correspond toujours √† une application Spark</u>.\n",
    "\n",
    "<u>Ici nous cr√©ons une session spark en sp√©cifiant dans l'ordre</u> :\n",
    "\n",
    "1. un **nom pour l'application**, qui sera affich√©e dans l'interface utilisateur Web Spark \"**P8**\"\n",
    "2. que l'application doit s'ex√©cuter **localement**.\n",
    "    * Nous ne d√©finissons pas le nombre de c≈ìurs √† utiliser (comme `.master('local[4])` pour 4 c≈ìurs √† utiliser),\n",
    "    * nous utiliserons donc tous les c≈ìurs disponibles dans notre processeur.\n",
    "3. une option de configuration suppl√©mentaire permettant d'utiliser le **format \"parquet\"** que nous utiliserons pour enregistrer et charger le r√©sultat de notre travail.\n",
    "4. vouloir **obtenir une session spark** existante ou si aucune n'existe, en cr√©er une nouvelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Fruits\")\n",
    "    .master(\"local\")\n",
    "    .config(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.app.submitTime', '1684166786102')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.sql.parquet.writeLegacyFormat', 'true')\n",
      "('spark.app.name', 'Fruits')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.sql.caseSensitive', 'True')\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.sql.caseSensitive\", \"True\")\n",
    "\n",
    "# Obtention des param√®tres de configuration\n",
    "configurations = conf.getAll()\n",
    "\n",
    "# Affichage des param√®tres de configuration\n",
    "for config in configurations:\n",
    "    print(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous affectons √† la variable **`sc`** le **`SparkContext`** attach√© √† l'objet **`spark`** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Affichage des informations de Spark en cours d'execution</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Fruits</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2717f545810>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Traitement des donn√©es\n",
    "\n",
    "<u>Dans la suite de notre flux de travail, nous allons successivement</u> :\n",
    "1. Pr√©parer nos donn√©es\n",
    "    1. Importer les images dans un dataframe **pandas UDF**\n",
    "    2. Associer aux images leur **label**\n",
    "    3. Pr√©processer en **redimensionnant nos images pour qu'elles soient compatibles avec notre mod√®le**\n",
    "2. Pr√©parer notre mod√®le\n",
    "    1. Importer le mod√®le **MobileNetV2**\n",
    "    2. Cr√©er un **nouveau mod√®le** d√©pourvu de la derni√®re couche de MobileNetV2\n",
    "3. D√©finir le processus de chargement des images et l'application de leur featurisation √† travers l'utilisation de pandas UDF\n",
    "3. Ex√©cuter les actions d'extraction de features\n",
    "4. Enregistrer le r√©sultat de nos actions\n",
    "5. Tester le bon fonctionnement en chargeant les donn√©es enregistr√©es"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.1 Chargement des donn√©es\n",
    "\n",
    "Les images sont charg√©es au format binaire pour en faciliter le pr√©-traitement.\n",
    "\n",
    "\n",
    "\n",
    "Avant de charger les images, nous sp√©cifions que nous voulons charger uniquement les fichiers dont l'extension est **jpg**.\n",
    "\n",
    "Nous indiquons √©galement de charger tous les objets possibles contenus dans les sous-dossiers du dossier communiqu√©.\n",
    "\n",
    "Code d'origine :\n",
    "```python\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Affichage des 5 premi√®res images contenant</u> :\n",
    " - le path de l'image\n",
    " - la date et heure de sa derni√®re modification\n",
    " - sa longueur\n",
    " - son contenu encod√© en valeur hexad√©cimal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Je ne conserve que le **path** de l'image et j'ajoute une colonne contenant les **labels** de chaque image</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n",
      "+---------------------------------------------------------------------------------------------------------------+----------+\n",
      "|path                                                                                                           |label     |\n",
      "+---------------------------------------------------------------------------------------------------------------+----------+\n",
      "|file:/C:/Users/franc/Projects/pepper_cloud_based_model/dataset/fruits-360_dataset/Test/Watermelon/r_106_100.jpg|Watermelon|\n",
      "|file:/C:/Users/franc/Projects/pepper_cloud_based_model/dataset/fruits-360_dataset/Test/Watermelon/r_109_100.jpg|Watermelon|\n",
      "|file:/C:/Users/franc/Projects/pepper_cloud_based_model/dataset/fruits-360_dataset/Test/Watermelon/r_108_100.jpg|Watermelon|\n",
      "|file:/C:/Users/franc/Projects/pepper_cloud_based_model/dataset/fruits-360_dataset/Test/Watermelon/r_107_100.jpg|Watermelon|\n",
      "|file:/C:/Users/franc/Projects/pepper_cloud_based_model/dataset/fruits-360_dataset/Test/Watermelon/r_95_100.jpg |Watermelon|\n",
      "+---------------------------------------------------------------------------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fruits.driver import init_spark_session, load_images\n",
    "spark = init_spark_session()\n",
    "images = load_images(spark)\n",
    "images.printSchema()\n",
    "images.select(\"path\", \"label\").show(5, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.2 Pr√©paration du mod√®le\n",
    "\n",
    "Je vais utiliser la technique du **transfert learning** pour extraire les features des images.\n",
    "\n",
    "J'ai choisi d'utiliser le mod√®le **MobileNetV2** pour sa rapidit√© d'ex√©cution compar√©e √† d'autres mod√®les comme *VGG16* par exemple.\n",
    "\n",
    "Pour en savoir plus sur la conception et le fonctionnement de MobileNetV2, je vous invite √† lire [cet article](https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c).\n",
    "\n",
    "<u>Voici le sch√©ma de son architecture globale</u> : \n",
    "\n",
    "![Architecture de MobileNetV2](../baseline/img/mobilenetv2_architecture.png)\n",
    "\n",
    "Il existe une derni√®re couche qui sert √† classer les images selon 1000 cat√©gories que nous ne voulons pas utiliser.\n",
    "\n",
    "L'id√©e dans ce projet est de r√©cup√©rer le **vecteur de caract√©ristiques de dimensions `(1, 1, 1280)`** qui servira, plus tard, au travers d'un moteur de classification √† reconna√Ætre les diff√©rents fruits du jeu de donn√©es.\n",
    "\n",
    "Comme d'autres mod√®les similaires, **MobileNetV2**, lorsqu'on l'utilise en incluant toutes ses couches, attend obligatoirement des images de dimension `(224, 224, 3)`. Nos images √©tant toutes de dimension `(100, 100, 3)`, nous devrons simplement les **redimensionner** avant de les confier au mod√®le.\n",
    "\n",
    "<u>Dans l'odre</u> :\n",
    " 1. Nous chargeons le mod√®le **MobileNetV2** avec les poids **pr√©calcul√©s** issus d'**imagenet** et en sp√©cifiant le format de nos images en entr√©e\n",
    " 2. Nous cr√©ons un nouveau mod√®le avec:\n",
    "  - <u>en entr√©e</u> : l'entr√©e du mod√®le MobileNetV2\n",
    "  - <u>en sortie</u> : l'avant derni√®re couche du mod√®le MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "model = MobileNetV2(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=True,\n",
    "    input_shape=(224, 224, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "new_model = Model(\n",
    "    inputs=model.input,\n",
    "    outputs=model.layers[-2].output\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage du r√©sum√© de notre nouveau mod√®le o√π nous constatons que <u>nous r√©cup√©rons bien en sortie un vecteur de dimension (1, 1, 1280)</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)                 (None, 112, 112, 32  864         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalization)  (None, 112, 112, 32  128         ['Conv1[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)              (None, 112, 112, 32  0           ['bn_Conv1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (Depth  (None, 112, 112, 32  288        ['Conv1_relu[0][0]']             \n",
      " wiseConv2D)                    )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN (Ba  (None, 112, 112, 32  128        ['expanded_conv_depthwise[0][0]']\n",
      " tchNormalization)              )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_relu (  (None, 112, 112, 32  0          ['expanded_conv_depthwise_BN[0][0\n",
      " ReLU)                          )                                ]']                              \n",
      "                                                                                                  \n",
      " expanded_conv_project (Conv2D)  (None, 112, 112, 16  512        ['expanded_conv_depthwise_relu[0]\n",
      "                                )                                [0]']                            \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (Batc  (None, 112, 112, 16  64         ['expanded_conv_project[0][0]']  \n",
      " hNormalization)                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)        (None, 112, 112, 96  1536        ['expanded_conv_project_BN[0][0]'\n",
      "                                )                                ]                                \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNormal  (None, 112, 112, 96  384        ['block_1_expand[0][0]']         \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)     (None, 112, 112, 96  0           ['block_1_expand_BN[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D)    (None, 113, 113, 96  0           ['block_1_expand_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_depthwise (DepthwiseCo  (None, 56, 56, 96)  864         ['block_1_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (BatchNor  (None, 56, 56, 96)  384         ['block_1_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (ReLU)  (None, 56, 56, 96)   0           ['block_1_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)       (None, 56, 56, 24)   2304        ['block_1_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_1_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_1_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_2_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_2_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_2_depthwise (DepthwiseCo  (None, 56, 56, 144)  1296       ['block_2_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (BatchNor  (None, 56, 56, 144)  576        ['block_2_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (ReLU)  (None, 56, 56, 144)  0           ['block_2_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)       (None, 56, 56, 24)   3456        ['block_2_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_2_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_add (Add)              (None, 56, 56, 24)   0           ['block_1_project_BN[0][0]',     \n",
      "                                                                  'block_2_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_2_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_3_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_3_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D)    (None, 57, 57, 144)  0           ['block_3_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_3_depthwise (DepthwiseCo  (None, 28, 28, 144)  1296       ['block_3_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (BatchNor  (None, 28, 28, 144)  576        ['block_3_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (ReLU)  (None, 28, 28, 144)  0           ['block_3_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)       (None, 28, 28, 32)   4608        ['block_3_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_3_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_3_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_4_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_4_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_4_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_4_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_4_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_4_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_4_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_4_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_add (Add)              (None, 28, 28, 32)   0           ['block_3_project_BN[0][0]',     \n",
      "                                                                  'block_4_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_4_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_5_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_5_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_5_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_5_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_5_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_5_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_5_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_5_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_add (Add)              (None, 28, 28, 32)   0           ['block_4_add[0][0]',            \n",
      "                                                                  'block_5_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_5_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_6_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_6_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D)    (None, 29, 29, 192)  0           ['block_6_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_6_depthwise (DepthwiseCo  (None, 14, 14, 192)  1728       ['block_6_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (BatchNor  (None, 14, 14, 192)  768        ['block_6_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (ReLU)  (None, 14, 14, 192)  0           ['block_6_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)       (None, 14, 14, 64)   12288       ['block_6_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_6_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_6_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_7_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_7_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_7_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_7_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_7_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_7_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_7_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_7_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_add (Add)              (None, 14, 14, 64)   0           ['block_6_project_BN[0][0]',     \n",
      "                                                                  'block_7_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_7_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_8_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_8_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_8_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_8_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_8_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_8_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_8_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_8_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_add (Add)              (None, 14, 14, 64)   0           ['block_7_add[0][0]',            \n",
      "                                                                  'block_8_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_8_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_9_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_9_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_9_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_9_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_9_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_9_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_9_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_9_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_add (Add)              (None, 14, 14, 64)   0           ['block_8_add[0][0]',            \n",
      "                                                                  'block_9_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)       (None, 14, 14, 384)  24576       ['block_9_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchNorma  (None, 14, 14, 384)  1536       ['block_10_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU)    (None, 14, 14, 384)  0           ['block_10_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_depthwise (DepthwiseC  (None, 14, 14, 384)  3456       ['block_10_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (BatchNo  (None, 14, 14, 384)  1536       ['block_10_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0          ['block_10_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)      (None, 14, 14, 96)   36864       ['block_10_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_10_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_10_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_10_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_11_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_11_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_11_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_11_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_11_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_11_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_11_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_11_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_11_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_add (Add)             (None, 14, 14, 96)   0           ['block_10_project_BN[0][0]',    \n",
      "                                                                  'block_11_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_11_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_12_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_12_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_12_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_12_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_12_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_12_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_12_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_12_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_12_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_add (Add)             (None, 14, 14, 96)   0           ['block_11_add[0][0]',           \n",
      "                                                                  'block_12_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_12_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_13_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_13_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2D)   (None, 15, 15, 576)  0           ['block_13_expand_relu[0][0]']   \n",
      "                                                                                                  \n",
      " block_13_depthwise (DepthwiseC  (None, 7, 7, 576)   5184        ['block_13_pad[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (BatchNo  (None, 7, 7, 576)   2304        ['block_13_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)   0           ['block_13_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)      (None, 7, 7, 160)    92160       ['block_13_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_13_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_13_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_13_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_14_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_14_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_14_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_14_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_14_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_14_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_14_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_14_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_14_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_add (Add)             (None, 7, 7, 160)    0           ['block_13_project_BN[0][0]',    \n",
      "                                                                  'block_14_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_14_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_15_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_15_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_15_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_15_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_15_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_15_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_15_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_15_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_15_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_add (Add)             (None, 7, 7, 160)    0           ['block_14_add[0][0]',           \n",
      "                                                                  'block_15_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_15_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_16_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_16_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_16_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_16_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_16_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_16_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)      (None, 7, 7, 320)    307200      ['block_16_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_16_project_BN (BatchNorm  (None, 7, 7, 320)   1280        ['block_16_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)                (None, 7, 7, 1280)   409600      ['block_16_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)  5120        ['Conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " out_relu (ReLU)                (None, 7, 7, 1280)   0           ['Conv_1_bn[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1280)        0           ['out_relu[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 2,223,872\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Affichage d√©grad√© que je ne m'explique pas, par rapport √† l'original :\n",
    "# pas de lignes blanches s√©paratrices\n",
    "# retour √† la ligne en seconde colonne\n",
    "# Vu la pression que j'ai au temps, pas le temps de regarder cela, mais je n'aime pas\n",
    "display(new_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 112, 112, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 112, 112, 32) 128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, 112, 112, 32) 0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 112, 112, 32) 288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 112, 112, 32) 128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, 112, 112, 32) 0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 112, 112, 16) 512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 112, 112, 16) 64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 112, 112, 96) 1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 112, 112, 96) 384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, 112, 112, 96) 0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_pad (ZeroPadding2D)     (None, 113, 113, 96) 0           block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 56, 56, 96)   864         block_1_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 56, 56, 96)   384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, 56, 56, 96)   0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 56, 56, 24)   2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 56, 56, 24)   96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 56, 56, 144)  3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 56, 56, 144)  1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 56, 56, 144)  576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, 56, 56, 144)  0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 56, 56, 24)   3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 56, 56, 24)   96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 56, 56, 24)   0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 56, 56, 144)  3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_pad (ZeroPadding2D)     (None, 57, 57, 144)  0           block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 28, 28, 144)  1296        block_3_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 28, 28, 144)  576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, 28, 28, 144)  0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 28, 28, 32)   4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 28, 28, 32)   128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 28, 28, 192)  6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 28, 28, 32)   6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 28, 28, 32)   128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 28, 28, 32)   0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 28, 28, 192)  6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 28, 28, 32)   6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 28, 28, 32)   128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 28, 28, 32)   0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, 28, 28, 192)  6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_pad (ZeroPadding2D)     (None, 29, 29, 192)  0           block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, 14, 14, 192)  1728        block_6_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, 14, 14, 192)  768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, 14, 14, 192)  0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, 14, 14, 64)   12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, 14, 14, 64)   256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, 14, 14, 384)  24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, 14, 14, 64)   24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, 14, 14, 64)   256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, 14, 14, 64)   0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, 14, 14, 384)  24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, 14, 14, 64)   24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, 14, 14, 64)   256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, 14, 14, 64)   0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, 14, 14, 384)  24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, 14, 14, 64)   24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, 14, 14, 64)   256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, 14, 14, 64)   0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, 14, 14, 384)  24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, 14, 14, 384)  1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, 14, 14, 384)  0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, 14, 14, 384)  3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, 14, 14, 384)  1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, 14, 14, 96)   36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, 14, 14, 96)   384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, 14, 14, 576)  55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, 14, 14, 96)   55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, 14, 14, 96)   384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, 14, 14, 96)   0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, 14, 14, 576)  55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, 14, 14, 96)   55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, 14, 14, 96)   384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, 14, 14, 96)   0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, 14, 14, 576)  55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_pad (ZeroPadding2D)    (None, 15, 15, 576)  0           block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, 7, 7, 576)    5184        block_13_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, 7, 7, 576)    2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)    0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, 7, 7, 160)    92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, 7, 7, 160)    640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, 7, 7, 960)    153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, 7, 7, 160)    153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, 7, 7, 160)    640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, 7, 7, 160)    0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, 7, 7, 960)    153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, 7, 7, 160)    153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, 7, 7, 160)    640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, 7, 7, 160)    0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, 7, 7, 960)    153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, 7, 7, 320)    307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, 7, 7, 320)    1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 7, 7, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1280)         0           out_relu[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 2,223,872\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les workeurs doivent pouvoir acc√©der au mod√®le ainsi qu'√† ses poids. Une bonne pratique consiste √† charger le mod√®le sur le driver puis √† diffuser ensuite les poids aux diff√©rents workeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_weights = sc.broadcast(new_model.get_weights())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Mettons cela sous forme de fonctions</u> :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöß **TODO** Pourquoi √ßa ne fonctionne pas sous cette forme ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fruits.model import init_keras_model\n",
    "from fruits.driver import (\n",
    "    init_spark_session,\n",
    "    broadcast_model_weights\n",
    ")\n",
    "\n",
    "spark = init_spark_session()\n",
    "model = init_keras_model()\n",
    "broadcast_model_weights(spark, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version d'origine\n",
    "\n",
    "```python\n",
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=True,\n",
    "        input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(\n",
    "        inputs=model.input,\n",
    "        outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(broadcast_weights.value)\n",
    "    return new_model\n",
    "```\n",
    "\n",
    "Avec juste quelques renommages.\n",
    "\n",
    "Notons que broadcast est pass√© comme variable globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_keras_model() -> Model:\n",
    "    \"\"\"Returns a MobileNetV2 model with the top layer removed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Model\n",
    "        The initialized model.\n",
    "    \"\"\"\n",
    "    # Create a MobileNetV2 model with pre-trained weights\n",
    "    model = MobileNetV2(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=True,\n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    \n",
    "    # Set all layers in the model as non-trainable\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Create a new model without the top layer\n",
    "    new_model = Model(\n",
    "        inputs=model.input,\n",
    "        outputs=model.layers[-2].output\n",
    "    )\n",
    "    \n",
    "    # Broadcast the model weights\n",
    "    new_model.set_weights(broadcast_weights.value)\n",
    "    \n",
    "    # Return the model\n",
    "    return new_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.3 D√©finition du processus de chargement des images et application <br/>de leur featurisation √† travers l'utilisation de pandas UDF\n",
    "\n",
    "Ce notebook d√©finit la logique par √©tapes, jusqu'√† Pandas UDF.\n",
    "\n",
    "<u>L'empilement des appels est la suivante</u> :\n",
    "\n",
    "* Pandas UDF\n",
    "    * extraire les caract√©ristiques de la collection d'images (`pandas.Series`) \n",
    "    * pr√©-traiter une image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöß **TODO** Pourquoi √ßa ne fonctionne pas sous cette forme ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idem, ces fonctions sont retravaill√©es, renomm√©es et d√©plac√©es dans `fruits.executor` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fruits.executor import (\n",
    "    preprocess_img,\n",
    "    extract_image_features,\n",
    "    extract_image_features_udf\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version d'origine\n",
    "\n",
    "```python\n",
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)\n",
    "```\n",
    "\n",
    "Avec quelques renommages :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Any, Iterator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import pandas_udf  # , PandasUDFType : Spark 3.4.0\n",
    "\n",
    "def preprocess_img(content: Union[bytes, bytearray]) -> Any:\n",
    "    \"\"\"Pre-processes raw image bytes for prediction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    content : Union[bytes, bytearray]\n",
    "        Raw image bytes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Any\n",
    "        Pre-processed image data.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the provided content is not valid image bytes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the image from raw bytes and resize it\n",
    "        img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "        # Convert the image to an array\n",
    "        arr = img_to_array(img)\n",
    "        return preprocess_input(arr)\n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "            \"Invalid image content. Please provide valid image bytes.\"\n",
    "        ) from e\n",
    "\n",
    "\n",
    "def extract_image_features(\n",
    "    model: Model,\n",
    "    content_series: pd.Series\n",
    ") -> pd.Series:\n",
    "    \"\"\"Extracts image features from a pd.Series of raw images using the input\n",
    "    model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Model\n",
    "        The pre-trained model used for feature extraction.\n",
    "    content_series : pd.Series\n",
    "        The pd.Series containing raw image data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        The pd.Series containing the extracted image features.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    The function assumes that the `preprocess_img` function is defined separately.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the model is not a valid TensorFlow Keras model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocess the images in the content_series\n",
    "        prep_imgs = np.stack(content_series.map(preprocess_img))\n",
    "        # Extract the features using the model\n",
    "        feats = model.predict(prep_imgs)\n",
    "        # Flatten the feature tensors to vectors\n",
    "        flat_feats = [f.flatten() for f in feats]\n",
    "        return pd.Series(flat_feats)\n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "            \"Invalid model. Please provide a valid TensorFlow Keras model.\"\n",
    "        ) from e\n",
    "\n",
    "\n",
    "# See : https://www.databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html\n",
    "@pandas_udf(\"array<float>\")  # , PandasUDFType.SCALAR_ITER) = warning\n",
    "def extract_image_features_udf(\n",
    "    content_series_iter: Iterator[pd.Series]\n",
    ") -> Iterator[pd.Series]:\n",
    "    \"\"\"This method is a Scalar Iterator pandas UDF wrapping our\n",
    "    `extract_image_features` function. The decorator specifies that this\n",
    "    returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    content_series_iter : Iterator[pd.Series]\n",
    "        An iterator over batches of data, where each batch is a pandas Series\n",
    "        of image data.\n",
    "    Yields\n",
    "    ------\n",
    "    Iterator[pd.Series]\n",
    "        An iterator over the extracted image features for each batch of data.\n",
    "    \"\"\"\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then\n",
    "    # re-use it for multiple data batches. This amortizes the overhead of\n",
    "    # loading big models.\n",
    "    # model_weights = spark.sparkContext.broadcast(broadcast_weights.value)\n",
    "    model = init_keras_model()\n",
    "    # model.set_weights(model_weights.values)\n",
    "    for content_series in content_series_iter:\n",
    "        yield extract_image_features(model, content_series)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.4 Ex√©cution des actions d'extraction de features\n",
    "\n",
    "Les Pandas UDF, sur de grands enregistrements (par exemple, de tr√®s grandes images), peuvent rencontrer des erreurs de type `Out Of Memory` (OOM). Si vous rencontrez de telles erreurs dans la cellule ci-dessous, essayez de r√©duire la taille du lot Arrow via `maxRecordsPerBatch`\n",
    "\n",
    "Je n'utiliserai pas cette commande dans ce projet et je laisse donc la commande en commentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant ex√©cuter la featurisation sur l'ensemble de notre DataFrame Spark.\n",
    "\n",
    "<u>REMARQUE</u> : Cela peut prendre beaucoup de temps, tout d√©pend du volume de donn√©es √† traiter.\n",
    "\n",
    "Notre jeu de donn√©es de **Test** contient **22 819 images**.\n",
    "\n",
    "Cependant, dans l'ex√©cution en mode **local**, nous <u>traiterons un ensemble r√©duit de **330 images**</u>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöß **TODO** Pourquoi √ßa ne fonctionne pas sous cette forme ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fruits.driver import init_spark_session, load_images\n",
    "\n",
    "spark = init_spark_session()\n",
    "images = load_images(spark)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version d'origine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fruits.executor import extract_image_features_udf\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#features_df = images.repartition(20).select(\n",
    "im_feats = images.repartition(20).select(\n",
    "    col(\"path\"), col(\"label\"),\n",
    "    extract_image_features_udf(\"content\").alias(\"features\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappel du chemin du r√©pertoire o√π seront enregistr√©s nos r√©sultats en plusieurs fichiers au format **`parquet`**.\n",
    "\n",
    "Nos r√©sultats se pr√©sentent sous la forme d'un DataFrame √† 3 colonnes :\n",
    " 1. `path` le chemin de l'image\n",
    " 2. `label` la classe de l'image\n",
    " 3. `features`, le vecteur de caract√©ristiques de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/walduch/Documents/P8/data/Results\n"
     ]
    }
   ],
   "source": [
    "print(PATH_Result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Enregistrement des donn√©es trait√©es au format \"**parquet**\"</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, label: string, features: array<float>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(im_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\franc\\\\Projects\\\\pepper_cloud_based_model\\\\tmp\\\\im_feats.pqt'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pepper.env import get_tmp_dir\n",
    "import os\n",
    "im_feats_pqt_path = os.path.join(get_tmp_dir(), \"im_feats.pqt\")\n",
    "display(im_feats_pqt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Fruits</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2717f545810>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o98.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 841) (host.docker.internal executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/franc/Projects/pepper_cloud_based_model/tmp/im_feats.pqt.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\franc\\Projects\\pepper_cloud_based_model\\modules\\fruits\\executor.py\", line 93, in <module>\n    @pandas_udf(\"array<float>\")  # , PandasUDFType.SCALAR_ITER) = warning\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\functions.py\", line 461, in _create_pandas_udf\n    return _create_udf(f, returnType, evalType)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 82, in _create_udf\n    return udf_obj._wrapped()\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 431, in _wrapped\n    wrapper.returnType = self.returnType  # type: ignore[attr-defined]\n                         ^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 236, in returnType\n    self._returnType_placeholder = _parse_datatype_string(self._returnType)\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1211, in _parse_datatype_string\n    sc = get_active_spark_context()\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 201, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n\t... 15 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/franc/Projects/pepper_cloud_based_model/tmp/im_feats.pqt.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\franc\\Projects\\pepper_cloud_based_model\\modules\\fruits\\executor.py\", line 93, in <module>\n    @pandas_udf(\"array<float>\")  # , PandasUDFType.SCALAR_ITER) = warning\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\functions.py\", line 461, in _create_pandas_udf\n    return _create_udf(f, returnType, evalType)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 82, in _create_udf\n    return udf_obj._wrapped()\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 431, in _wrapped\n    wrapper.returnType = self.returnType  # type: ignore[attr-defined]\n                         ^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 236, in returnType\n    self._returnType_placeholder = _parse_datatype_string(self._returnType)\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1211, in _parse_datatype_string\n    sc = get_active_spark_context()\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 201, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n\t... 15 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m im_feats\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mmode(\u001b[39m\"\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mparquet(im_feats_pqt_path)\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1656\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1654\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   1655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[1;32m-> 1656\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o98.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 841) (host.docker.internal executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/franc/Projects/pepper_cloud_based_model/tmp/im_feats.pqt.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\franc\\Projects\\pepper_cloud_based_model\\modules\\fruits\\executor.py\", line 93, in <module>\n    @pandas_udf(\"array<float>\")  # , PandasUDFType.SCALAR_ITER) = warning\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\functions.py\", line 461, in _create_pandas_udf\n    return _create_udf(f, returnType, evalType)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 82, in _create_udf\n    return udf_obj._wrapped()\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 431, in _wrapped\n    wrapper.returnType = self.returnType  # type: ignore[attr-defined]\n                         ^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 236, in returnType\n    self._returnType_placeholder = _parse_datatype_string(self._returnType)\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1211, in _parse_datatype_string\n    sc = get_active_spark_context()\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 201, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n\t... 15 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/franc/Projects/pepper_cloud_based_model/tmp/im_feats.pqt.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\franc\\Projects\\pepper_cloud_based_model\\modules\\fruits\\executor.py\", line 93, in <module>\n    @pandas_udf(\"array<float>\")  # , PandasUDFType.SCALAR_ITER) = warning\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\functions.py\", line 461, in _create_pandas_udf\n    return _create_udf(f, returnType, evalType)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 82, in _create_udf\n    return udf_obj._wrapped()\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 431, in _wrapped\n    wrapper.returnType = self.returnType  # type: ignore[attr-defined]\n                         ^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\udf.py\", line 236, in returnType\n    self._returnType_placeholder = _parse_datatype_string(self._returnType)\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1211, in _parse_datatype_string\n    sc = get_active_spark_context()\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 201, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n\t... 15 more\r\n"
     ]
    }
   ],
   "source": [
    "im_feats.write.mode(\"overwrite\").parquet(im_feats_pqt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.write.mode(\"overwrite\").parquet(PATH_Result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ICI UN POINT TRES DOULOUREUX**\n",
    "\n",
    "Pass√© 6h30 sans avancer, √† tourner en rond, rien ne fonctionne et surtout, je ne comprends pas le broadcasting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Chargement des donn√©es enregistr√©es et validation du r√©sultat\n",
    "\n",
    "<u>On charge les donn√©es fraichement enregistr√©es dans un **DataFrame Pandas**</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(PATH_Result, engine=\"pyarrow\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>On affiche les 5 premi√®res lignes du DataFrame</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/home/walduch/Documents/P8/data/Test1/App...</td>\n",
       "      <td>Apple Braeburn</td>\n",
       "      <td>[0.86105645, 0.16019525, 0.0, 0.0, 0.0, 1.0233...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/home/walduch/Documents/P8/data/Test1/Cle...</td>\n",
       "      <td>Clementine</td>\n",
       "      <td>[0.45963708, 0.0, 0.0, 0.0, 0.036376934, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/home/walduch/Documents/P8/data/Test1/Cle...</td>\n",
       "      <td>Clementine</td>\n",
       "      <td>[1.3859445, 0.04571251, 0.0, 0.0, 0.9309062, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/home/walduch/Documents/P8/data/Test1/App...</td>\n",
       "      <td>Apple Braeburn</td>\n",
       "      <td>[1.7865905, 0.20313944, 0.0, 0.0, 0.41594356, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/home/walduch/Documents/P8/data/Test1/App...</td>\n",
       "      <td>Apple Braeburn</td>\n",
       "      <td>[0.81415516, 0.18681705, 0.0, 0.0, 0.0, 0.3806...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path           label  \\\n",
       "0  file:/home/walduch/Documents/P8/data/Test1/App...  Apple Braeburn   \n",
       "1  file:/home/walduch/Documents/P8/data/Test1/Cle...      Clementine   \n",
       "2  file:/home/walduch/Documents/P8/data/Test1/Cle...      Clementine   \n",
       "3  file:/home/walduch/Documents/P8/data/Test1/App...  Apple Braeburn   \n",
       "4  file:/home/walduch/Documents/P8/data/Test1/App...  Apple Braeburn   \n",
       "\n",
       "                                            features  \n",
       "0  [0.86105645, 0.16019525, 0.0, 0.0, 0.0, 1.0233...  \n",
       "1  [0.45963708, 0.0, 0.0, 0.0, 0.036376934, 0.0, ...  \n",
       "2  [1.3859445, 0.04571251, 0.0, 0.0, 0.9309062, 0...  \n",
       "3  [1.7865905, 0.20313944, 0.0, 0.0, 0.41594356, ...  \n",
       "4  [0.81415516, 0.18681705, 0.0, 0.0, 0.0, 0.3806...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>On valide que la dimension du vecteur de caract√©ristiques des images est bien de dimension 1280</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.loc[0, \"features\"].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous venons de valider le processus sur un jeu de donn√©es all√©g√© en local o√π nous avons simul√© un cluster de machines en r√©partissant la charge de travail sur diff√©rents c≈ìurs de processeur au sein d'une m√™me machine.\n",
    "\n",
    "Nous allons maintenant g√©n√©raliser le processus en d√©ployant notre solution sur un r√©el cluster de machines et nous travaillerons d√©sormais sur la totalit√© des $22\\,819$ images de notre dossier `Test`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4\\. D√©ploiement de la solution sur le cloud\n",
    "\n",
    "Nous avons pu valider le fonctionnement de notre solution sur un cluster local. Il s'agit √† pr√©sent de le d√©ployer sur une infrastructure *√©lastique*, c'est-√†-dire capable de se redimensionner pour accompagner la mont√©e en puissance et donc en charge de l'application.\n",
    "\n",
    "**Attention**, *je travaille sous Linux avec une version Ubuntu, les commandes d√©crites ci-dessous sont donc r√©alis√©es  exclusivement dans cet environnement.*\n",
    "\n",
    "Pour arr√™ter un choix d'architecture technique, r√©pondons √† ces 4 questions :\n",
    "1. Quel prestataire de Cloud choisir ?\n",
    "2. Quelles solutions de ce prestataire adopter ?\n",
    "3. O√π stocker nos donn√©es ?\n",
    "4. Comment configurer nos outils dans ce nouvel environnement ?\n",
    "\n",
    "## 4.1 Choix du prestataire cloud : AWS\n",
    "\n",
    "Le leader incontest√© du march√© du cloud computing en son pr√©curseur historique. L'offre **Amazon Web Services** (AWS) d'Amazon demeure √† ce jour la plus large et la plus compl√®te dans le domaine du cloud computing, m√™me si ses challengers (Google, Microsoft, etc) n'ont pas √† rougir de leur offre, en particulier sur le segment du Big Data.\n",
    "\n",
    "D'autres acteurs proposent des offres plus sp√©cialis√©es ou plus comp√©titives qu'il faut pouvoir envisager selon les sp√©cificit√©s de chaque projet. Mais en l'occurrence, notre projet est une application classique simple bas√©e sur quelques librairies standard bien √©tablies qui ne n√©cessite donc pas d'explorer de telles voies et peut se satisfaire de micro-services standards.\n",
    "\n",
    "Nous choisissons donc AWS, pour cette raison pratique, pour la facilit√© et la rapidit√© de mise en oeuvre que nous lui connaissons d√©j√†, ainsi que pour les possibilit√©s avanc√©es de contr√¥le des co√ªts et de modulations tarifaires offertes par ce prestataire (possibilit√© de choisir les modalit√©s tarifaires les plus avantageuses selon le profil analytique de charge).\n",
    "\n",
    "L'objectif imm√©diat est de pouvoir louer de la puissance de calcul √† la demande (donc sans immobilisation), de d√©ployer facilement en se d√©chargeant de la responsabilit√© de maintenance √©volutive de l'infrastructure, de disposer d'un dimensionnement automatique transparent des ressources de service pour maintenir une continuit√© de service √† mesure qu'augmente la base d'utilisateurs de l'application, de ma√Ætriser les co√ªts induits √† l'aide de tableaux de bord analytiques.\n",
    "\n",
    "## 4.2 Choix de la solution technique : EMR\n",
    "\n",
    "Classiquement (depuis le mod√®le OSI des ann√©es 70), le marketing IT segmente les offres par niveaux d'abstraction informatique et remanie le corpus pour renforcer l'id√©e de nouveaut√©.\n",
    "\n",
    "Dans le domaine du *cloud computing*, o√π les ressources sont lou√©es *as a Service* utilis√© [*√† la demande* (ann√©es 2000)](https://www.csoonline.com/article/2115856/ibm-s-on-demand-strategy.html), nous distinguerons essentiellement les deux niveaux auxquels nous pouvons nous d√©ployer en tant que :\n",
    "1. Sur une offre de solution **IaaS** (Infrastructure as a Service) : h√©bergement classique avec location de l'infrastructure.\n",
    "2. Sur une offre de solution **PaaS** (Plateforme as a Service) : approche orient√©e micro-services, avec location des seuls espace de stockage et puissance de calcul, et donc avec une virtualisation (abstraction et d√©couplage) totale des couches mat√©rielles.\n",
    "\n",
    "### Solution **IaaS**\n",
    "\n",
    "Dans cette configuration **AWS** met √† notre disposition des serveurs vierges (*instances EC2*) que nous pouvons directement administrer.\n",
    "\n",
    "Avec une telle solution, nous pouvons reproduire pratiquement √† l'identique la solution mise en ≈ìuvre localement sur notre machine.\n",
    "\n",
    "Nous installons nous-m√™mes l'ensemble des outils et d√©pendances dont nous avons besoin, puis nous soumettons notre script :\n",
    "* Installation de **Spark**, **Java**, **Python**, **Jupyter Notebook**, et des **librairies compl√©mentaires**\n",
    "* Il nous faudra notamment veiller √† configurer **chacune des machines (workers) du cluster**\n",
    "\n",
    "|**Avantages**|**Inconv√©nients**|\n",
    "|-|-|\n",
    "|- **Libert√© totale** de mise en ≈ìuvre de la solution<br/>- **Facilit√© de mise en ≈ìuvre** √† partir d'un mod√®le qui s'ex√©cute en local sur une machine Linux|- **Chronophage** : il est n√©cessit√© d'installer et de configurer toute la solution<br/>- Possible **probl√®mes d'installation** des outils (des probl√©matiques qui n'existaient pas en local sur notre machine peuvent appara√Ætre sur le serveur EC2)<br/>- Solution **non p√©renne**, il faudra veiller √† la mise √† jour des outils et √©ventuellement devoir r√©installer Spark, Java etc.|\n",
    "\n",
    "### Solution **PaaS**\n",
    "\n",
    "La galaxie des micro-services **AWS** est l'une des plus riches de l'univers du *cloud*.\n",
    "\n",
    "En particulier, Amazon nous propose une offre de plate-forme Big Data, son offre [EMR (*Elastic MapReduce*)](https://aws.amazon.com/fr/emr/) qui prend en charge Apache Spark, Hive, Presto et autres applications Big Data.\n",
    "\n",
    "\n",
    "fournit √©norm√©ment de services diff√©rents, dans l'un de ceux-l√† il existe une offre qui permet de louer des **instances EC2** avec des applications pr√©install√©es et configur√©es : il s'agit du **service EMR**.\n",
    "* **Spark** y sera d√©j√† pr√©-install√©\n",
    "* Il est possible de demander l'installation de :\n",
    "    * **Tensorflow** et **JupyterHub**\n",
    "    * des **packages compl√©mentaires**\n",
    "* .. **sur l'ensemble des workers du cluster**.\n",
    "\n",
    "\n",
    "|**Avantages**|**Inconv√©nients**|\n",
    "|-|-|\n",
    "|- **Facilit√© de mise en ≈ìuvre** : Il suffit de tr√®s peu de configuration pour obtenir un environnement parfaitement op√©rationnel<br/>- **Rapidit√© de mise en ≈ìuvre** : Une fois la premi√®re configuration r√©alis√©e, il est tr√®s facile et tr√®s rapide de recr√©er des clusters √† l'identique qui seront disponibles presque instantan√©ment (le temps d'instancier les serveurs soit environ 15/20 minutes)<br/>- **Solutions mat√©rielles et logicielles optimis√©es** par les ing√©nieurs d'AWS : On sait que les versions install√©es vont fonctionner et que l'architecture propos√©e est optimis√©e<br/>- **Stabilit√© de la solution**<br/>- **Solution √©volutive** : Il est facile d‚Äôobtenir √† chaque nouvelle instanciation une version √† jour de chaque package, en √©tant garanti de leur compatibilit√© avec le reste de l‚Äôenvironnement<br/>- **Maintenance de s√©curis√©** : Les √©ventuels patchs de s√©curit√© seront automatiquement mis √† jour √† chaque nouvelle instanciation du cluster EMR.|<br/>- Peut-√™tre un certain **manque de libert√©** sur la version des packages disponibles ? M√™me si je n'ai pas constat√© ce probl√®me.|\n",
    "\n",
    "### Choix retenu\n",
    "\n",
    "Nous sommes actuellement en phase d'amor√ßage du d√©ploiement de l'application sur une infrastructure capable de monter en charge.\n",
    "\n",
    "La bonne solution est √©videmment de nous focaliser sur un d√©ploiement rapide sur une plate-forme qui nous assure la **continuit√© de service** et la *scalabilit√©* sans nous faire courir les risques et les co√ªts induits par une approche *IaaS*.\n",
    "\n",
    "Il sera toujours possible d'opter, apr√®s une premi√®re phase d'observation de la production, pour une architecture technique plus *√† fa√ßon* et √©ventuellement bas√©e sur l'offre *IaaS*, mais bien plus probablement bas√©e sur une int√©gration horizontale de micro-services *PaaS* alternatifs ou compl√©mentaires √† *EMR*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Choix de la solution de stockage des donn√©es : Amazon S3\n",
    "\n",
    "Nous pourrions √™tre tent√©s de stocker nos donn√©es d'application sur l'espace de nos serveurs **EC2** sous-jacents. En effet, un principe fondamental des approches Big Data est d'assurer autant que possible la *collocalisation* des donn√©es et des traitements qui s'y rapportent.\n",
    "\n",
    "Mais cela pose trois probl√®mes que nous pr√©f√©rons √©viter :\n",
    "1. Ce mode de stockage des donn√©es est **plus on√©reux** que l'utilisation de services de stockage sp√©cialis√©s.\n",
    "2. En cas de r√©siliation d'une instance EMR, pour cause d'inactivit√© (un EMR co√ªte, m√™me s'il n'est pas utilis√©), ou de simple bascule d'une instance sur une autre, √† d√©faut d'une solution suppl√©mentaire de *backup*, les donn√©es seraient perdues.\n",
    "3. Nous nous exposons √† des **risques de ralentissements voire dysfonctionnements de service** en raison du risque de saturation de l'espace disponible, limit√©. \n",
    "\n",
    "**saturer** l'espace disponible de nos serveurs (ralentissements, dysfonctionnements).\n",
    "\n",
    "Amazon propose un service de stockage des donn√©es, [**S3** (*Simple Storage Service*)](https://aws.amazon.com/fr/s3/), qui nous permet de nous affranchir de ces 3 probl√©matiques :\n",
    "* Co√ªt de stockage comp√©titif.\n",
    "* Persistence d√©coupl√©e du cycle de vie des instances EMR.\n",
    "* Espace **illimit√©** (certes c'est une vue de l'esprit, mais disons que le risque de saturation est proche de 0).\n",
    "\n",
    "En outre, en faisant le choix d'un service de stockage AWS, et en prenant notamment soin de choisir la m√™me r√©gion (c'est-√†-dire le m√™me Data Center ou bien des Data Centers g√©ographiquement proches) pour nos serveurs **EC2** et **S3**, nous nous assurons de minimiser la latence dans l'acc√®s aux donn√©es depuis l'EMR.\n",
    "\n",
    "<mark>?? De plus, comme nous le verrons <u>il est possible d'acc√©der aux donn√©es sur **S3** de la m√™me mani√®re que l'on **acc√®de aux donn√©es sur un disque local**</u> -> ?? acc√®s s√©quentiel, non ?</mark>\n",
    "\n",
    "L'ensemble des donn√©es sera stock√© dans le compartiment s√©curis√© (un *bucket* non accessible de l'Internet) **`s3://pepper-labs-fruits`**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Configuration de l'environnement de travail\n",
    "\n",
    "La premi√®re √©tape est de cr√©er un utilisateur distinct du compte racine (Root), pour des raisons √©videntes de s√©curit√©.\n",
    "\n",
    "Cet utilisateur doit disposer de droits suffisants pour op√©rer comme d√©veloppeur et comme administrateur des services vis√©s, donc disposer d'un contr√¥le total sur les services EMR et S3.\n",
    "\n",
    "La cr√©ation des utilisateurs r√©guliers, des groupes (par exemple *Developers*), la d√©finition et l'attribution des droits, s'effectue depuis un service incontournable d'AWS, [**IAM** (Identity and Access Management)](https://aws.amazon.com/fr/iam/).\n",
    "\n",
    "Nous choisissons, pour l'utilisateur *Root*, comme pour l'utilisateur r√©gulier, *Pepper*, la mise en place d'une authentification [**MFA** (Multi-Factor Authentication)](https://aws.amazon.com/fr/iam/features/mfa/) comme barri√®re √† l'entr√©e de la console AWS, ce qui nous assure le meilleur niveau de s√©curit√©. En effet, dans le cadre d'une architecture cloud, le d√©tournement d'un compte, en particulier un compte qui dispose des droits les plus √©tendus, pourrait avoir des cons√©quences dramatiques.\n",
    "\n",
    "Pour acc√©der ensuite √† distance, en ligne de commande, ou encore programmatiquement via les APIs d'AWS (boto3) aux services AWS sans passer par l'interface Web de la console AWS, il est n√©cessaire de g√©n√©rer des paires de cl√©s publique/priv√©e (RSA), chaque utilisateur devant prendre le plus grand soin √† conserver sa cl√© priv√©e secr√®te et donc √† ne la communiquer sous aucun pr√©texte √† qui que ce soit (s√ªret√©).\n",
    "\n",
    "Cette cl√© priv√©e sera le s√©same pour ouvrir les diff√©rentes serrures (commande AWS, acc√®s SSH si n√©cessaires (par exemple √† nos instances EC2), d√©chiffrement de mot de passe administrateur (par exemple si nous installions un serveur Windows), interactions programmatiques via l'API, etc). Notons qu'il est possible de multiplier ces cl√©s, et m√™me de les attribuer pour un usage unique, afin de renforcer la s√©curit√©, mais nous laissons √† l'√©quipe des Administrateurs Syst√®me le soin d'aligner la configuration d'IAM avec l'annuaire et les politiques de l'entreprise.\n",
    "\n",
    "Pour ce qui les manipulations que nous allons effectuer dans la suite, il sera pratique d'utiliser [**AWS CLI**](https://aws.amazon.com/fr/cli/), l'interface en ligne de commande d'AWS, qui permet d'interagir directement depuis un terminal (Linux ou PowerShell) avec les diff√©rents services d'AWS**. Cela nous permettra par exemple de lister le contenu de notre compartiment S3, d'en t√©l√©charger ou d'y t√©l√©verser des fichiers, avec la m√™me facilit√© que si ces fichiers √©taient locaux."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Upload de nos donn√©es sur S3\n",
    "\n",
    "Nos outils sont configur√©s.\n",
    "\n",
    "Il faut maintenant de t√©l√©verser nos donn√©es de travail sur Amazon S3.\n",
    "\n",
    "Ces donn√©es sont les images contenues dans le r√©pertoire `Test` du jeu de donn√©es t√©l√©charg√© sur [**Kaggle**](https://www.kaggle.com/moltean/fruits/download).\n",
    "\n",
    "Cr√©ons le compartiment S3 :\n",
    "\n",
    "```sh\n",
    "$ aws s3 mb s3://pepper-labs-fruits\n",
    "```\n",
    "\n",
    "V√©rifions qu'il a bien √©t√© cr√©√© :\n",
    "\n",
    "```sh\n",
    "$ aws s3 ls\n",
    "2023-05-15 20:31:10 pepper-labs-fruits\n",
    "```\n",
    "\n",
    "Copions √† pr√©sent le contenu du dossier `Test` dans un r√©pertoire `Test` sur notre bucket `p8-data` :\n",
    "\n",
    "On se place √† l'int√©rieur du r√©pertoire `Test` et on synchronise les contenus local et distant √† l'aide de la commande `sync` :\n",
    "\n",
    "```sh\n",
    "$ cd <fruits-360_dataset_path>/Test\n",
    "$ aws sync . s3://pepper-labs-fruits/Test\n",
    "```\n",
    "\n",
    "Nos donn√©es de projet sont √† pr√©sent disponibles sur Amazon S3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Configuration du serveur EMR\n",
    "\n",
    "Une fois encore, le cours [R√©alisez des calculs distribu√©s sur des donn√©es massives / D√©ployez un cluster de calculs distribu√©s](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues) <br /> d√©taille l'essentiel des √©tapes pour lancer un cluster avec **EMR**.\n",
    "\n",
    "R√©√©crire les r√©f√©rences 1/ en plus dense 2/ en plus '√† la source'\n",
    "\n",
    "----\n",
    "\n",
    "Je d√©taillerai ici les √©tapes particuli√®res qui nous permettent de configurer le serveur selon nos besoins :\n",
    "1. Cliquez sur `Cr√©er un cluster` :\n",
    "![Cr√©er un cluster](../baseline/img/EMR_creer.png)\n",
    "\n",
    "2. Cliquez sur `Acc√©der aux options avanc√©es` :\n",
    "![Cr√©er un cluster](../baseline/img/EMR_options_avancees.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.1 √âtape 1 : Logiciels et √©tapes\n",
    "\n",
    "#### 4.6.1.1 Configuration des logiciels\n",
    "\n",
    "S√©lectionnez les packages dont nous aurons besoin comme dans la capture d'√©cran :\n",
    "1. Nous s√©lectionnons la derni√®re version d'`EMR`, soit la version `6.3.0` au moment o√π je r√©dige ce document\n",
    "2. Nous cochons bien √©videment `Hadoop` et `Spark` qui seront pr√©install√©s dans leur version la plus r√©cente\n",
    "3. Nous aurons √©galement besoin de `TensorFlow` pour importer notre mod√®le et r√©aliser le *transfert learning*\n",
    "4. Nous travaillerons enfin avec un *notebook `Jupyter`* via l'application `JupyterHub`\n",
    "  * Comme nous le verrons dans un instant nous allons <u>param√©trer l'application afin que les notebooks</u>,\n",
    "  * comme le reste de nos donn√©es de travail, <u>soient enregistr√©s directement sur S3</u>.\n",
    "  \n",
    "![Cr√©er un cluster](../baseline/img/EMR_configuration_logiciels.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1.2 Modifier les param√®tres du logiciel\n",
    "\n",
    "Param√©trez la persistance des notebooks cr√©√©s et ouvert via JupyterHub :\n",
    "\n",
    "On peut √† cette √©tape effectuer des demandes de param√©trage particuli√®res sur nos applications.\n",
    "\n",
    "L'objectif est, comme pour le reste de nos donn√©es de travail, d'√©viter toutes les probl√©matiques √©voqu√©es pr√©c√©demment.\n",
    "\n",
    "C'est l'objectif √† cette √©tape, <u>nous allons enregistrer et ouvrir les notebooks</u> non pas sur l'espace disque de  l'instance EC2 (comme ce serait le cas dans la configuration par d√©faut de JupyterHub) mais <u>directement sur **Amazon S3**</u>.\n",
    "\n",
    "Deux solutions sont possibles pour r√©aliser cela :\n",
    "1. Cr√©er un **fichier de configuration JSON** que l'on **upload sur S3** et on indique ensuite le chemin d‚Äôacc√®s au fichier JSON\n",
    "2. Rentrez directement la configuration au format JSON\n",
    " \n",
    "J'ai personnellement cr√©√© un fichier JSON lors de la cr√©ation de ma premi√®re instance EMR, puis lorsqu'on d√©cide de cloner notre serveur pour en recr√©er un facilement √† l'identique, la configuration du fichier JSON se retrouve directement copi√© comme dans la capture ci-dessous.\n",
    "\n",
    "<u>Voici le contenu de mon fichier JSON</u> :\n",
    "\n",
    "```sh\n",
    "[\n",
    "    {\n",
    "        \"classification\": \"jupyter-s3-conf\",\n",
    "        \"properties\": {\n",
    "            \"s3.persistence.bucket\": \"pepper-labs-fruits\",\n",
    "            \"s3.persistence.enabled\": \"true\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "Appuyez ensuite sur \"**Suivant**\"\n",
    "\n",
    "![Modifier les param√®tres du logiciel](../baseline/img/EMR_parametres_logiciel.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.2 √âtape 2 : Mat√©riel\n",
    "\n",
    "A cette √©tape, laissez les choix par d√©faut.\n",
    "\n",
    "L'important ici est la s√©lection des instances :\n",
    "\n",
    "1. Nous choisissons des instances de type `M5` qui sont des **instances de type √©quilibr√©s**\n",
    "2. Nous choisissons le type `xlarge` qui est l'instance la **moins on√©reuse disponible** \n",
    "3. Nous s√©lectionnons 1 instance **Ma√Ætre** (le *pilote*) et 2 instances **Principales** (les *travailleurs*) soit 3 instances EC2.\n",
    "\n",
    "R√©f√©rences :\n",
    "* [Instances M5 Amazon EC2](https://aws.amazon.com/fr/ec2/instance-types/m5/)\n",
    "\n",
    "![Choix du materiel](../baseline/img/EMR_materiel.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.3 √âtape 3 : Param√®tres de cluster g√©n√©raux\n",
    "\n",
    "#### 4.6.3.1 Options g√©n√©rales\n",
    "\n",
    "La premi√®re chose √† faire est de donner un nom au cluster.\n",
    "\n",
    "Pour des raisons pratiques, j'ai √©galement d√©coch√© `Protection de la r√©siliation`.\n",
    "    \n",
    "![Nom du Cluster](../baseline/img/EMR_nom_cluster.png)\n",
    "\n",
    "#### 4.6.3.2 Actions d'amor√ßage\n",
    "\n",
    "Nous allons √† cette √©tape **choisir les packages manquants √† installer** qui sont indispensables pour ex√©cuter notre notebook.\n",
    "\n",
    "L'avantage de r√©aliser cette √©tape maintenant est que les packages install√©s le seront sur l'ensemble des machines du cluster.\n",
    "\n",
    "La proc√©dure pour cr√©er le fichier **bootstrap** qui contient l'ensemble des instructions permettant d'installer tous les packages dont nous aurons besoin est expliqu√© dans le cours [R√©alisez des calculs distribu√©s sur des donn√©es massives / Bootstrapping](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues#/id/r-4356490)\n",
    "\n",
    "Nous cr√©ons donc un fichier nomm√© `bootstrap-emr.sh` que nous t√©l√©versons sur S3 (√† la racine du compartiment `pepper-bucket`) et nous l'ajoutons comme indiqu√© dans la capture d'√©cran ci-dessous:\n",
    "\n",
    "```sh\n",
    "$ aws s3 cp .\\bootstrap-emr.sh s3://pepper-bucket\n",
    "upload: .\\bootstrap-emr.sh to s3://pepper-bucket/bootstrap-emr.sh\n",
    "```\n",
    "\n",
    "![Actions d‚Äôamor√ßage](../baseline/img/EMR_amorcage.png)\n",
    "\n",
    "Voici le contenu du fichier `bootstrap-emr.sh`\n",
    "\n",
    "Il s'agit d'une s√©quence de commandes `pip install` pour installer les biblioth√®ques manquantes comme r√©alis√© en local.\n",
    "\n",
    "Il est n√©cessaire de r√©aliser ces actions √† cette √©tape pour que les packages soient install√©s sur l'ensemble des machines du cluster et non pas uniquement sur le driver, comme cela serait le cas si nous ex√©cutions ces commandes directement dans le notebook JupyterHub ou dans la console EMR (connect√© au driver).\n",
    "\n",
    "Ces actions d'amor√ßage sont ex√©cut√©es avant l'installation des applications et avant qu'Amazon EMR ne commence √† traiter les donn√©es. En cas d'ajout de nouveaux n≈ìuds √† un cluster en cours d'ex√©cution, ces actions d'amor√ßage seront √©galement ex√©cut√©es sur ces nouveaux n≈ìuds.\n",
    "\n",
    "En revanche, l'action d'amor√ßage n'est effectu√©e qu'une fois sur chaque n≈ìud. Pour modifier la configuration d'amor√ßage, il faut donc r√©silier l'instance EMR et lancer un nouveau cluster. \n",
    "\n",
    "```sh\n",
    "#!bin/bash\n",
    "sudo python3 -m pip install -U setuptools\n",
    "sudo python3 -m pip install -U pip\n",
    "sudo python3 -m pip install wheel\n",
    "sudo python3 -m pip install pillow\n",
    "sudo python3 -m pip install pandas\n",
    "sudo python3 -m pip install pyarrow\n",
    "sudo python3 -m pip install boto3\n",
    "sudo python3 -m pip install s3fs\n",
    "sudo python3 -m pip install fsspec\n",
    "```\n",
    "\n",
    "\n",
    "* **`setuptools`** et **`pip`** doivent √™tre mis √† jour pour √©viter un probl√®me avec l'installation de **`pyarrow`**.\n",
    "* **`Pandas`** a eu droit √† une mise √† jour majeure (`1.3.0`) il y a moins d'une semaine au moment de la r√©daction de ce notebook, et cette nouvelle version de **`Pandas`** d√©pend d'une version plus r√©cente de **`Numpy`** que la version install√©e par d√©faut (`1.16.5`) lors de l'initialisation des instances **EC2**. <u>Il ne semble pas possible d'imposer une autre version de Numpy que celle install√© par d√©faut</u> m√™me si on force l'installation d'une version r√©cente de **Numpy** (en tout cas, ni simplement ni intuitivement).\n",
    "\n",
    "La mise √† jour √©tant tr√®s r√©cente <u>la version de **Numpy** n'est pas encore mise √† jour sur **EC2**</u> mais on peut imaginer que ce sera le cas tr√®s rapidement et il ne sera plus n√©cessaire d'imposer une version sp√©cifique de **Pandas**.\n",
    "\n",
    "En attendant, je demande <u>l'installation de l'avant derni√®re version de **Pandas (1.2.5)**</u>\n",
    "\n",
    "On clique ensuite sur `Suivant`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.4 √âtape 4 : S√©curit√©\n",
    "\n",
    "#### 4.6.4.1 Options de s√©curit√©\n",
    "\n",
    "A cette √©tape nous s√©lectionnons la **paire de cl√©s EC2** cr√©√©e pr√©c√©demment.\n",
    "\n",
    "Elle nous permettra de nous connecter en `ssh` √† nos **instances EC2** sans avoir √† saisir nos login et mot de passe.\n",
    "\n",
    "Laissons les autres param√®tres √† leur valeur par d√©faut.\n",
    "\n",
    "Cliquons enfin sur `Cr√©er un cluster`.\n",
    " \n",
    "![EMR S√©curit√©](../baseline/img/EMR_securite.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Instanciation du serveur\n",
    "\n",
    "Il ne nous reste plus qu'√† attendre que le serveur soit pr√™t.\n",
    "\n",
    "Cette √©tape peut prendre entre **15 et 20 minutes**.\n",
    "\n",
    "Plusieurs √©tapes s'encha√Ænent et on peut suivre l'√©volution du statut du **cluster EMR** :\n",
    "\n",
    "![Instanciation √©tape 1](../baseline/img/EMR_instanciation_01.png)\n",
    "![Instanciation √©tape 2](../baseline/img/EMR_instanciation_02.png)\n",
    "![Instanciation √©tape 3](../baseline/img/EMR_instanciation_03.png)\n",
    "\n",
    "Lorsque le statut affiche en vert `\"En attente\"` cela signifie que l'instanciation s'est bien d√©roul√©e et que notre serveur est pr√™t √† √™tre utilis√©. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Cr√©ation du tunnel SSH √† l'instance EC2 (Ma√Ætre)\n",
    "\n",
    "### 4.8.1 Cr√©ation des autorisations sur les connexions entrantes\n",
    "\n",
    "Nous souhaitons maintenant pouvoir acc√©der aux applications :\n",
    "* **JupyterHub**, pour l'ex√©cution de notre notebook ;\n",
    "* **Serveur d'historique Spark**, pour le suivi de l'ex√©cution des t√¢ches de notre script lorsqu'il sera lanc√©.\n",
    " \n",
    "Ces applications ne sont accessibles que depuis le r√©seau local du pilote.\n",
    "\n",
    "Pour y acc√©der nous devons donc **cr√©er un tunnel SSH vers le pilote**.\n",
    "\n",
    "Par d√©faut, ce driver se situe derri√®re un pare-feu qui bloque l'acc√®s SSH.\n",
    "\n",
    "Pour ouvrir le port `22` sur lequel √©coute le serveur SSH, il faut modifier le **groupe de s√©curit√© EC2 du driver**.\n",
    "\n",
    "Cette √©tape est d√©crite dans le cours [R√©alisez des calculs distribu√©s sur des donn√©es massives / Lancement d'une application √† partir du driver](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues#/id/r-4356512): \n",
    "\n",
    "Sur la page de la console consacr√©e √† EC2, dans l'onglet \"R√©seau et s√©curit√©\", cliquez sur \"Groupes de s√©curit√©\".\n",
    "Vous allez devoir modifier le groupe de s√©curit√© d‚ÄôElasticMapReduce-Master. \n",
    "\n",
    "Dans l'onglet \"Entrant\", ajoutez une r√®gle SSH dont la source est \"N'importe o√π\" (ou \"Mon IP\" si vous disposez d'une adresse IP fixe).\n",
    "\n",
    "![Configuration autorisation ports entrants pour ssh](../baseline/img/EMR_config_ssh_01.png)\n",
    "\n",
    "Une fois cette √©tape r√©alis√©e vous devriez avoir une configuration semblable √† la mienne :\n",
    "\n",
    "![Configuration ssh termin√©e](../baseline/img/EMR_config_ssh_02.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.2 Cr√©ation du tunnel ssh vers le Driver\n",
    "\n",
    "On peut maintenant √©tablir le **tunnel SSH** vers le **Pilote**.\n",
    "\n",
    "Pour cela on r√©cup√®re les informations de connexion fournis par Amazon depuis la page du service EMR / Cluster / onglet R√©capitulatif en cliquant sur \"**Activer la connexion Web**\".\n",
    "\n",
    "![Activer la connexion Web](../baseline/img/EMR_tunnel_ssh_01.png)\n",
    "\n",
    "<u>On r√©cup√®re ensuite la commande fournie par Amazon pour **√©tablir le tunnel SSH**</u> :\n",
    "\n",
    "![R√©cup√©rer la commande pour √©tablir le tunnel ssh](../baseline/img/EMR_tunnel_ssh_02.png)\n",
    "\n",
    "<u>Dans mon cas, la commande ne fonctionne pas telle</u> quelle et j'ai du **l'adapter √† ma configuration**. <br />\n",
    "\n",
    "La **cl√© ssh** se situe dans un dossier `.ssh` elle-m√™me situ√©e dans mon **r√©pertoire personnel** dont le symbole est, sous Linux, identifi√© par un tilde `~`.\n",
    "\n",
    "Ayant suivi le cours [R√©alisez des calculs distribu√©s sur des donn√©es massives / Lancement d'une application √† partir du driver](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives)\n",
    "\n",
    "* j'ai choisi d'utiliser le port `5555` au lieu du `8157`, m√™me si le choix n'est pas tr√®s important.\n",
    "* j'ai √©galement rencontr√© un <u>probl√®me de compatibilit√©</u> avec l'argument `-N` (la liste des arguments et leur significations disponibles [ici](https://explainshell.com/explain?cmd=ssh+-L+-N+-f+-l+-D)) et j'ai d√©cid√© de simplement le supprimer.\n",
    "\n",
    "<u>Finalement, j'utilise la commande suivante dans un terminal pour √©tablir mon tunnel SSH (seule l'URL change d'une instance √† une autre)</u> :\n",
    "\n",
    "```sh\n",
    "$ ssh -i ~/.ssh/p8-ec2.pem -D 5555 hadoop@ec2-35-180-91-39.eu-west-3.compute.amazonaws.com\n",
    "```\n",
    "\n",
    "<u>On inscrit `yes` pour valider la connexion et si la connexion est √©tablie on obtient le r√©sultat suivant</u> :\n",
    "\n",
    "![Cr√©ation du tunnel SSH](../baseline/img/EMR_connexion_ssh_01.png)\n",
    "\n",
    "Nous avons **correctement √©tabli le tunnel ssh avec le driver** sur le port `5555`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.3 Configuration de FoxyProxy\n",
    "\n",
    "Une derni√®re √©tape est n√©cessaire pour acc√©der √† nos applications, en demandant √† notre navigateur d'emprunter le tunnel ssh.\n",
    "\n",
    "J'utilise pour cela **FoxyProxy**.\n",
    "\n",
    "[Une fois encore, vous pouvez utiliser le cours pour le configurer](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308701-realisez-la-maintenance-dun-cluster#/id/r-4356554).\n",
    "\n",
    "Sinon, ouvrez la configuration de **FoxyProxy** et <u>cliquez sur **Ajouter**</u> en haut √† gauche puis renseigner les √©l√©ments comme dans la capture ci-dessous :\n",
    "\n",
    "![Configuration FoxyProxy Etape 1](../baseline/img/EMR_foxyproxy_config_01.png)\n",
    "\n",
    "<u>On obtient le r√©sultat ci-dessous</u> :\n",
    "\n",
    "![Configuration FoxyProxy Etape 2](../baseline/img/EMR_foxyproxy_config_02.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.4 Acc√®s aux applications du serveur EMR via le tunnel ssh\n",
    "\n",
    "<u>Avant d'√©tablir notre **tunnel ssh** nous avions √ßa</u> :\n",
    "\n",
    "![avant tunnel ssh](../baseline/img/EMR_tunnel_ssh_avant.png)\n",
    "\n",
    "<u>On active le **tunnel ssh** comme vu pr√©c√©demment puis on demande √† notre navigateur de l'utiliser avec **FoxyProxy**</u> :\n",
    "\n",
    "![FoxyProxy activation](../baseline/img/EMR_foxyproxy_activation.png)\n",
    "\n",
    "<u>On peut maintenant s'apercevoir que plusieurs applications nous sont accessibles</u> :\n",
    "\n",
    "![avant tunnel ssh](../baseline/img/EMR_tunnel_ssh_apres.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Connexion au notebook JupyterHub\n",
    "\n",
    "Pour se connecter √† **JupyterHub** en vue d'ex√©cuter notre **notebook**, il faut commencer par <u>cliquer sur l'application **JupyterHub**</u> apparue depuis que nous avons configur√© le **tunnel ssh** et **foxyproxy** sur notre navigateur (actualisez la page si ce n‚Äôest pas le cas).\n",
    "\n",
    "![D√©marrage de JupyterHub](../baseline/img/EMR_jupyterhub_connexion_01.png)\n",
    "\n",
    "On passe les √©ventuels avertissements de s√©curit√© puis nous arrivons sur une page de connexion.\n",
    "\n",
    "<u>On se connecte avec les informations par d√©faut</u> :\n",
    " - <u>login</u>: **jovyan**\n",
    " - <u>password</u>: **jupyter**\n",
    " \n",
    "![Connexion √† JupyterHub](../baseline/img/EMR_jupyterhub_connexion_02.png)\n",
    "\n",
    "Nous arrivons ensuite dans un dossier vierge de notebook.\n",
    "\n",
    "Il suffit d'en cr√©er un en cliquant sur \"**New**\" en haut √† droite.\n",
    "\n",
    "![Liste et cr√©ation des notebook](../baseline/img/EMR_jupyterhub_creer_notebooks.png)\n",
    "\n",
    "Il est √©galement possible d'en <u>uploader un directement dans notre **bucket S3**</u>.\n",
    "\n",
    "Grace √† la <u>**persistance** param√©tr√©e √† l'instanciation du cluster nous sommes actuellement dans l'arborescence de notre **bucket S3**</u>\n",
    "\n",
    "![Notebook stock√©s sur S3](../baseline/img/EMR_jupyterhub_S3.png)\n",
    "\n",
    "Je d√©cide d'**importer un notebook d√©j√† r√©dig√© en local directement sur S3** et je l'ouvre depuis **l'interface JupyterHub**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 Ex√©cution du code\n",
    "\n",
    "Je d√©cide d'ex√©cuter cette partie du code depuis **JupyterHub h√©berg√© sur notre cluster EMR**.\n",
    "\n",
    "Pour ne pas alourdir inutilement les explications du **notebook**, je ne r√©expliquerai pas les √©tapes communes que nous avons d√©j√† vues dans la premi√®re partie o√π l'on a ex√©cut√© le code localement sur notre machine virtuelle Ubuntu.\n",
    "\n",
    "<u>Avant de commencer</u>, il faut s'assurer d'utiliser le **kernel pyspark**.\n",
    "\n",
    "**En utilisant ce kernel, une session spark est cr√©√©e √† l'ex√©cution de la premi√®re cellule**.\n",
    "\n",
    "Il n'est donc **plus n√©cessaire d'ex√©cuter le code `spark = (SparkSession ...`** comme lors de l'ex√©cution de notre notebook en local sur notre VM Ubuntu."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10.1 D√©marrage de la session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1626050279029_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-10-195.eu-west-3.compute.internal:20888/proxy/application_1626050279029_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-2-58.eu-west-3.compute.internal:8042/node/containerlogs/container_1626050279029_0001_01_000001/livy\">Link</a></td><td>‚úî</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# L'ex√©cution de cette cellule d√©marre l'application Spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Affichage des informations sur la session en cours et liens vers Spark UI</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1000M', 'executorCores': 2, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1626050279029_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-10-195.eu-west-3.compute.internal:20888/proxy/application_1626050279029_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-2-58.eu-west-3.compute.internal:8042/node/containerlogs/container_1626050279029_0001_01_000001/livy\">Link</a></td><td>‚úî</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10.2 Installation des packages\n",
    "\n",
    "Les packages n√©cessaires ont √©t√© install√© via l'√©tape de **bootstrap** √† l'instanciation du serveur.\n",
    "\n",
    "### 4.10.3 Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10.4 D√©finition des PATH pour charger les images et enregistrer les r√©sultats\n",
    "\n",
    "Nous acc√©dons directement √† nos **donn√©es sur S3** comme si elles √©taient **stock√©es localement**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version d'origine de l'alternant\n",
    "\n",
    "```python\n",
    ">>> PATH = 's3://p8-data'\n",
    ">>> PATH_Data = PATH+'/Test'\n",
    ">>> PATH_Result = PATH+'/Results'\n",
    ">>> print('PATH:        '+\\\n",
    ">>>       PATH+'\\nPATH_Data:   '+\\\n",
    ">>>       PATH_Data+'\\nPATH_Result: '+PATH_Result)\n",
    "PATH:        s3://p8-data\n",
    "PATH_Data:   s3://p8-data/Test\n",
    "PATH_Result: s3://p8-data/Results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket_path: s3://p8-data\n",
      " input_path: s3://p8-data/Test\n",
      "output_path: s3://p8-data/Results\n"
     ]
    }
   ],
   "source": [
    "bucket_path = \"s3://p8-data\"\n",
    "input_path = f\"{bucket_path}/Test\"\n",
    "output_path = f\"{bucket_path}/Results\"\n",
    "print(\"bucket_path:\", bucket_path)\n",
    "print(\" input_path:\", input_path)\n",
    "print(\"output_path:\", output_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10.5 Traitement des donn√©es"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.10.5.1 Chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+\n",
      "|                path|   modificationTime|length|             content|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "|s3://p8-data/Test...|2021-07-03 09:00:08|  7353|[FF D8 FF E0 00 1...|\n",
      "|s3://p8-data/Test...|2021-07-03 09:00:08|  7350|[FF D8 FF E0 00 1...|\n",
      "|s3://p8-data/Test...|2021-07-03 09:00:08|  7349|[FF D8 FF E0 00 1...|\n",
      "|s3://p8-data/Test...|2021-07-03 09:00:08|  7348|[FF D8 FF E0 00 1...|\n",
      "|s3://p8-data/Test...|2021-07-03 09:00:09|  7328|[FF D8 FF E0 00 1...|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "images.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Je ne conserve que le **path** de l'image et j'ajoute <br />\n",
    "    une colonne contenant les **labels** de chaque image</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n",
      "None\n",
      "+------------------------------------------+----------+\n",
      "|path                                      |label     |\n",
      "+------------------------------------------+----------+\n",
      "|s3://p8-data/Test/Watermelon/r_106_100.jpg|Watermelon|\n",
      "|s3://p8-data/Test/Watermelon/r_109_100.jpg|Watermelon|\n",
      "|s3://p8-data/Test/Watermelon/r_108_100.jpg|Watermelon|\n",
      "|s3://p8-data/Test/Watermelon/r_107_100.jpg|Watermelon|\n",
      "|s3://p8-data/Test/Watermelon/r_95_100.jpg |Watermelon|\n",
      "+------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None"
     ]
    }
   ],
   "source": [
    "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n",
    "print(images.printSchema())\n",
    "print(images.select('path','label').show(5,False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.10.5.2 Pr√©paration du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
      "\r\n",
      "    8192/14536120 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      " 4202496/14536120 [=======>......................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      "14540800/14536120 [==============================] - 0s 0us/step"
     ]
    }
   ],
   "source": [
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "broadcast_weights = sc.broadcast(new_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 112, 112, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 112, 112, 32) 128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, 112, 112, 32) 0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 112, 112, 32) 288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 112, 112, 32) 128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, 112, 112, 32) 0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 112, 112, 16) 512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 112, 112, 16) 64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 112, 112, 96) 1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 112, 112, 96) 384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, 112, 112, 96) 0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_pad (ZeroPadding2D)     (None, 113, 113, 96) 0           block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 56, 56, 96)   864         block_1_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 56, 56, 96)   384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, 56, 56, 96)   0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 56, 56, 24)   2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 56, 56, 24)   96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 56, 56, 144)  3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 56, 56, 144)  1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 56, 56, 144)  576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, 56, 56, 144)  0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 56, 56, 24)   3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 56, 56, 24)   96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 56, 56, 24)   0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 56, 56, 144)  3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_pad (ZeroPadding2D)     (None, 57, 57, 144)  0           block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 28, 28, 144)  1296        block_3_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 28, 28, 144)  576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, 28, 28, 144)  0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 28, 28, 32)   4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 28, 28, 32)   128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 28, 28, 192)  6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 28, 28, 32)   6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 28, 28, 32)   128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 28, 28, 32)   0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 28, 28, 192)  6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 28, 28, 32)   6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 28, 28, 32)   128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 28, 28, 32)   0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, 28, 28, 192)  6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_pad (ZeroPadding2D)     (None, 29, 29, 192)  0           block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, 14, 14, 192)  1728        block_6_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, 14, 14, 192)  768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, 14, 14, 192)  0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, 14, 14, 64)   12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, 14, 14, 64)   256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, 14, 14, 384)  24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, 14, 14, 64)   24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, 14, 14, 64)   256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, 14, 14, 64)   0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, 14, 14, 384)  24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, 14, 14, 64)   24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, 14, 14, 64)   256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, 14, 14, 64)   0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, 14, 14, 384)  24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, 14, 14, 64)   24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, 14, 14, 64)   256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, 14, 14, 64)   0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, 14, 14, 384)  24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, 14, 14, 384)  1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, 14, 14, 384)  0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, 14, 14, 384)  3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, 14, 14, 384)  1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, 14, 14, 96)   36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, 14, 14, 96)   384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, 14, 14, 576)  55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, 14, 14, 96)   55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, 14, 14, 96)   384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, 14, 14, 96)   0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, 14, 14, 576)  55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, 14, 14, 96)   55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, 14, 14, 96)   384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, 14, 14, 96)   0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, 14, 14, 576)  55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_pad (ZeroPadding2D)    (None, 15, 15, 576)  0           block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, 7, 7, 576)    5184        block_13_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, 7, 7, 576)    2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)    0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, 7, 7, 160)    92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, 7, 7, 160)    640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, 7, 7, 960)    153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, 7, 7, 160)    153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, 7, 7, 160)    640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, 7, 7, 160)    0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, 7, 7, 960)    153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, 7, 7, 160)    153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, 7, 7, 160)    640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, 7, 7, 160)    0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, 7, 7, 960)    153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, 7, 7, 320)    307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, 7, 7, 320)    1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 7, 7, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1280)         0           out_relu[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 2,223,872\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(broadcast_weights.value)\n",
    "    return new_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.10.5.3 D√©finition du processus de chargement des images <br/> et application de leur featurisation √† travers l'utilisation de pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/functions.py:392: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details."
     ]
    }
   ],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.10.5.4 Ex√©cutions des actions d'extractions de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_df = images.repartition(24).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\")\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://p8-data/Results"
     ]
    }
   ],
   "source": [
    "print(PATH_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_df.write.mode(\"overwrite\").parquet(PATH_Result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10.6 Chargement des donn√©es enregistr√©es et validation du r√©sultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_parquet(PATH_Result, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           path  ...                                           features\n",
      "0    s3://p8-data/Test/Watermelon/r_174_100.jpg  ...  [0.0059991637, 0.44703647, 0.0, 0.0, 3.3713572...\n",
      "1  s3://p8-data/Test/Pineapple Mini/128_100.jpg  ...  [0.0146466885, 4.080593, 0.055877004, 0.0, 0.0...\n",
      "2  s3://p8-data/Test/Pineapple Mini/137_100.jpg  ...  [0.0, 4.9659867, 0.0, 0.0, 0.0, 0.0, 0.5144821...\n",
      "3      s3://p8-data/Test/Watermelon/275_100.jpg  ...  [0.22511952, 0.07235509, 0.0, 0.0, 1.690149, 0...\n",
      "4      s3://p8-data/Test/Watermelon/271_100.jpg  ...  [0.3286234, 0.18830013, 0.0, 0.0, 1.9123534, 0...\n",
      "\n",
      "[5 rows x 3 columns]"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280,)"
     ]
    }
   ],
   "source": [
    "df.loc[0,'features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22688, 3)"
     ]
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut √©galement constater la pr√©sence des fichiers au format \"**parquet**\" sur le **serveur S3** :\n",
    "\n",
    "![Affichage des r√©sultats sur S3](../baseline/img/S3_Results.png)\n",
    "\n",
    "## 4.11 Suivi de l'avancement des t√¢ches avec le Serveur d'Historique Spark\n",
    "\n",
    "Il est possible de voir l'avancement des t√¢ches en cours avec le **serveur d'historique Spark**.\n",
    "\n",
    "![Acc√®s au serveur d'historique spark](../baseline/img/EMR_serveur_historique_spark_acces.png)\n",
    "\n",
    "**Il est √©galement possible de revenir et d'√©tudier les t√¢ches qui ont √©t√© r√©alis√©, afin de debugger, optimiser les futurs t√¢ches √† r√©aliser.**\n",
    "\n",
    "Lorsque la commande `features_df.write.mode(\"overwrite\").parquet(PATH_Result)` √©tait en cours, nous pouvions observer son √©tat d'avancement :\n",
    "\n",
    "![Progression execution script](../baseline/img/EMR_jupyterhub_avancement.png)\n",
    "\n",
    "Le **serveur d'historique Spark** nous permet une vision beaucoup plus pr√©cise de l'ex√©cution des diff√©rentes t√¢che sur les diff√©rentes machines du cluster :\n",
    "\n",
    "![Suivi des t√¢ches spark](../baseline/img/EMR_SHSpark_01.png)\n",
    "\n",
    "On peut √©galement constater que notre cluster de calcul a mis un tout petit peu **moins de 8 minutes** pour traiter les **22 688 images**.\n",
    "\n",
    "![Temps de traitement](../baseline/img/EMR_SHSpark_02.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.12 R√©siliation de l'instance EMR\n",
    "\n",
    "Notre travail est maintenant termin√©.\n",
    "\n",
    "Le cluster de machines EMR est **factur√© √† la demande**, et nous continuons d'√™tre factur√© m√™me lorsque les machines sont au repos.\n",
    "\n",
    "Pour **optimiser la facturation**, il nous faut maintenant **r√©silier le cluster**.\n",
    "\n",
    "Je r√©alise cette commande depuis l'interface AWS :\n",
    "\n",
    "1. Commencez par **d√©sactiver le tunnel ssh dans FoxyProxy** pour √©viter des probl√®mes de **timeout**.\n",
    "![D√©sactivation de FoxyProxy](../baseline/img/EMR_foxyproxy_desactivation.png)\n",
    "2. Cliquez sur \"**R√©silier**\"\n",
    "![Cliquez sur R√©silier](../baseline/img/EMR_resiliation_01.png)\n",
    "3. Confirmez la r√©siliation\n",
    "![Confirmez la r√©siliation](../baseline/img/EMR_resiliation_02.png)\n",
    "4. La r√©siliation prend environ **1 minute**\n",
    "![R√©siliation en cours](../baseline/img/EMR_resiliation_03.png)\n",
    "5. La r√©siliation est effectu√©e\n",
    "![R√©siliation termin√©e](../baseline/img/EMR_resiliation_04.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.13 Cloner le serveur EMR (si besoin)\n",
    "\n",
    "Si nous devons de nouveau ex√©cuter notre notebook dans les m√™mes conditions, il nous suffit de **cloner notre cluster** et ainsi en obtenir une copie fonctionnelle sous 15/20 minutes, le temps de son instanciation.\n",
    "\n",
    "<u>Pour cela deux solutions</u> :\n",
    "1. <u>Depuis l'interface AWS</u> :\n",
    "    1. Cliquez sur \"**Cloner**\"\n",
    "    ![Cloner un cluster](../baseline/img/EMR_cloner_01.png)\n",
    "    2. Dans notre cas nous ne souhaitons pas inclure d'√©tapes\n",
    "    ![Ne pas inclure d'√©tapes](../baseline/img/EMR_cloner_02.png)\n",
    "    3. La configuration du cluster est recr√©√©e √† l‚Äôidentique.\n",
    "        * On peut revenir sur les diff√©rentes √©tapes si on souhaite apporter des modifications\n",
    "        * Quand tout est pr√™t, cliquez sur \"**Cr√©er un cluster**\"\n",
    "    ![V√©rification/Modification/Cr√©er un cluster](../baseline/img/EMR_cloner_03.png)\n",
    "2. <u>En ligne de commande</u> (avec AWS CLI d'install√© et de configur√© et en s'assurant de s'attribuer les droits n√©cessaires sur le compte AMI utilis√©)\n",
    "    1. Cliquez sur \"**Exporter AWS CLI**\"\n",
    "    ![Exporter AWS CLI](../baseline/img/EMR_cloner_cli_01.png)\n",
    "    2. Copier/Coller la commande **depuis un terminal**\n",
    "    ![Copier Coller Commande](../baseline/img/EMR_cloner_cli_02.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.14 Arborescence du serveur S3 √† la fin du projet\n",
    "\n",
    "Pour information, voici **l'arborescence compl√®te de mon bucket S3 p8-data** √† la fin du projet :\n",
    "\n",
    "*Par soucis de lisibilit√©, nous ne listons pas les 131 sous dossiers du r√©pertoire \"Test\"*\n",
    "\n",
    "```sh\n",
    "1. Results/_SUCCESS\n",
    "1. Results/part-00000-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00001-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00002-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00003-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00004-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00005-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00006-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00007-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00008-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00009-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00010-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00011-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00012-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00013-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00014-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00015-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00016-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00017-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00018-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00019-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00020-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00021-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00022-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00023-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Test/\n",
    "1. bootstrap-emr.sh\n",
    "1. jupyter-s3-conf.json\n",
    "1. jupyter/jovyan/.s3keep\n",
    "1. jupyter/jovyan/P8_01_Notebook.ipynb\n",
    "1. jupyter/jovyan/_metadata\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/file-perm.sqlite\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/html/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/latex/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbsignatures.db\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/notebook_secret\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/Untitled-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/Untitled1-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/test3-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/Untitled.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/Untitled1.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/test3.ipynb\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5\\. Conclusion\n",
    "\n",
    "Nous avons r√©alis√© ce projet **en deux temps** en tenant compte des contraintes qui nous ont √©t√© impos√©es.\n",
    "\n",
    "Nous avons **dans un premier temps d√©velopp√© notre solution en local** sur une machine virtuelle dans un environnement Linux Ubuntu.\n",
    "\n",
    "La <u>premi√®re phase</u> a consist√© √† **installer l'environnement de travail Spark**.\n",
    "\n",
    "**Spark** a un param√®tre qui nous permet de travailler en local et nous permet ainsi de **simuler du calcul partag√©** en consid√©rant **chaque c≈ìur d'un processeur comme un worker ind√©pendant**.\n",
    "\n",
    "Nous avons travaill√© sur un plus **petit jeu de donn√©e**, l'id√©e √©tait simplement de **valider le bon fonctionnement de la solution**.\n",
    "\n",
    "Nous avons fait le choix de r√©aliser du **transfert learning** √† partir du model **MobileNetV2**.\n",
    "\n",
    "Ce mod√®le a √©t√© retenu pour sa **l√©g√®ret√©** et sa **rapidit√© d'ex√©cution** ainsi que pour la **faible dimension de son vecteur en sortie**.\n",
    "\n",
    "Les r√©sultats ont √©t√© enregistr√©s sur disque en plusieurs partitions au format \"**parquet**\".\n",
    "\n",
    "**La solution a parfaitement fonctionn√© en mode local**.\n",
    "\n",
    "La <u>deuxi√®me phase</u> a consist√© √† cr√©er un **r√©el cluster de calculs**.\n",
    "\n",
    "L'objectif √©tait de pouvoir **anticiper une future augmentation de la charge de travail**.\n",
    "\n",
    "Le meilleur choix retenu a √©t√© l'utilisation du prestataire de services **Amazon Web Services** qui nous permet de **louer √† la demande de la puissance de calculs**, pour un **co√ªt tout √† fait acceptable**.\n",
    "\n",
    "Ce service se nomme **EC2** et se classe parmi les offres **Infrastructure as a Service** (IaaS).\n",
    "\n",
    "Nous sommes allez plus loin en utilisant un service de plus haut niveau (**Plateforme as a Service** PaaS) en utilisant le service **`EMR`** qui nous permet d'un seul coup d'**instancier plusieurs serveurs (un cluster)** sur lesquels nous avons pu demander l'installation et la configuration de plusieurs programmes et librairies n√©cessaires √† notre projet comme **`Spark`**, **`Hadoop`**, **`JupyterHub`** ainsi que la librairie **`TensorFlow`**.\n",
    "\n",
    "En plus d'√™tre plus **rapide et efficace √† mettre en place**, nous avons la **certitude du bon fonctionnement de la solution**, celle-ci ayant √©t√© pr√©alablement valid√© par les ing√©nieurs d'Amazon.\n",
    "\n",
    "Nous avons √©galement pu installer, sans difficult√©, **les packages n√©cessaires sur l'ensembles des machines du cluster**.\n",
    "\n",
    "Enfin, avec tr√®s peu de modification, et plus simplement encore, nous avons pu **ex√©cuter notre notebook comme nous l'avions fait localement**.\n",
    "\n",
    "Nous avons cette fois-ci ex√©cut√© le traitement sur **l'ensemble des images de notre dossier \"Test\"**.\n",
    "\n",
    "Nous avons opt√© pour le service **Amazon S3** pour **stocker les donn√©es de notre projet**.\n",
    "\n",
    "S3 offre, pour un faible co√ªt, toutes les conditions dont nous avons besoin pour stocker et exploiter de mani√®re efficace nos donn√©es.\n",
    "\n",
    "L'espace allou√© est potentiellement **illimit√©**, mais les co√ªts seront fonction de l'espace utilis√©.\n",
    "\n",
    "Il nous sera **facile de faire face √† une mont√©e de la charge de travail** en **redimensionnant** simplement notre cluster de machines (horizontalement et/ou verticalement au besoin), les co√ªts augmenteront en cons√©quence mais resteront nettement inf√©rieurs aux co√ªts engendr√©s par l'achat de mat√©riels ou par la location de serveurs d√©di√©s."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
