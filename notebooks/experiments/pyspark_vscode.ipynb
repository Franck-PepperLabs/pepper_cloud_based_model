{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PySpark VS Code**\n",
    "\n",
    "PySpark in VS Code.\n",
    "* [PySpark Development Made Simple. Using VS Code, Jupyter Notebooks, and… | by Jason Clarke | Better Programming](https://betterprogramming.pub/pyspark-development-made-simple-9449a893ab17)\n",
    "* [PySpark.SQL and Jupyter Notebooks on Visual Studio Code (Python kernel)](https://blog.openthreatresearch.com/spark_jupyter_notebook_vscode)\n",
    "* [Run jobs: Spark & Hive Tools for VS Code - SQL Server Big Data Clusters | Microsoft Learn](https://learn.microsoft.com/en-us/sql/big-data-cluster/spark-hive-tools-vscode?view=sql-server-ver15)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PySpark Development Made Simple. Using VS Code, Jupyter Notebooks, and… | by Jason Clarke | Better Programming](https://betterprogramming.pub/pyspark-development-made-simple-9449a893ab17)\n",
    "\n",
    "Using VS Code, Jupyter Notebooks, and Docker\n",
    "\n",
    "By **Jason Clarke**, Better Programming, Sep 30, 2022\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*6DROvAbbSlWimgPKKnGovQ.png)\n",
    "\n",
    "Il y a quelques semaines, je cherchais désespérément un tutoriel décrivant comment utiliser VS Code avec Jupyter Notebooks et PySpark... sur un Mac. Et étonnamment, je n'ai trouvé aucun tutoriel qui passait mon test de \"explique-le-moi comme si j'avais cinq ans\".\n",
    "\n",
    "## Cet article est le résultat d'un samedi après-midi difficile.\n",
    "\n",
    "Le chemin de moindre résistance : les REPL à la rescousse\n",
    "\n",
    "De nos jours, j'ai très peu de temps libre pour jouer avec de nouvelles technologies. Quand je le fais, je veux que ce soit aussi indolore que possible. Et surtout, je veux que ce soit amusant - sinon, à quoi bon ?\n",
    "\n",
    "De plus, rien n'est pire que de perdre des heures de votre temps libre à configurer un environnement de développement. C'est juste douloureux.\n",
    "\n",
    "### VS Code avec Jupyter Notebooks\n",
    "\n",
    "Je suis un grand fan des REPL pour le développement rapide - par exemple, l'évaluation d'un nouveau framework, l'analyse de données, la correction de données, etc.\n",
    "\n",
    "Dans ces situations, je ne veux pas configurer un nouveau projet et être embourbé dans des complexités de configuration triviales. J'ai simplement besoin d'un bloc-notes pour écrire du code.\n",
    "\n",
    "Les Jupyter Notebooks sont un système basé sur REPL conçu pour analyser, visualiser et collaborer sur des données. Ils sont également excellents comme bloc-notes.\n",
    "\n",
    "### Qu'est-ce qu'un REPL ?\n",
    "\n",
    "Une boucle de lecture-évaluation-affichage (REPL), également appelée niveau supérieur interactif ou shell de langage, est un environnement de programmation informatique interactif simple qui prend des entrées utilisateur uniques, les exécute et renvoie le résultat à l'utilisateur ; un programme écrit dans un environnement REPL est exécuté morceau par morceau.\n",
    "\n",
    "[Wikipedia:Read–eval–print loop](https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop)\n",
    "\n",
    "Visual Studio Code prend en charge nativement les [blocs-notes](https://code.visualstudio.com/blogs/2021/11/08/custom-notebooks), y compris Jupyter.\n",
    "\n",
    "### Configuration\n",
    "\n",
    "#### Prérequis\n",
    "\n",
    "* Installez Docker. <br/>\n",
    "*Si vous utilisez un Mac et ne pouvez pas installer Docker Desktop en raison de restrictions de licence, consultez Colima.*\n",
    "* Installez VS Code.\n",
    "\n",
    "#### Conteneur de développement de VS Code\n",
    "\n",
    "1. Créez un nouveau répertoire pour votre projet.\n",
    "2. Créez un fichier Docker à la racine du répertoire du projet en utilisant le code ci-dessous. Au moment de la rédaction de cet article, la version actuelle de PySpark est 3.3.0. Je vérifierais ici pour vous assurer que vous utilisez la dernière version.\n",
    "\n",
    "```sh\n",
    "ARG IMAGE_VARIANT=slim-buster\n",
    "ARG OPENJDK_VERSION=8\n",
    "ARG PYTHON_VERSION=3.9.8\n",
    "\n",
    "FROM python:${PYTHON_VERSION}-${IMAGE_VARIANT} AS py3\n",
    "FROM openjdk:${OPENJDK_VERSION}-${IMAGE_VARIANT}\n",
    "\n",
    "COPY --from=py3 / /\n",
    "\n",
    "ARG PYSPARK_VERSION=3.3.0\n",
    "\n",
    "RUN pip --no-cache-dir install pyspark==${PYSPARK_VERSION}\n",
    "RUN pip --no-cache-dir install pandas\n",
    "RUN pip --no-cache-dir install ipykernel\n",
    "\n",
    "ENTRYPOINT [\"bash\"]\n",
    "```\n",
    "\n",
    "4. Within the .devcontainer directory, add the following JSON configuration.\n",
    "\n",
    "3. Créez un répertoire avec le nom `.devcontainer`.\n",
    "4. Dans le répertoire `.devcontainer`, ajoutez la configuration JSON suivante.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"name\": \"Dockerfile\",\n",
    "    \"context\": \"../\",\n",
    "    \"dockerFile\": \"../Dockerfile\",\n",
    "    \"extensions\": [\"ms-python.python\", \"ms-toolsai.jupyter\"],\n",
    "    \"settings\": {\n",
    "        \"terminal.integrated.shell.linux\": null\n",
    "    },\n",
    "    \"forwardPorts\": [4050]\n",
    "}\n",
    "```\n",
    "\n",
    "5. Dans le coin inférieur gauche de VS Code, cliquez sur le bouton Ouvrir la fenêtre distante → Ouvrir dans le conteneur.\n",
    "\n",
    "Cliquez [ici](https://code.visualstudio.com/docs/remote/remote-overview) pour en savoir plus sur le développement distant dans VS Code.\n",
    "\n",
    "VS Code redémarrera l'IDE et se connectera au conteneur de développement de VS Code - instancié à partir de l'image Docker définie à l'étape 2.\n",
    "\n",
    "C'est tout pour la configuration.\n",
    "\n",
    "## Développement de votre première application PySpark\n",
    "\n",
    "### Création d'un notebook\n",
    "\n",
    "1.Créez un nouveau fichier avec l'extension .ipynb dans votre répertoire de projet.\n",
    "2. Ouvrez le fichier - vous devriez voir l'expérience de bloc-notes de VS Code.\n",
    "\n",
    "### Données de test\n",
    "\n",
    "1. Dans le répertoire racine, ajoutez un nouveau dossier appelé data.\n",
    "2. Dans le dossier data, créez un nouveau fichier CSV appelé users.csv et ajoutez les données suivantes :\n",
    "```\n",
    "name,age,gender\n",
    "jon,45,male\n",
    "sarah,32,female\n",
    "jane,65,female\n",
    "jim,70,male\n",
    "joe,22,male\n",
    "```\n",
    "\n",
    "#### Exemple : Application Spark\n",
    "\n",
    "Cette section suppose que vous avez installé Docker, configuré un conteneur de développement de VS Code et créé un bloc-notes vide.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import *\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"test-app\")\\\n",
    "            .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"./data/users.csv\", \n",
    "                            header=\"true\", \n",
    "                            inferSchema=\"true\")\n",
    "\n",
    "df.createOrReplaceTempView(\"users\")\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT gender, AVG(age) as average_age\n",
    "FROM users\n",
    "GROUP BY gender\n",
    "\"\"\"\n",
    "query = spark.sql(sql)\n",
    "query.toPandas()\n",
    "\n",
    "```\n",
    "\n",
    "### Question circonstantielle à Chat\n",
    "\n",
    "❓*Cela fait bien longtemps que je fais un blocage sur Docker. Es-tu capable, en quelques mots, de m'expliquer en quoi Docker est une innovation majeure, et en quoi cela explique qu'il soit devenu indispensable, incontournable ?*\n",
    "\n",
    "Docker est une innovation majeure dans le domaine de la virtualisation et de la gestion des conteneurs. Il permet de créer des environnements d'exécution isolés, appelés conteneurs, qui encapsulent une application et toutes ses dépendances. Voici quelques raisons pour lesquelles Docker est devenu indispensable :\n",
    "* **Portabilité** : Les conteneurs Docker sont légers et portables, ce qui signifie qu'ils peuvent être exécutés sur n'importe quel système d'exploitation ou infrastructure qui prend en charge Docker. Cela facilite le déploiement et la gestion des applications sur différents environnements, qu'il s'agisse de serveurs locaux, de machines virtuelles ou de services cloud.\n",
    "* **Isolation** : Les conteneurs offrent une isolation complète des applications, ce qui signifie que chaque application s'exécute dans son propre environnement isolé. Cela garantit qu'une application ne peut pas interférer avec d'autres applications exécutées sur le même système, améliorant ainsi la stabilité et la sécurité.\n",
    "* **Rapidité et efficacité** : Docker utilise une approche de virtualisation légère, ce qui permet un démarrage rapide des conteneurs et une utilisation efficiente des ressources système. Les conteneurs partagent le même noyau de système d'exploitation, ce qui évite la surcharge liée au lancement de multiples machines virtuelles.\n",
    "* **Gestion des dépendances** : Docker simplifie la gestion des dépendances logicielles en encapsulant toutes les bibliothèques et les dépendances requises dans le conteneur. Cela permet d'éviter les conflits entre différentes versions de bibliothèques et facilite le déploiement cohérent des applications sur différentes plates-formes.\n",
    "* **Scalabilité** : Docker permet de facilement mettre à l'échelle horizontalement les applications en ajoutant ou en supprimant des instances de conteneurs selon les besoins. Cette flexibilité de mise à l'échelle facilite la gestion des charges de travail variables et l'optimisation des ressources.\n",
    "\n",
    "Dans l'ensemble, Docker a révolutionné la façon dont les applications sont développées, déployées et exécutées, offrant une solution efficace et cohérente pour la gestion des environnements logiciels. Son adoption généralisée s'explique par les avantages qu'il apporte en termes de portabilité, d'isolation, de rapidité, de gestion des dépendances et de scalabilité.\n",
    "\n",
    "❓*Es-tu d'accord avec ce résumé : Docker est la quintessence, synthétique, de la gestion de packages et de la gestion d'environnements (virtuels) ?*\n",
    "\n",
    "Oui, je suis d'accord avec ce résumé. Docker permet en effet de gérer de manière efficace les packages logiciels et les environnements virtuels en fournissant une plate-forme de conteneurisation. Il offre une approche légère et portable pour créer, distribuer et exécuter des applications avec toutes leurs dépendances dans un environnement isolé. Ainsi, Docker simplifie la gestion des packages et permet de créer des environnements cohérents et reproductibles, facilitant ainsi le développement, le déploiement et la gestion des applications.\n",
    "\n",
    "### 📌 Décorticons cela cellule par cellule.\n",
    "\n",
    "1. **Importation des bibliothèques** : La première cellule importe les bibliothèques Python PySpark et Pandas.\n",
    "2. **Connexion à Spark** : La deuxième cellule est l'endroit où nous définissons la connexion à Spark. Comme nous sommes en mode local, nous n'avons pas besoin de nous soucier d'une chaîne de connexion.\n",
    "3. **Lecture du CSV** dans une vue temporaire : Dans la troisième cellule, nous importons un fichier CSV du système de fichiers local dans Spark - le CSV contient des données de test.\n",
    "La deuxième étape crée une vue temporaire appelée 'users' - cela nous permet d'interroger la table en utilisant du SQL standard.\n",
    "4. **Requête** : Dans la dernière cellule, nous définissons une requête SQL qui renvoie l'âge moyen de tous les utilisateurs par genre. L'appel de fonction `toPandas()` convertit le dataframe Spark en un dataframe Panda - ce qui nous permet d'utiliser l'affichage des dataframes de VS Code.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*bLVoxtiISzMlrVZhPCMLFw.png)\n",
    "\n",
    "5. Cliquez sur \"Tout exécuter\" en haut pour exécuter toutes les cellules du notebook. Si tout fonctionne correctement, vous devriez voir un dataframe à deux lignes - comme représenté dans l'image ci-dessus.\n",
    "\n",
    "### Réflexions finales\n",
    "\n",
    "Utiliser Visual Studio Code avec les notebooks Jupyter et Docker est une manière simple de commencer avec PySpark.\n",
    "\n",
    "Si vous avez des conseils pour améliorer le flux de développement décrit ci-dessus, n'hésitez pas à me le faire savoir dans les commentaires.\n",
    "\n",
    "J'espère que cela vous a été intéressant.\n",
    "\n",
    "L'Architecte Yam Yam."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [**VS Code Remote Development**](https://code.visualstudio.com/docs/remote/remote-overview)\n",
    "\n",
    "**Visual Studio Code Remote Development** vous permet d'utiliser un conteneur, une machine distante ou le [**sous-système Windows pour Linux**](https://learn.microsoft.com/en-us/windows/wsl/) (WSL) en tant qu'environnement de développement complet.\n",
    "\n",
    "Vous pouvez :\n",
    "* Développer sur le même **système d'exploitation** que celui sur lequel vous déployez votre application, ou utiliser du matériel **plus puissant ou plus spécialisé**.\n",
    "* **Séparer** votre environnement de développement pour éviter d'impacter la **configuration de votre machine** locale.\n",
    "* Faciliter la **prise en main** des nouveaux contributeurs et maintenir tout le monde dans un **environnement cohérent**.\n",
    "* Utiliser des outils ou des environnements d'exécution qui **ne sont pas disponibles** sur votre système d'exploitation local, ou gérer **plusieurs versions** de ces outils.\n",
    "* Développer vos applications déployées sur Linux en utilisant le **sous-système Windows pour Linux**.\n",
    "* Accéder à un environnement de développement **existant** à partir de **plusieurs machines ou emplacements**.\n",
    "* Déboguer une **application qui s'exécute ailleurs**, comme sur le site d'un client ou dans le cloud.\n",
    "\n",
    "**Aucun code source** n'a besoin d'être sur votre machine locale pour bénéficier de ces avantages. Chaque extension du [pack d'extensions Remote Development](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack) peut exécuter des commandes et d'autres extensions directement dans un conteneur, dans WSL ou sur une machine distante, de sorte que tout se déroule comme si vous exécutez localement.\n",
    "\n",
    "![](https://code.visualstudio.com/assets/docs/remote/remote-overview/architecture.png)\n",
    "\n",
    "## Premiers pas\n",
    "\n",
    "### Pack d'extensions Remote Development\n",
    "\n",
    "Le [pack d'extensions Remote Development](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack) comprend trois extensions. Consultez les articles suivants pour commencer avec chacune d'entre elles :\n",
    "* [Remote - SSH](https://code.visualstudio.com/docs/remote/ssh) - Connectez-vous à n'importe quel emplacement en ouvrant des dossiers sur une machine distante/VM en utilisant SSH.\n",
    "* [Dev Containers](https://code.visualstudio.com/docs/devcontainers/containers) - Travaillez avec une chaîne d'outils séparée ou une application basée sur des conteneurs à l'intérieur (ou montée dans) un conteneur.\n",
    "* [WSL](https://code.visualstudio.com/docs/remote/wsl) - Obtenez une expérience de développement basée sur Linux dans le sous-système Windows pour Linux.\n",
    "\n",
    "Bien que la plupart des extensions de VS Code devraient fonctionner sans modification dans un environnement distant, les auteurs d'extensions peuvent en savoir plus sur le [support du développement à distance](https://code.visualstudio.com/api/advanced-topics/remote-extensions).\n",
    "\n",
    "### Tutoriels sur le \"à distance\"\n",
    "\n",
    "Les tutoriels ci-dessous vous guideront pour exécuter Visual Studio Code avec les extensions Remote Development.\n",
    "\n",
    "|Tutoriel|Description|\n",
    "|-|-|\n",
    "|[À distance via SSH](https://code.visualstudio.com/docs/remote/ssh-tutorial)|Connectez-vous à des machines distantes et virtuelles avec Visual Studio Code via SSH.|\n",
    "|[Travaillez dans WSL](https://code.visualstudio.com/docs/remote/wsl-tutorial)|Exécutez Visual Studio Code dans le sous-système Windows pour Linux.|\n",
    "|[Développez dans des conteneurs](https://code.visualstudio.com/docs/devcontainers/tutorial)|Exécutez Visual Studio Code dans un conteneur Docker.|\n",
    "|[GitHub Codespaces](https://docs.github.com/en/codespaces/developing-in-codespaces/using-github-codespaces-in-visual-studio-code)|Connectez-vous à un espace de codes avec Visual Studio Code.|\n",
    "\n",
    "### GitHub Codespaces\n",
    "\n",
    "[GitHub Codespaces](https://code.visualstudio.com/docs/remote/codespaces) fournit des environnements de développement à distance gérés pour vous. Vous pouvez configurer et créer un environnement de développement hébergé dans le cloud, qui est démarré et disponible lorsque vous en avez besoin.\n",
    "\n",
    "### Le serveur VS Code (version privée de prévisualisation)\n",
    "\n",
    "[Le serveur VS Code](https://code.visualstudio.com/docs/remote/vscode-server) est un service en version privée de prévisualisation que vous pouvez exécuter sur une machine de développement distante, comme votre PC de bureau ou une machine virtuelle (VM). Il vous permet de vous connecter en toute sécurité à cette machine distante de n'importe où via une URL vscode.dev, sans nécessiter de SSH.\n",
    "\n",
    "### Petit point avec Chat\n",
    "\n",
    "*Si tu n'as pas oublié tous nos échanges ci-dessus pour mettre en place d'une architecture de développement pyspark avec un client de développement VS Code+Jupyter+PySpark/Windows en interaction avec un serveur de cluster Spark intallé sur la même machine, mais dans le sous-système Ubuntu/WSL, laquelle architecture s'est heurtée à des problèmes de configuration et notamment de paramétrage du firewall, n'es-tu pas d'accord que finalement, ce dernier tuto que je t'ai passé, et qui concerne l'extension VS Code Remote Developement, répond exactement à mes besoins, et que cela mérite donc d'être investi, c'est à dire installé et testé sur la base des tutoriaux fournis ?*\n",
    "\n",
    "Oui, en fonction des difficultés que vous avez rencontrées lors de la configuration de l'architecture de développement PySpark précédente, l'utilisation de l'extension VS Code Remote Development peut être une solution intéressante pour répondre à vos besoins. Cette extension vous permet d'utiliser un environnement de développement distant, tel que le sous-système Windows pour Linux (WSL), pour travailler avec des outils comme Jupyter, PySpark et VS Code.\n",
    "\n",
    "En utilisant cette approche, vous pouvez éviter les problèmes de configuration du firewall et bénéficier d'un environnement de développement plus cohérent et facile à gérer. Les tutoriels fournis avec l'extension vous guideront pas à pas dans la mise en place de cette configuration et vous permettront de tester son fonctionnement.\n",
    "\n",
    "Il est recommandé d'installer et de tester l'extension en suivant les tutoriels fournis pour évaluer si elle répond effectivement à vos besoins et facilite votre workflow de développement PySpark avec VS Code et Jupyter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [**PySpark.SQL and Jupyter Notebooks on Visual Studio Code (Python kernel)**](https://blog.openthreatresearch.com/spark_jupyter_notebook_vscode)\n",
    "\n",
    "Dans cet article de blog, je vais partager les étapes que vous pouvez suivre pour exécuter des commandes PySpark.SQL (Spark + Python) en utilisant un notebook Jupyter dans Visual Studio Code (VSCode). Lors du développement de cet article, j'ai utilisé un noyau Python sur un ordinateur Windows.\n",
    "\n",
    "## Prérequis\n",
    "\n",
    "Pour suivre les étapes de cet article, vous devez installer les éléments suivants sur votre ordinateur Windows :\n",
    "* `Java` : vous pouvez trouver les étapes d'installation ici.\n",
    "* `Visual Studio Code` : vous pouvez trouver les étapes d'installation ici.\n",
    "* `Extension Python pour Visual Studio Code` : vous pouvez trouver les étapes d'installation ici.\n",
    "* `Interpréteur Python` : vous pouvez trouver les étapes d'installation ici."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration d'une session PySpark.SQL\n",
    "\n",
    "### 1) Création d'un notebook Jupyter dans VSCode\n",
    "\n",
    "Créez un notebook Jupyter en suivant les étapes décrites dans Mon premier notebook Jupyter sur Visual Studio Code (noyau Python).\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_01_spark_new_notebook.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Installation de la bibliothèque PySpark Python\n",
    "\n",
    "À l'aide de la première cellule de notre notebook, exécutez le code suivant pour installer l'`API Python` pour Spark.\n",
    "\n",
    "```sh\n",
    "!pip install pyspark\n",
    "```\n",
    "\n",
    "Vous pouvez également utiliser le terminal de VSCode pour installer PySpark. Les étapes pour installer une bibliothèque Python à l'aide d'un notebook Jupyter ou du terminal dans VSCode sont décrites ici.\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_02_spark_pyspark_installation.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.4.0\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: C:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show pyspark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Importation de la classe SparkSession\n",
    "\n",
    "Nous commençons par importer la classe SparkSession du module PySpark SQL.\n",
    "\n",
    "La `SparkSession` est le point d'entrée principal pour les fonctionnalités DataFrame et SQL. Un SparkSession peut être utilisé pour créer un DataFrame, enregistrer un DataFrame en tant que table, exécuter des requêtes SQL sur les tables, mettre en cache des tables, et même lire des fichiers Parquet.\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_03_spark_pyspark_sql_spark_session.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Création d'un SparkSession\n",
    "\n",
    "Pour créer un SparkSession, nous utilisons la classe [`Builder`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.SparkSession.Builder).\n",
    "\n",
    "Nous donnons un nom à notre application Spark (`OTR`) et ajoutons une configuration caseSensitive.\n",
    "\n",
    "Nous attribuons le SparkSession à une variable nommée `spark`.\n",
    "\n",
    "```python\n",
    "spark = SparkSession.builder.appName(\"OTR\").config(\"spark.sql.caseSensitive\", \"True\").getOrCreate()\n",
    "```\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_04_spark_pyspark_sql_spark_session_builder.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession.Builder at 0x2020ffabd50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "otr = SparkSession.builder.appName(\"OTR\").config(\"spark.sql.caseSensitive\", \"True\")\n",
    "display(otr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x2020986c250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "display(SparkConf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLUSERSPROFILE: C:\\ProgramData\n",
      "APPDATA: C:\\Users\\franc\\AppData\\Roaming\n",
      "CHROME_CRASHPAD_PIPE_NAME: \\\\.\\pipe\\LOCAL\\crashpad_8680_AIHYRYQPRMSYWVQJ\n",
      "COMMONPROGRAMFILES: C:\\Program Files\\Common Files\n",
      "COMMONPROGRAMFILES(X86): C:\\Program Files (x86)\\Common Files\n",
      "COMMONPROGRAMW6432: C:\\Program Files\\Common Files\n",
      "COMPUTERNAME: BELLADONNA\n",
      "COMSPEC: C:\\WINDOWS\\system32\\cmd.exe\n",
      "DRIVERDATA: C:\\Windows\\System32\\Drivers\\DriverData\n",
      "EFC_8804: 1\n",
      "ELECTRON_RUN_AS_NODE: 1\n",
      "HOMEDRIVE: C:\n",
      "HOMEPATH: \\Users\\franc\n",
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-17\n",
      "JPY_INTERRUPT_EVENT: 2708\n",
      "LOCALAPPDATA: C:\\Users\\franc\\AppData\\Local\n",
      "LOGONSERVER: \\\\BELLADONNA\n",
      "NUMBER_OF_PROCESSORS: 12\n",
      "OMP_NUM_THREADS: 4\n",
      "ONEDRIVE: C:\\Users\\franc\\OneDrive\n",
      "ONEDRIVECONSUMER: C:\\Users\\franc\\OneDrive\n",
      "ORIGINAL_XDG_CURRENT_DESKTOP: undefined\n",
      "OS: Windows_NT\n",
      "PATH: c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311;c:\\Users\\franc\\AppData\\Roaming\\Python\\Python311\\Scripts;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\IDM Computer Solutions\\UltraEdit;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\GTK3-Runtime Win64\\bin;C:\\OSGeo4W\\bin;C:\\Program Files\\Graphviz\\bin;C:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\;C:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\;C:\\Users\\franc\\.pyenv\\pyenv-win\\bin;C:\\Users\\franc\\.pyenv\\pyenv-win\\shims;C:\\Users\\franc\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\franc\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Program Files\\JetBrains\\PyCharm Community Edition 2022.1.3\\bin;;C:\\Users\\franc\\AppData\\Local\\GitHubDesktop\\bin;C:\\Users\\franc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts;C:\\Users\\franc\\AppData\\Local\\Programs\\ffmpeg-5.1.2-essentials_build\\bin;C:\\Users\\franc\\AppData\\Local\\Programs\\MiKTeX\\miktex\\bin\\x64\\;;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\IDM Computer Solutions\\UltraEdit;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\GTK3-Runtime Win64\\bin;C:\\OSGeo4W\\bin;C:\\Program Files\\Graphviz\\bin;C:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\;C:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\;C:\\Users\\franc\\.pyenv\\pyenv-win\\bin;C:\\Users\\franc\\.pyenv\\pyenv-win\\shims;C:\\Users\\franc\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\franc\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Program Files\\JetBrains\\PyCharm Community Edition 2022.1.3\\bin;;C:\\Users\\franc\\AppData\\Local\\GitHubDesktop\\bin;C:\\Users\\franc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts;C:\\Users\\franc\\AppData\\Local\\Programs\\ffmpeg-5.1.2-essentials_build\\bin;C:\\Users\\franc\\AppData\\Local\\Programs\\MiKTeX\\miktex\\bin\\x64\\;\n",
      "PATHEXT: .COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\n",
      "PROCESSOR_ARCHITECTURE: AMD64\n",
      "PROCESSOR_IDENTIFIER: Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\n",
      "PROCESSOR_LEVEL: 6\n",
      "PROCESSOR_REVISION: 9e0a\n",
      "PROGRAMDATA: C:\\ProgramData\n",
      "PROGRAMFILES: C:\\Program Files\n",
      "PROGRAMFILES(X86): C:\\Program Files (x86)\n",
      "PROGRAMW6432: C:\\Program Files\n",
      "PSMODULEPATH: C:\\Program Files\\WindowsPowerShell\\Modules;C:\\WINDOWS\\system32\\WindowsPowerShell\\v1.0\\Modules\n",
      "PUBLIC: C:\\Users\\Public\n",
      "PYCHARM COMMUNITY EDITION: C:\\Program Files\\JetBrains\\PyCharm Community Edition 2022.1.3\\bin;\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING: 1\n",
      "PYENV: C:\\Users\\franc\\.pyenv\\pyenv-win\\\n",
      "PYENV_HOME: C:\\Users\\franc\\.pyenv\\pyenv-win\\\n",
      "PYENV_ROOT: C:\\Users\\franc\\.pyenv\\pyenv-win\\\n",
      "PYSPARK_SUBMIT_ARGS: --master local[*] pyspark-shell\n",
      "PYTHONIOENCODING: utf-8\n",
      "PYTHONUNBUFFERED: 1\n",
      "SESSIONNAME: Console\n",
      "SPARK_HOME: C:\\Program Files\\Spark\n",
      "SYSTEMDRIVE: C:\n",
      "SYSTEMROOT: C:\\WINDOWS\n",
      "TEMP: C:\\Users\\franc\\AppData\\Local\\Temp\n",
      "TMP: C:\\Users\\franc\\AppData\\Local\\Temp\n",
      "USERDOMAIN: BELLADONNA\n",
      "USERDOMAIN_ROAMINGPROFILE: BELLADONNA\n",
      "USERNAME: franc\n",
      "USERPROFILE: C:\\Users\\franc\n",
      "VSCODE_AMD_ENTRYPOINT: vs/workbench/api/node/extensionHostProcess\n",
      "VSCODE_CODE_CACHE_PATH: C:\\Users\\franc\\AppData\\Roaming\\Code\\CachedData\\b3e4e68a0bc097f0ae7907b217c1119af9e03435\n",
      "VSCODE_CRASH_REPORTER_PROCESS_TYPE: extensionHost\n",
      "VSCODE_CWD: C:\\Users\\franc\\AppData\\Local\\Programs\\Microsoft VS Code\n",
      "VSCODE_HANDLES_UNCAUGHT_ERRORS: true\n",
      "VSCODE_IPC_HOOK: \\\\.\\pipe\\528ce89b3964f0be503a218713263ba8-1.78.2-main-sock\n",
      "VSCODE_L10N_BUNDLE_LOCATION: file:///c%3A/Users/franc/.vscode/extensions/ms-ceintl.vscode-language-pack-fr-1.78.2023051009/translations/extensions/vscode.json-language-features.i18n.json\n",
      "VSCODE_NLS_CONFIG: {\"locale\":\"fr\",\"osLocale\":\"fr-fr\",\"availableLanguages\":{\"*\":\"fr\"},\"_languagePackId\":\"464e33a418b005a7ef0102b15075ed48.fr\",\"_translationsConfigFile\":\"C:\\\\Users\\\\franc\\\\AppData\\\\Roaming\\\\Code\\\\clp\\\\464e33a418b005a7ef0102b15075ed48.fr\\\\tcf.json\",\"_cacheRoot\":\"C:\\\\Users\\\\franc\\\\AppData\\\\Roaming\\\\Code\\\\clp\\\\464e33a418b005a7ef0102b15075ed48.fr\",\"_resolvedLanguagePackCoreLocation\":\"C:\\\\Users\\\\franc\\\\AppData\\\\Roaming\\\\Code\\\\clp\\\\464e33a418b005a7ef0102b15075ed48.fr\\\\b3e4e68a0bc097f0ae7907b217c1119af9e03435\",\"_corruptedFile\":\"C:\\\\Users\\\\franc\\\\AppData\\\\Roaming\\\\Code\\\\clp\\\\464e33a418b005a7ef0102b15075ed48.fr\\\\corrupted.info\",\"_languagePackSupport\":true}\n",
      "VSCODE_PID: 8680\n",
      "WINDIR: C:\\WINDOWS\n",
      "PYDEVD_USE_FRAME_EVAL: NO\n",
      "TERM: xterm-color\n",
      "CLICOLOR: 1\n",
      "FORCE_COLOR: 1\n",
      "CLICOLOR_FORCE: 1\n",
      "PAGER: cat\n",
      "GIT_PAGER: cat\n",
      "MPLBACKEND: module://matplotlib_inline.backend_inline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Récupérer toutes les variables d'environnement\n",
    "env_vars = os.environ\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master local[*] pyspark-shell\"\n",
    "\n",
    "# Afficher les valeurs des variables d'environnement\n",
    "for var, value in env_vars.items():\n",
    "    print(f'{var}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.sql.caseSensitive', 'True')\n",
      "spark.sql.caseSensitive=True\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.sql.caseSensitive\", \"True\")\n",
    "\n",
    "# Obtention des paramètres de configuration\n",
    "configurations = conf.getAll()\n",
    "\n",
    "# Affichage des paramètres de configuration\n",
    "for config in configurations:\n",
    "    print(config)\n",
    "\n",
    "\n",
    "# Affichage des valeurs par défaut et des paramètres définis\n",
    "print(conf.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mappName(\u001b[39m\"\u001b[39m\u001b[39mOTR\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mconfig(\u001b[39m\"\u001b[39m\u001b[39mspark.sql.caseSensitive\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTrue\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mgetOrCreate()\n\u001b[0;32m      2\u001b[0m display(spark)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"OTR\").config(\"spark.sql.caseSensitive\", \"True\").getOrCreate()\n",
    "display(spark)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5) Vérification du SparkSession\n",
    "\n",
    "Une fois que le SparkSession est construit, nous pouvons exécuter la variable spark pour la vérification.\n",
    "\n",
    "```python\n",
    "spark\n",
    "```\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_05_spark_pyspark_sql_spark_session_info.jpg)\n",
    "\n",
    "## Exécution de plus de commandes Spark\n",
    "\n",
    "Pour la dernière section de cet article de blog, je partage trois autres commandes de base qui sont très utiles lors de l'exécution de tâches avec Spark :\n",
    "* Création d'un DataFrame Spark en utilisant la méthode `read.json`.\n",
    "* Création d'une vue temporaire d'un DataFrame Spark en utilisant la méthode `createOrReplaceTempView`.\n",
    "* Exécution d'une requête de type SQL en utilisant la méthode `sql`.\n",
    "\n",
    "### 0) Importation d'un jeu de données Mordor\n",
    "\n",
    "Afin de vous montrer ces exemples, nous avons besoin de données. Par conséquent, j'utiliserai un jeu de données Mordor qui contient des journaux d'événements de sécurité pour l'exécution d'une preuve de concept publique d'exploitation de vulnérabilités Exchange (vulnérabilité CVE-2021-26855 de falsification de requête côté serveur (SSRF)).\n",
    "\n",
    "Téléchargez le jeu de données Mordor (fichier json) en suivant les étapes décrites dans l'article [Importing a Mordor Dataset with Jupyter Notebooks on Visual Studio Code (noyau Python)](https://blog.openthreatresearch.com/importing-mordor-dataset-jupyter-notebook-vscode).\n",
    "\n",
    "```python\n",
    "# Importing libraries\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "# Downloading and Extracting Json File\n",
    "url = 'https://raw.githubusercontent.com/OTRF/mordor/master/datasets/small/windows/persistence/host/proxylogon_ssrf_rce_poc.zip'\n",
    "zipFileRequest = requests.get(url)\n",
    "zipFile = ZipFile(BytesIO(zipFileRequest.content))\n",
    "jsonFilePath = zipFile.extract(zipFile.namelist()[0])\n",
    "jsonFilePath\n",
    "```\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_06_spark_mordor_file.jpg)\n",
    "\n",
    "\n",
    "### 1) Création d'un DataFrame Spark\n",
    "\n",
    "Pour créer un DataFrame Spark à partir d'un fichier `JSON`, nous utilisons la méthode `read.json`.\n",
    "\n",
    "Nous utilisons la variable `jsonFilePath` de la section précédente qui contient le chemin ou le répertoire où le fichier `JSON` a été stocké.\n",
    "\n",
    "```python\n",
    "# Creating a Spark Dataframe\n",
    "df = spark.read.json(jsonFilePath)\n",
    "# Validating Type of Output\n",
    "type(df)\n",
    "```\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_07_spark_dataframe.jpg)\n",
    "\n",
    "### 2) Création d'une vue temporaire d'un DataFrame Spark\n",
    "\n",
    "Pour créer une vue temporaire d'un DataFrame Spark, nous utilisons la méthode `createOrReplaceTempView`.\n",
    "\n",
    "Nous pouvons utiliser cette vue temporaire d'un DataFrame Spark comme une table `SQL` et définir des requêtes de type SQL pour analyser nos données.\n",
    "\n",
    "Nous utiliserons le DataFrame Spark `df` défini dans la section précédente. Le nom que nous utilisons pour notre vue temporaire est `mordorTable`.\n",
    "\n",
    "```python\n",
    "df.createOrReplaceTempView('mordorTable')\n",
    "```\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_08_spark_dataframe_temporary_view.jpg)\n",
    "\n",
    "### 3) Exécution d'une requête de type SQL\n",
    "\n",
    "Pour exécuter une requête de type SQL, nous utilisons la méthode `sql`.\n",
    "\n",
    "En utilisant `mordorTable` comme référence, nous exécuterons le code suivant pour résumer les journaux d'événements de sécurité fournis par le jeu de données.\n",
    "\n",
    "Nous effectuons une opération de comptage par empilement sur les données, et nous regroupons le résultat par Nom de l'hôte (`Hostname`), Canal (`Channel`) et ID d'événement (`EventID`).\n",
    "\n",
    "```python\n",
    "df = spark.sql(\n",
    "'''\n",
    "SELECT Hostname,Channel,EventID, Count(*) as count\n",
    "FROM mordorTable\n",
    "GROUP BY Hostname,Channel,EventID\n",
    "ORDER BY count DESC\n",
    "'''\n",
    ")\n",
    "df.show(truncate=False)\n",
    "```\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_09_spark_dataframe_temporary_view_sql_query.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
