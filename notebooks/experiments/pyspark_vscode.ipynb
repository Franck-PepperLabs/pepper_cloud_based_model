{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PySpark VS Code**\n",
    "\n",
    "PySpark in VS Code.\n",
    "* [PySpark Development Made Simple. Using VS Code, Jupyter Notebooks, and‚Ä¶ | by Jason Clarke | Better Programming](https://betterprogramming.pub/pyspark-development-made-simple-9449a893ab17)\n",
    "* [PySpark.SQL and Jupyter Notebooks on Visual Studio Code (Python kernel)](https://blog.openthreatresearch.com/spark_jupyter_notebook_vscode)\n",
    "* [Run jobs: Spark & Hive Tools for VS Code - SQL Server Big Data Clusters | Microsoft Learn](https://learn.microsoft.com/en-us/sql/big-data-cluster/spark-hive-tools-vscode?view=sql-server-ver15)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PySpark Development Made Simple. Using VS Code, Jupyter Notebooks, and‚Ä¶ | by Jason Clarke | Better Programming](https://betterprogramming.pub/pyspark-development-made-simple-9449a893ab17)\n",
    "\n",
    "Using VS Code, Jupyter Notebooks, and Docker\n",
    "\n",
    "By **Jason Clarke**, Better Programming, Sep 30, 2022\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*6DROvAbbSlWimgPKKnGovQ.png)\n",
    "\n",
    "Il y a quelques semaines, je cherchais d√©sesp√©r√©ment un tutoriel d√©crivant comment utiliser VS Code avec Jupyter Notebooks et PySpark... sur un Mac. Et √©tonnamment, je n'ai trouv√© aucun tutoriel qui passait mon test de \"explique-le-moi comme si j'avais cinq ans\".\n",
    "\n",
    "## Cet article est le r√©sultat d'un samedi apr√®s-midi difficile.\n",
    "\n",
    "Le chemin de moindre r√©sistance : les REPL √† la rescousse\n",
    "\n",
    "De nos jours, j'ai tr√®s peu de temps libre pour jouer avec de nouvelles technologies. Quand je le fais, je veux que ce soit aussi indolore que possible. Et surtout, je veux que ce soit amusant - sinon, √† quoi bon ?\n",
    "\n",
    "De plus, rien n'est pire que de perdre des heures de votre temps libre √† configurer un environnement de d√©veloppement. C'est juste douloureux.\n",
    "\n",
    "### VS Code avec Jupyter Notebooks\n",
    "\n",
    "Je suis un grand fan des REPL pour le d√©veloppement rapide - par exemple, l'√©valuation d'un nouveau framework, l'analyse de donn√©es, la correction de donn√©es, etc.\n",
    "\n",
    "Dans ces situations, je ne veux pas configurer un nouveau projet et √™tre embourb√© dans des complexit√©s de configuration triviales. J'ai simplement besoin d'un bloc-notes pour √©crire du code.\n",
    "\n",
    "Les Jupyter Notebooks sont un syst√®me bas√© sur REPL con√ßu pour analyser, visualiser et collaborer sur des donn√©es. Ils sont √©galement excellents comme bloc-notes.\n",
    "\n",
    "### Qu'est-ce qu'un REPL ?\n",
    "\n",
    "Une boucle de lecture-√©valuation-affichage (REPL), √©galement appel√©e niveau sup√©rieur interactif ou shell de langage, est un environnement de programmation informatique interactif simple qui prend des entr√©es utilisateur uniques, les ex√©cute et renvoie le r√©sultat √† l'utilisateur ; un programme √©crit dans un environnement REPL est ex√©cut√© morceau par morceau.\n",
    "\n",
    "[Wikipedia:Read‚Äìeval‚Äìprint loop](https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop)\n",
    "\n",
    "Visual Studio Code prend en charge nativement les [blocs-notes](https://code.visualstudio.com/blogs/2021/11/08/custom-notebooks), y compris Jupyter.\n",
    "\n",
    "### Configuration\n",
    "\n",
    "#### Pr√©requis\n",
    "\n",
    "* Installez Docker. <br/>\n",
    "*Si vous utilisez un Mac et ne pouvez pas installer Docker Desktop en raison de restrictions de licence, consultez Colima.*\n",
    "* Installez VS Code.\n",
    "\n",
    "#### Conteneur de d√©veloppement de VS Code\n",
    "\n",
    "1. Cr√©ez un nouveau r√©pertoire pour votre projet.\n",
    "2. Cr√©ez un fichier Docker √† la racine du r√©pertoire du projet en utilisant le code ci-dessous. Au moment de la r√©daction de cet article, la version actuelle de PySpark est 3.3.0. Je v√©rifierais ici pour vous assurer que vous utilisez la derni√®re version.\n",
    "\n",
    "```sh\n",
    "ARG IMAGE_VARIANT=slim-buster\n",
    "ARG OPENJDK_VERSION=8\n",
    "ARG PYTHON_VERSION=3.9.8\n",
    "\n",
    "FROM python:${PYTHON_VERSION}-${IMAGE_VARIANT} AS py3\n",
    "FROM openjdk:${OPENJDK_VERSION}-${IMAGE_VARIANT}\n",
    "\n",
    "COPY --from=py3 / /\n",
    "\n",
    "ARG PYSPARK_VERSION=3.3.0\n",
    "\n",
    "RUN pip --no-cache-dir install pyspark==${PYSPARK_VERSION}\n",
    "RUN pip --no-cache-dir install pandas\n",
    "RUN pip --no-cache-dir install ipykernel\n",
    "\n",
    "ENTRYPOINT [\"bash\"]\n",
    "```\n",
    "\n",
    "4. Within the .devcontainer directory, add the following JSON configuration.\n",
    "\n",
    "3. Cr√©ez un r√©pertoire avec le nom `.devcontainer`.\n",
    "4. Dans le r√©pertoire `.devcontainer`, ajoutez la configuration JSON suivante.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"name\": \"Dockerfile\",\n",
    "    \"context\": \"../\",\n",
    "    \"dockerFile\": \"../Dockerfile\",\n",
    "    \"extensions\": [\"ms-python.python\", \"ms-toolsai.jupyter\"],\n",
    "    \"settings\": {\n",
    "        \"terminal.integrated.shell.linux\": null\n",
    "    },\n",
    "    \"forwardPorts\": [4050]\n",
    "}\n",
    "```\n",
    "\n",
    "5. Dans le coin inf√©rieur gauche de VS Code, cliquez sur le bouton Ouvrir la fen√™tre distante ‚Üí Ouvrir dans le conteneur.\n",
    "\n",
    "Cliquez [ici](https://code.visualstudio.com/docs/remote/remote-overview) pour en savoir plus sur le d√©veloppement distant dans VS Code.\n",
    "\n",
    "VS Code red√©marrera l'IDE et se connectera au conteneur de d√©veloppement de VS Code - instanci√© √† partir de l'image Docker d√©finie √† l'√©tape 2.\n",
    "\n",
    "C'est tout pour la configuration.\n",
    "\n",
    "## D√©veloppement de votre premi√®re application PySpark\n",
    "\n",
    "### Cr√©ation d'un notebook\n",
    "\n",
    "1.Cr√©ez un nouveau fichier avec l'extension .ipynb dans votre r√©pertoire de projet.\n",
    "2. Ouvrez le fichier - vous devriez voir l'exp√©rience de bloc-notes de VS Code.\n",
    "\n",
    "### Donn√©es de test\n",
    "\n",
    "1. Dans le r√©pertoire racine, ajoutez un nouveau dossier appel√© data.\n",
    "2. Dans le dossier data, cr√©ez un nouveau fichier CSV appel√© users.csv et ajoutez les donn√©es suivantes :\n",
    "```\n",
    "name,age,gender\n",
    "jon,45,male\n",
    "sarah,32,female\n",
    "jane,65,female\n",
    "jim,70,male\n",
    "joe,22,male\n",
    "```\n",
    "\n",
    "#### Exemple : Application Spark\n",
    "\n",
    "Cette section suppose que vous avez install√© Docker, configur√© un conteneur de d√©veloppement de VS Code et cr√©√© un bloc-notes vide.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import *\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"test-app\")\\\n",
    "            .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"./data/users.csv\", \n",
    "                            header=\"true\", \n",
    "                            inferSchema=\"true\")\n",
    "\n",
    "df.createOrReplaceTempView(\"users\")\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT gender, AVG(age) as average_age\n",
    "FROM users\n",
    "GROUP BY gender\n",
    "\"\"\"\n",
    "query = spark.sql(sql)\n",
    "query.toPandas()\n",
    "\n",
    "```\n",
    "\n",
    "### Question circonstantielle √† Chat\n",
    "\n",
    "‚ùì*Cela fait bien longtemps que je fais un blocage sur Docker. Es-tu capable, en quelques mots, de m'expliquer en quoi Docker est une innovation majeure, et en quoi cela explique qu'il soit devenu indispensable, incontournable ?*\n",
    "\n",
    "Docker est une innovation majeure dans le domaine de la virtualisation et de la gestion des conteneurs. Il permet de cr√©er des environnements d'ex√©cution isol√©s, appel√©s conteneurs, qui encapsulent une application et toutes ses d√©pendances. Voici quelques raisons pour lesquelles Docker est devenu indispensable :\n",
    "* **Portabilit√©** : Les conteneurs Docker sont l√©gers et portables, ce qui signifie qu'ils peuvent √™tre ex√©cut√©s sur n'importe quel syst√®me d'exploitation ou infrastructure qui prend en charge Docker. Cela facilite le d√©ploiement et la gestion des applications sur diff√©rents environnements, qu'il s'agisse de serveurs locaux, de machines virtuelles ou de services cloud.\n",
    "* **Isolation** : Les conteneurs offrent une isolation compl√®te des applications, ce qui signifie que chaque application s'ex√©cute dans son propre environnement isol√©. Cela garantit qu'une application ne peut pas interf√©rer avec d'autres applications ex√©cut√©es sur le m√™me syst√®me, am√©liorant ainsi la stabilit√© et la s√©curit√©.\n",
    "* **Rapidit√© et efficacit√©** : Docker utilise une approche de virtualisation l√©g√®re, ce qui permet un d√©marrage rapide des conteneurs et une utilisation efficiente des ressources syst√®me. Les conteneurs partagent le m√™me noyau de syst√®me d'exploitation, ce qui √©vite la surcharge li√©e au lancement de multiples machines virtuelles.\n",
    "* **Gestion des d√©pendances** : Docker simplifie la gestion des d√©pendances logicielles en encapsulant toutes les biblioth√®ques et les d√©pendances requises dans le conteneur. Cela permet d'√©viter les conflits entre diff√©rentes versions de biblioth√®ques et facilite le d√©ploiement coh√©rent des applications sur diff√©rentes plates-formes.\n",
    "* **Scalabilit√©** : Docker permet de facilement mettre √† l'√©chelle horizontalement les applications en ajoutant ou en supprimant des instances de conteneurs selon les besoins. Cette flexibilit√© de mise √† l'√©chelle facilite la gestion des charges de travail variables et l'optimisation des ressources.\n",
    "\n",
    "Dans l'ensemble, Docker a r√©volutionn√© la fa√ßon dont les applications sont d√©velopp√©es, d√©ploy√©es et ex√©cut√©es, offrant une solution efficace et coh√©rente pour la gestion des environnements logiciels. Son adoption g√©n√©ralis√©e s'explique par les avantages qu'il apporte en termes de portabilit√©, d'isolation, de rapidit√©, de gestion des d√©pendances et de scalabilit√©.\n",
    "\n",
    "‚ùì*Es-tu d'accord avec ce r√©sum√© : Docker est la quintessence, synth√©tique, de la gestion de packages et de la gestion d'environnements (virtuels) ?*\n",
    "\n",
    "Oui, je suis d'accord avec ce r√©sum√©. Docker permet en effet de g√©rer de mani√®re efficace les packages logiciels et les environnements virtuels en fournissant une plate-forme de conteneurisation. Il offre une approche l√©g√®re et portable pour cr√©er, distribuer et ex√©cuter des applications avec toutes leurs d√©pendances dans un environnement isol√©. Ainsi, Docker simplifie la gestion des packages et permet de cr√©er des environnements coh√©rents et reproductibles, facilitant ainsi le d√©veloppement, le d√©ploiement et la gestion des applications.\n",
    "\n",
    "### üìå D√©corticons cela cellule par cellule.\n",
    "\n",
    "1. **Importation des biblioth√®ques** : La premi√®re cellule importe les biblioth√®ques Python PySpark et Pandas.\n",
    "2. **Connexion √† Spark** : La deuxi√®me cellule est l'endroit o√π nous d√©finissons la connexion √† Spark. Comme nous sommes en mode local, nous n'avons pas besoin de nous soucier d'une cha√Æne de connexion.\n",
    "3. **Lecture du CSV** dans une vue temporaire : Dans la troisi√®me cellule, nous importons un fichier CSV du syst√®me de fichiers local dans Spark - le CSV contient des donn√©es de test.\n",
    "La deuxi√®me √©tape cr√©e une vue temporaire appel√©e 'users' - cela nous permet d'interroger la table en utilisant du SQL standard.\n",
    "4. **Requ√™te** : Dans la derni√®re cellule, nous d√©finissons une requ√™te SQL qui renvoie l'√¢ge moyen de tous les utilisateurs par genre. L'appel de fonction `toPandas()` convertit le dataframe Spark en un dataframe Panda - ce qui nous permet d'utiliser l'affichage des dataframes de VS Code.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*bLVoxtiISzMlrVZhPCMLFw.png)\n",
    "\n",
    "5. Cliquez sur \"Tout ex√©cuter\" en haut pour ex√©cuter toutes les cellules du notebook. Si tout fonctionne correctement, vous devriez voir un dataframe √† deux lignes - comme repr√©sent√© dans l'image ci-dessus.\n",
    "\n",
    "### R√©flexions finales\n",
    "\n",
    "Utiliser Visual Studio Code avec les notebooks Jupyter et Docker est une mani√®re simple de commencer avec PySpark.\n",
    "\n",
    "Si vous avez des conseils pour am√©liorer le flux de d√©veloppement d√©crit ci-dessus, n'h√©sitez pas √† me le faire savoir dans les commentaires.\n",
    "\n",
    "J'esp√®re que cela vous a √©t√© int√©ressant.\n",
    "\n",
    "L'Architecte Yam Yam."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [**VS Code Remote Development**](https://code.visualstudio.com/docs/remote/remote-overview)\n",
    "\n",
    "**Visual Studio Code Remote Development** vous permet d'utiliser un conteneur, une machine distante ou le [**sous-syst√®me Windows pour Linux**](https://learn.microsoft.com/en-us/windows/wsl/) (WSL) en tant qu'environnement de d√©veloppement complet.\n",
    "\n",
    "Vous pouvez :\n",
    "* D√©velopper sur le m√™me **syst√®me d'exploitation** que celui sur lequel vous d√©ployez votre application, ou utiliser du mat√©riel **plus puissant ou plus sp√©cialis√©**.\n",
    "* **S√©parer** votre environnement de d√©veloppement pour √©viter d'impacter la **configuration de votre machine** locale.\n",
    "* Faciliter la **prise en main** des nouveaux contributeurs et maintenir tout le monde dans un **environnement coh√©rent**.\n",
    "* Utiliser des outils ou des environnements d'ex√©cution qui **ne sont pas disponibles** sur votre syst√®me d'exploitation local, ou g√©rer **plusieurs versions** de ces outils.\n",
    "* D√©velopper vos applications d√©ploy√©es sur Linux en utilisant le **sous-syst√®me Windows pour Linux**.\n",
    "* Acc√©der √† un environnement de d√©veloppement **existant** √† partir de **plusieurs machines ou emplacements**.\n",
    "* D√©boguer une **application qui s'ex√©cute ailleurs**, comme sur le site d'un client ou dans le cloud.\n",
    "\n",
    "**Aucun code source** n'a besoin d'√™tre sur votre machine locale pour b√©n√©ficier de ces avantages. Chaque extension du [pack d'extensions Remote Development](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack) peut ex√©cuter des commandes et d'autres extensions directement dans un conteneur, dans WSL ou sur une machine distante, de sorte que tout se d√©roule comme si vous ex√©cutez localement.\n",
    "\n",
    "![](https://code.visualstudio.com/assets/docs/remote/remote-overview/architecture.png)\n",
    "\n",
    "## Premiers pas\n",
    "\n",
    "### Pack d'extensions Remote Development\n",
    "\n",
    "Le [pack d'extensions Remote Development](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack) comprend trois extensions. Consultez les articles suivants pour commencer avec chacune d'entre elles :\n",
    "* [Remote - SSH](https://code.visualstudio.com/docs/remote/ssh) - Connectez-vous √† n'importe quel emplacement en ouvrant des dossiers sur une machine distante/VM en utilisant SSH.\n",
    "* [Dev Containers](https://code.visualstudio.com/docs/devcontainers/containers) - Travaillez avec une cha√Æne d'outils s√©par√©e ou une application bas√©e sur des conteneurs √† l'int√©rieur (ou mont√©e dans) un conteneur.\n",
    "* [WSL](https://code.visualstudio.com/docs/remote/wsl) - Obtenez une exp√©rience de d√©veloppement bas√©e sur Linux dans le sous-syst√®me Windows pour Linux.\n",
    "\n",
    "Bien que la plupart des extensions de VS Code devraient fonctionner sans modification dans un environnement distant, les auteurs d'extensions peuvent en savoir plus sur le [support du d√©veloppement √† distance](https://code.visualstudio.com/api/advanced-topics/remote-extensions).\n",
    "\n",
    "### Tutoriels sur le \"√† distance\"\n",
    "\n",
    "Les tutoriels ci-dessous vous guideront pour ex√©cuter Visual Studio Code avec les extensions Remote Development.\n",
    "\n",
    "|Tutoriel|Description|\n",
    "|-|-|\n",
    "|[√Ä distance via SSH](https://code.visualstudio.com/docs/remote/ssh-tutorial)|Connectez-vous √† des machines distantes et virtuelles avec Visual Studio Code via SSH.|\n",
    "|[Travaillez dans WSL](https://code.visualstudio.com/docs/remote/wsl-tutorial)|Ex√©cutez Visual Studio Code dans le sous-syst√®me Windows pour Linux.|\n",
    "|[D√©veloppez dans des conteneurs](https://code.visualstudio.com/docs/devcontainers/tutorial)|Ex√©cutez Visual Studio Code dans un conteneur Docker.|\n",
    "|[GitHub Codespaces](https://docs.github.com/en/codespaces/developing-in-codespaces/using-github-codespaces-in-visual-studio-code)|Connectez-vous √† un espace de codes avec Visual Studio Code.|\n",
    "\n",
    "### GitHub Codespaces\n",
    "\n",
    "[GitHub Codespaces](https://code.visualstudio.com/docs/remote/codespaces) fournit des environnements de d√©veloppement √† distance g√©r√©s pour vous. Vous pouvez configurer et cr√©er un environnement de d√©veloppement h√©berg√© dans le cloud, qui est d√©marr√© et disponible lorsque vous en avez besoin.\n",
    "\n",
    "### Le serveur VS Code (version priv√©e de pr√©visualisation)\n",
    "\n",
    "[Le serveur VS Code](https://code.visualstudio.com/docs/remote/vscode-server) est un service en version priv√©e de pr√©visualisation que vous pouvez ex√©cuter sur une machine de d√©veloppement distante, comme votre PC de bureau ou une machine virtuelle (VM). Il vous permet de vous connecter en toute s√©curit√© √† cette machine distante de n'importe o√π via une URL vscode.dev, sans n√©cessiter de SSH.\n",
    "\n",
    "### Petit point avec Chat\n",
    "\n",
    "*Si tu n'as pas oubli√© tous nos √©changes ci-dessus pour mettre en place d'une architecture de d√©veloppement pyspark avec un client de d√©veloppement VS Code+Jupyter+PySpark/Windows en interaction avec un serveur de cluster Spark intall√© sur la m√™me machine, mais dans le sous-syst√®me Ubuntu/WSL, laquelle architecture s'est heurt√©e √† des probl√®mes de configuration et notamment de param√©trage du firewall, n'es-tu pas d'accord que finalement, ce dernier tuto que je t'ai pass√©, et qui concerne l'extension VS Code Remote Developement, r√©pond exactement √† mes besoins, et que cela m√©rite donc d'√™tre investi, c'est √† dire install√© et test√© sur la base des tutoriaux fournis ?*\n",
    "\n",
    "Oui, en fonction des difficult√©s que vous avez rencontr√©es lors de la configuration de l'architecture de d√©veloppement PySpark pr√©c√©dente, l'utilisation de l'extension VS Code Remote Development peut √™tre une solution int√©ressante pour r√©pondre √† vos besoins. Cette extension vous permet d'utiliser un environnement de d√©veloppement distant, tel que le sous-syst√®me Windows pour Linux (WSL), pour travailler avec des outils comme Jupyter, PySpark et VS Code.\n",
    "\n",
    "En utilisant cette approche, vous pouvez √©viter les probl√®mes de configuration du firewall et b√©n√©ficier d'un environnement de d√©veloppement plus coh√©rent et facile √† g√©rer. Les tutoriels fournis avec l'extension vous guideront pas √† pas dans la mise en place de cette configuration et vous permettront de tester son fonctionnement.\n",
    "\n",
    "Il est recommand√© d'installer et de tester l'extension en suivant les tutoriels fournis pour √©valuer si elle r√©pond effectivement √† vos besoins et facilite votre workflow de d√©veloppement PySpark avec VS Code et Jupyter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [**PySpark.SQL and Jupyter Notebooks on Visual Studio Code (Python kernel)**](https://blog.openthreatresearch.com/spark_jupyter_notebook_vscode)\n",
    "\n",
    "Dans cet article de blog, je vais partager les √©tapes que vous pouvez suivre pour ex√©cuter des commandes PySpark.SQL (Spark + Python) en utilisant un notebook Jupyter dans Visual Studio Code (VSCode). Lors du d√©veloppement de cet article, j'ai utilis√© un noyau Python sur un ordinateur Windows.\n",
    "\n",
    "## Pr√©requis\n",
    "\n",
    "Pour suivre les √©tapes de cet article, vous devez installer les √©l√©ments suivants sur votre ordinateur Windows :\n",
    "* `Java` : vous pouvez trouver les √©tapes d'installation ici.\n",
    "* `Visual Studio Code` : vous pouvez trouver les √©tapes d'installation ici.\n",
    "* `Extension Python pour Visual Studio Code` : vous pouvez trouver les √©tapes d'installation ici.\n",
    "* `Interpr√©teur Python` : vous pouvez trouver les √©tapes d'installation ici."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration d'une session PySpark.SQL\n",
    "\n",
    "### 1) Cr√©ation d'un notebook Jupyter dans VSCode\n",
    "\n",
    "Cr√©ez un notebook Jupyter en suivant les √©tapes d√©crites dans Mon premier notebook Jupyter sur Visual Studio Code (noyau Python).\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_01_spark_new_notebook.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Installation de la biblioth√®que PySpark Python\n",
    "\n",
    "√Ä l'aide de la premi√®re cellule de notre notebook, ex√©cutez le code suivant pour installer l'`API Python` pour Spark.\n",
    "\n",
    "```sh\n",
    "!pip install pyspark\n",
    "```\n",
    "\n",
    "Vous pouvez √©galement utiliser le terminal de VSCode pour installer PySpark. Les √©tapes pour installer une biblioth√®que Python √† l'aide d'un notebook Jupyter ou du terminal dans VSCode sont d√©crites ici.\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_02_spark_pyspark_installation.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.4.0\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: C:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show pyspark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Importation de la classe SparkSession\n",
    "\n",
    "Nous commen√ßons par importer la classe SparkSession du module PySpark SQL.\n",
    "\n",
    "La `SparkSession` est le point d'entr√©e principal pour les fonctionnalit√©s DataFrame et SQL. Un SparkSession peut √™tre utilis√© pour cr√©er un DataFrame, enregistrer un DataFrame en tant que table, ex√©cuter des requ√™tes SQL sur les tables, mettre en cache des tables, et m√™me lire des fichiers Parquet.\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_03_spark_pyspark_sql_spark_session.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Cr√©ation d'un SparkSession\n",
    "\n",
    "Pour cr√©er un SparkSession, nous utilisons la classe [`Builder`](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.SparkSession.Builder).\n",
    "\n",
    "Nous donnons un nom √† notre application Spark (`OTR`) et ajoutons une configuration caseSensitive.\n",
    "\n",
    "Nous attribuons le SparkSession √† une variable nomm√©e `spark`.\n",
    "\n",
    "```python\n",
    "spark = SparkSession.builder.appName(\"OTR\").config(\"spark.sql.caseSensitive\", \"True\").getOrCreate()\n",
    "```\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_04_spark_pyspark_sql_spark_session_builder.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession.Builder at 0x2020ffabd50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "otr = SparkSession.builder.appName(\"OTR\").config(\"spark.sql.caseSensitive\", \"True\")\n",
    "display(otr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x2020986c250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "display(SparkConf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLUSERSPROFILE: C:\\ProgramData\n",
      "APPDATA: C:\\Users\\franc\\AppData\\Roaming\n",
      "CHROME_CRASHPAD_PIPE_NAME: \\\\.\\pipe\\LOCAL\\crashpad_8680_AIHYRYQPRMSYWVQJ\n",
      "COMMONPROGRAMFILES: C:\\Program Files\\Common Files\n",
      "COMMONPROGRAMFILES(X86): C:\\Program Files (x86)\\Common Files\n",
      "COMMONPROGRAMW6432: C:\\Program Files\\Common Files\n",
      "COMPUTERNAME: BELLADONNA\n",
      "COMSPEC: C:\\WINDOWS\\system32\\cmd.exe\n",
      "DRIVERDATA: C:\\Windows\\System32\\Drivers\\DriverData\n",
      "EFC_8804: 1\n",
      "ELECTRON_RUN_AS_NODE: 1\n",
      "HOMEDRIVE: C:\n",
      "HOMEPATH: \\Users\\franc\n",
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-17\n",
      "JPY_INTERRUPT_EVENT: 2708\n",
      "LOCALAPPDATA: C:\\Users\\franc\\AppData\\Local\n",
      "LOGONSERVER: \\\\BELLADONNA\n",
      "NUMBER_OF_PROCESSORS: 12\n",
      "OMP_NUM_THREADS: 4\n",
      "ONEDRIVE: C:\\Users\\franc\\OneDrive\n",
      "ONEDRIVECONSUMER: C:\\Users\\franc\\OneDrive\n",
      "ORIGINAL_XDG_CURRENT_DESKTOP: undefined\n",
      "OS: Windows_NT\n",
      "PATH: c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311;c:\\Users\\franc\\AppData\\Roaming\\Python\\Python311\\Scripts;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\IDM Computer Solutions\\UltraEdit;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\GTK3-Runtime Win64\\bin;C:\\OSGeo4W\\bin;C:\\Program Files\\Graphviz\\bin;C:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\;C:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\;C:\\Users\\franc\\.pyenv\\pyenv-win\\bin;C:\\Users\\franc\\.pyenv\\pyenv-win\\shims;C:\\Users\\franc\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\franc\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Program Files\\JetBrains\\PyCharm Community Edition 2022.1.3\\bin;;C:\\Users\\franc\\AppData\\Local\\GitHubDesktop\\bin;C:\\Users\\franc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts;C:\\Users\\franc\\AppData\\Local\\Programs\\ffmpeg-5.1.2-essentials_build\\bin;C:\\Users\\franc\\AppData\\Local\\Programs\\MiKTeX\\miktex\\bin\\x64\\;;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\IDM Computer Solutions\\UltraEdit;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\GTK3-Runtime Win64\\bin;C:\\OSGeo4W\\bin;C:\\Program Files\\Graphviz\\bin;C:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\;C:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\;C:\\Users\\franc\\.pyenv\\pyenv-win\\bin;C:\\Users\\franc\\.pyenv\\pyenv-win\\shims;C:\\Users\\franc\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\franc\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Program Files\\JetBrains\\PyCharm Community Edition 2022.1.3\\bin;;C:\\Users\\franc\\AppData\\Local\\GitHubDesktop\\bin;C:\\Users\\franc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts;C:\\Users\\franc\\AppData\\Local\\Programs\\ffmpeg-5.1.2-essentials_build\\bin;C:\\Users\\franc\\AppData\\Local\\Programs\\MiKTeX\\miktex\\bin\\x64\\;\n",
      "PATHEXT: .COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\n",
      "PROCESSOR_ARCHITECTURE: AMD64\n",
      "PROCESSOR_IDENTIFIER: Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\n",
      "PROCESSOR_LEVEL: 6\n",
      "PROCESSOR_REVISION: 9e0a\n",
      "PROGRAMDATA: C:\\ProgramData\n",
      "PROGRAMFILES: C:\\Program Files\n",
      "PROGRAMFILES(X86): C:\\Program Files (x86)\n",
      "PROGRAMW6432: C:\\Program Files\n",
      "PSMODULEPATH: C:\\Program Files\\WindowsPowerShell\\Modules;C:\\WINDOWS\\system32\\WindowsPowerShell\\v1.0\\Modules\n",
      "PUBLIC: C:\\Users\\Public\n",
      "PYCHARM COMMUNITY EDITION: C:\\Program Files\\JetBrains\\PyCharm Community Edition 2022.1.3\\bin;\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING: 1\n",
      "PYENV: C:\\Users\\franc\\.pyenv\\pyenv-win\\\n",
      "PYENV_HOME: C:\\Users\\franc\\.pyenv\\pyenv-win\\\n",
      "PYENV_ROOT: C:\\Users\\franc\\.pyenv\\pyenv-win\\\n",
      "PYSPARK_SUBMIT_ARGS: --master local[*] pyspark-shell\n",
      "PYTHONIOENCODING: utf-8\n",
      "PYTHONUNBUFFERED: 1\n",
      "SESSIONNAME: Console\n",
      "SPARK_HOME: C:\\Program Files\\Spark\n",
      "SYSTEMDRIVE: C:\n",
      "SYSTEMROOT: C:\\WINDOWS\n",
      "TEMP: C:\\Users\\franc\\AppData\\Local\\Temp\n",
      "TMP: C:\\Users\\franc\\AppData\\Local\\Temp\n",
      "USERDOMAIN: BELLADONNA\n",
      "USERDOMAIN_ROAMINGPROFILE: BELLADONNA\n",
      "USERNAME: franc\n",
      "USERPROFILE: C:\\Users\\franc\n",
      "VSCODE_AMD_ENTRYPOINT: vs/workbench/api/node/extensionHostProcess\n",
      "VSCODE_CODE_CACHE_PATH: C:\\Users\\franc\\AppData\\Roaming\\Code\\CachedData\\b3e4e68a0bc097f0ae7907b217c1119af9e03435\n",
      "VSCODE_CRASH_REPORTER_PROCESS_TYPE: extensionHost\n",
      "VSCODE_CWD: C:\\Users\\franc\\AppData\\Local\\Programs\\Microsoft VS Code\n",
      "VSCODE_HANDLES_UNCAUGHT_ERRORS: true\n",
      "VSCODE_IPC_HOOK: \\\\.\\pipe\\528ce89b3964f0be503a218713263ba8-1.78.2-main-sock\n",
      "VSCODE_L10N_BUNDLE_LOCATION: file:///c%3A/Users/franc/.vscode/extensions/ms-ceintl.vscode-language-pack-fr-1.78.2023051009/translations/extensions/vscode.json-language-features.i18n.json\n",
      "VSCODE_NLS_CONFIG: {\"locale\":\"fr\",\"osLocale\":\"fr-fr\",\"availableLanguages\":{\"*\":\"fr\"},\"_languagePackId\":\"464e33a418b005a7ef0102b15075ed48.fr\",\"_translationsConfigFile\":\"C:\\\\Users\\\\franc\\\\AppData\\\\Roaming\\\\Code\\\\clp\\\\464e33a418b005a7ef0102b15075ed48.fr\\\\tcf.json\",\"_cacheRoot\":\"C:\\\\Users\\\\franc\\\\AppData\\\\Roaming\\\\Code\\\\clp\\\\464e33a418b005a7ef0102b15075ed48.fr\",\"_resolvedLanguagePackCoreLocation\":\"C:\\\\Users\\\\franc\\\\AppData\\\\Roaming\\\\Code\\\\clp\\\\464e33a418b005a7ef0102b15075ed48.fr\\\\b3e4e68a0bc097f0ae7907b217c1119af9e03435\",\"_corruptedFile\":\"C:\\\\Users\\\\franc\\\\AppData\\\\Roaming\\\\Code\\\\clp\\\\464e33a418b005a7ef0102b15075ed48.fr\\\\corrupted.info\",\"_languagePackSupport\":true}\n",
      "VSCODE_PID: 8680\n",
      "WINDIR: C:\\WINDOWS\n",
      "PYDEVD_USE_FRAME_EVAL: NO\n",
      "TERM: xterm-color\n",
      "CLICOLOR: 1\n",
      "FORCE_COLOR: 1\n",
      "CLICOLOR_FORCE: 1\n",
      "PAGER: cat\n",
      "GIT_PAGER: cat\n",
      "MPLBACKEND: module://matplotlib_inline.backend_inline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# R√©cup√©rer toutes les variables d'environnement\n",
    "env_vars = os.environ\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master local[*] pyspark-shell\"\n",
    "\n",
    "# Afficher les valeurs des variables d'environnement\n",
    "for var, value in env_vars.items():\n",
    "    print(f'{var}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.sql.caseSensitive', 'True')\n",
      "spark.sql.caseSensitive=True\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.sql.caseSensitive\", \"True\")\n",
    "\n",
    "# Obtention des param√®tres de configuration\n",
    "configurations = conf.getAll()\n",
    "\n",
    "# Affichage des param√®tres de configuration\n",
    "for config in configurations:\n",
    "    print(config)\n",
    "\n",
    "\n",
    "# Affichage des valeurs par d√©faut et des param√®tres d√©finis\n",
    "print(conf.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mappName(\u001b[39m\"\u001b[39m\u001b[39mOTR\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mconfig(\u001b[39m\"\u001b[39m\u001b[39mspark.sql.caseSensitive\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTrue\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mgetOrCreate()\n\u001b[0;32m      2\u001b[0m display(spark)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"OTR\").config(\"spark.sql.caseSensitive\", \"True\").getOrCreate()\n",
    "display(spark)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5) V√©rification du SparkSession\n",
    "\n",
    "Une fois que le SparkSession est construit, nous pouvons ex√©cuter la variable spark pour la v√©rification.\n",
    "\n",
    "```python\n",
    "spark\n",
    "```\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_05_spark_pyspark_sql_spark_session_info.jpg)\n",
    "\n",
    "## Ex√©cution de plus de commandes Spark\n",
    "\n",
    "Pour la derni√®re section de cet article de blog, je partage trois autres commandes de base qui sont tr√®s utiles lors de l'ex√©cution de t√¢ches avec Spark :\n",
    "* Cr√©ation d'un DataFrame Spark en utilisant la m√©thode `read.json`.\n",
    "* Cr√©ation d'une vue temporaire d'un DataFrame Spark en utilisant la m√©thode `createOrReplaceTempView`.\n",
    "* Ex√©cution d'une requ√™te de type SQL en utilisant la m√©thode `sql`.\n",
    "\n",
    "### 0) Importation d'un jeu de donn√©es Mordor\n",
    "\n",
    "Afin de vous montrer ces exemples, nous avons besoin de donn√©es. Par cons√©quent, j'utiliserai un jeu de donn√©es Mordor qui contient des journaux d'√©v√©nements de s√©curit√© pour l'ex√©cution d'une preuve de concept publique d'exploitation de vuln√©rabilit√©s Exchange (vuln√©rabilit√© CVE-2021-26855 de falsification de requ√™te c√¥t√© serveur (SSRF)).\n",
    "\n",
    "T√©l√©chargez le jeu de donn√©es Mordor (fichier json) en suivant les √©tapes d√©crites dans l'article [Importing a Mordor Dataset with Jupyter Notebooks on Visual Studio Code (noyau Python)](https://blog.openthreatresearch.com/importing-mordor-dataset-jupyter-notebook-vscode).\n",
    "\n",
    "```python\n",
    "# Importing libraries\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "# Downloading and Extracting Json File\n",
    "url = 'https://raw.githubusercontent.com/OTRF/mordor/master/datasets/small/windows/persistence/host/proxylogon_ssrf_rce_poc.zip'\n",
    "zipFileRequest = requests.get(url)\n",
    "zipFile = ZipFile(BytesIO(zipFileRequest.content))\n",
    "jsonFilePath = zipFile.extract(zipFile.namelist()[0])\n",
    "jsonFilePath\n",
    "```\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_06_spark_mordor_file.jpg)\n",
    "\n",
    "\n",
    "### 1) Cr√©ation d'un DataFrame Spark\n",
    "\n",
    "Pour cr√©er un DataFrame Spark √† partir d'un fichier `JSON`, nous utilisons la m√©thode `read.json`.\n",
    "\n",
    "Nous utilisons la variable `jsonFilePath` de la section pr√©c√©dente qui contient le chemin ou le r√©pertoire o√π le fichier `JSON` a √©t√© stock√©.\n",
    "\n",
    "```python\n",
    "# Creating a Spark Dataframe\n",
    "df = spark.read.json(jsonFilePath)\n",
    "# Validating Type of Output\n",
    "type(df)\n",
    "```\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_07_spark_dataframe.jpg)\n",
    "\n",
    "### 2) Cr√©ation d'une vue temporaire d'un DataFrame Spark\n",
    "\n",
    "Pour cr√©er une vue temporaire d'un DataFrame Spark, nous utilisons la m√©thode `createOrReplaceTempView`.\n",
    "\n",
    "Nous pouvons utiliser cette vue temporaire d'un DataFrame Spark comme une table `SQL` et d√©finir des requ√™tes de type SQL pour analyser nos donn√©es.\n",
    "\n",
    "Nous utiliserons le DataFrame Spark `df` d√©fini dans la section pr√©c√©dente. Le nom que nous utilisons pour notre vue temporaire est `mordorTable`.\n",
    "\n",
    "```python\n",
    "df.createOrReplaceTempView('mordorTable')\n",
    "```\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_08_spark_dataframe_temporary_view.jpg)\n",
    "\n",
    "### 3) Ex√©cution d'une requ√™te de type SQL\n",
    "\n",
    "Pour ex√©cuter une requ√™te de type SQL, nous utilisons la m√©thode `sql`.\n",
    "\n",
    "En utilisant `mordorTable` comme r√©f√©rence, nous ex√©cuterons le code suivant pour r√©sumer les journaux d'√©v√©nements de s√©curit√© fournis par le jeu de donn√©es.\n",
    "\n",
    "Nous effectuons une op√©ration de comptage par empilement sur les donn√©es, et nous regroupons le r√©sultat par Nom de l'h√¥te (`Hostname`), Canal (`Channel`) et ID d'√©v√©nement (`EventID`).\n",
    "\n",
    "```python\n",
    "df = spark.sql(\n",
    "'''\n",
    "SELECT Hostname,Channel,EventID, Count(*) as count\n",
    "FROM mordorTable\n",
    "GROUP BY Hostname,Channel,EventID\n",
    "ORDER BY count DESC\n",
    "'''\n",
    ")\n",
    "df.show(truncate=False)\n",
    "```\n",
    "\n",
    "![](https://blog.openthreatresearch.com/assets/images/blog/2021-01-02_09_spark_dataframe_temporary_view_sql_query.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
