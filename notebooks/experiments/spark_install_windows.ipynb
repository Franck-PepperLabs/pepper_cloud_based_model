{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [**D√©marrer avec PySpark sur Windows**](http://deelesh.github.io/pyspark-windows.html)\n",
    "\n",
    "J'ai d√©cid√© d'apprendre √† travailler avec les big data et je suis tomb√© sur [Apache Spark](https://spark.apache.org/). Bien que j'aie entendu parler d'[Apache Hadoop](https://hadoop.apache.org/), pour utiliser Hadoop avec les big data, je devais √©crire du code en Java, ce que je n'√©tais pas vraiment impatient de faire car j'adore √©crire du code en Python. Spark prend en charge une API de programmation Python appel√©e PySpark qui est activement entretenue et cela a √©t√© suffisant pour me convaincre de commencer √† apprendre PySpark pour travailler avec les big data.\n",
    "\n",
    "Dans cet article, je d√©cris comment j'ai commenc√© avec PySpark sur Windows. Mon ordinateur portable fonctionne sous Windows 10. Les captures d'√©cran sont donc sp√©cifiques √† Windows 10. Je suppose √©galement que vous √™tes √† l'aise avec l'utilisation de l'invite de commandes sur Windows. Vous n'avez pas besoin d'√™tre un expert, mais vous devez savoir comment ouvrir une invite de commandes et ex√©cuter des commandes telles que celles qui vous permettent de vous d√©placer dans le syst√®me de fichiers de votre ordinateur. Au cas o√π vous auriez besoin d'un rappel, une br√®ve introduction pourrait √™tre utile.\n",
    "\n",
    "*Il arrive souvent que de nombreux projets open source n'aient pas un bon support sur Windows. J'ai donc d√ª d'abord v√©rifier si Spark et PySpark fonctionneraient bien sur Windows. La [documentation ](https://spark.apache.org/docs/latest/#downloading) officielle de Spark mentionne effectivement la prise en charge de Windows.*\n",
    "\n",
    "# Installation des pr√©requis\n",
    "\n",
    "PySpark n√©cessite Java version 7 ou ult√©rieure et Python version 2.6 ou ult√©rieure. Commen√ßons par v√©rifier s'ils sont d√©j√† install√©s ou les installer et nous assurer que PySpark peut fonctionner avec ces deux composants.\n",
    "\n",
    "\n",
    "## Java\n",
    "\n",
    "Java est utilis√© par de nombreux autres logiciels. Il est donc tout √† fait possible qu'une version requise (dans notre cas, la version 7 ou ult√©rieure) soit d√©j√† disponible sur votre ordinateur. Pour v√©rifier si Java est disponible et trouver sa version, ouvrez une invite de commandes et saisissez la commande suivante.\n",
    "\n",
    "```sh\n",
    "java -version\n",
    "```\n",
    "\n",
    "Si Java est install√© et configur√© pour fonctionner √† partir d'une invite de commandes, l'ex√©cution de la commande ci-dessus affichera les informations sur la version de Java dans la console. Par exemple, voici le r√©sultat que j'ai obtenu sur mon ordinateur portable :\n",
    "\n",
    "```sh\n",
    "java version \"1.8.0_92\"\n",
    "Java(TM) SE Runtime Environment (build 1.8.0_92-b14)\n",
    "Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode)\n",
    "```\n",
    "\n",
    "Si vous obtenez plut√¥t un message tel que :\n",
    "\n",
    "```sh\n",
    "'java' n'est pas reconnu en tant que commande interne ou externe, un programme ex√©cutable ou un fichier de commandes.\n",
    "```\n",
    "\n",
    "Cela signifie que vous devez installer Java. Pour ce faire :\n",
    "1. Rendez-vous sur la page de t√©l√©chargement de Java. Si le lien de t√©l√©chargement a chang√©, recherchez `Java SE Runtime Environment` sur Internet et vous devriez pouvoir trouver la page de t√©l√©chargement.\n",
    "2. Cliquez sur le bouton *T√©l√©charger* sous *JRE*.\n",
    "3. Acceptez l'accord de licence et t√©l√©chargez la derni√®re version de l'installateur de Java SE Runtime Environment. Je vous sugg√®re de prendre l'ex√©cutable pour Windows x64 (tel que `jre-8u92-windows-x64.exe`), sauf si vous utilisez une version 32 bits de Windows, auquel cas vous devez obtenir la version hors ligne Windows x86.\n",
    "4. Ex√©cutez l'installateur.\n",
    "\n",
    "Une fois l'installation termin√©e, fermez l'invite de commandes si elle √©tait d√©j√† ouverte, ouvrez-la √† nouveau et v√©rifiez si vous pouvez ex√©cuter avec succ√®s la commande `java -version`.\n",
    "\n",
    "Effectuer l'√©quivalent de `wget` avec `PowerShell` (lanc√© en mode administrateur) :\n",
    "\n",
    "```sh\n",
    "Invoke-WebRequest -Uri \"<URL_du_programme_d'installation>\" -OutFile \"<chemin_du_fichier_de_sortie>\"\n",
    "Start-Process -FilePath \"<chemin_du_fichier_de_sortie>\"\n",
    "```\n",
    "\n",
    "üìå Ici, point d'arr√™t : quelle version de Java ?\n",
    "\n",
    "https://spark.apache.org/docs/latest/\n",
    "\n",
    "*Spark runs on both Windows and UNIX-like systems (e.g. Linux, Mac OS), and it should run on any platform that runs a supported version of Java. This should include JVMs on x86_64 and ARM64. It‚Äôs easy to run locally on one machine ‚Äî all you need is to have java installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation.*\n",
    "\n",
    "*Spark runs on Java 8/11/17, Scala 2.12/2.13, Python 3.7+, and R 3.5+. Python 3.7 support is deprecated as of Spark 3.4.0. Java 8 prior to version 8u362 support is deprecated as of Spark 3.4.0. When using the Scala API, it is necessary for applications to use the same version of Scala that Spark was compiled for. For example, when using Scala 2.13, use Spark compiled for 2.13, and compile code/applications for Scala 2.13 as well.*\n",
    "\n",
    "*For Java 11, setting -Dio.netty.tryReflectionSetAccessible=true is required for the Apache Arrow library. This prevents the java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.(long, int) not available error when Apache Arrow uses Netty internally.*\n",
    "\n",
    "Spark prend en charge les syst√®mes Windows et UNIX-like (Linux, Mac OS) et devrait fonctionner sur n'importe quelle plateforme ex√©cutant une version prise en charge de Java, y compris les JVM sur x86_64 et ARM64.\n",
    "\n",
    "Spark est compatible avec Java 8/11/17, Scala 2.12/2.13, Python 3.7+ et R 3.5+. √Ä partir de Spark 3.4.0, la prise en charge de Python 3.7 est d√©pr√©ci√©e, et la prise en charge de Java 8 ant√©rieur √† la version 8u362 est √©galement d√©pr√©ci√©e.\n",
    "\n",
    "Lors de l'utilisation de l'API Scala, il est n√©cessaire que les applications utilisent la m√™me version de Scala pour laquelle Spark a √©t√© compil√©. Par exemple, si vous utilisez Scala 2.13, utilisez une version de Spark compil√©e pour Scala 2.13 et compilez √©galement le code/applications pour Scala 2.13.\n",
    "\n",
    "Pour Java 11, il est n√©cessaire de d√©finir -Dio.netty.tryReflectionSetAccessible=true pour la biblioth√®que Apache Arrow. Cela √©vite l'erreur \"java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.(long, int) not available\" lorsque Apache Arrow utilise Netty en interne.\n",
    "\n",
    "Selon les informations fournies, Spark prend en charge Java 8/11/17. Java 17 est la version la plus r√©cente et offre des fonctionnalit√©s et des am√©liorations par rapport aux versions ant√©rieures. Utiliser la derni√®re version de Java peut nous permettre de b√©n√©ficier des derni√®res am√©liorations en termes de performances, de s√©curit√© et de fonctionnalit√©s.\n",
    "\n",
    "PowerShell (as admin):\n",
    "\n",
    "```sh\n",
    "Invoke-WebRequest -Uri \"https://download.oracle.com/java/17/latest/jdk-17_windows-x64_bin.exe\" -OutFile \"C:\\Temp\\jdk-17_windows-x64_bin.exe\"\n",
    "Start-Process -FilePath \"C:\\Temp\\jdk-17_windows-x64_bin.exe\"\n",
    "Remove-Item -Path \"C:\\Temp\\jdk-17_windows-x64_bin.exe\"\n",
    "```\n",
    "\n",
    "PowerShell n'a pas d'√©quivalent de `source ~/.bashrc`, il faut donc relancer un terminal pour visualiser l'effet de mise √† jour d'environnement :\n",
    "\n",
    "```sh\n",
    "java -version\n",
    "java version \"17.0.7\" 2023-04-18 LTS\n",
    "Java(TM) SE Runtime Environment (build 17.0.7+8-LTS-224)\n",
    "Java HotSpot(TM) 64-Bit Server VM (build 17.0.7+8-LTS-224, mixed mode, sharing)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python\n",
    "\n",
    "Python est utilis√© par de nombreux autres logiciels. Il est donc tout √† fait possible qu'une version requise (dans notre cas, la version 2.6 ou ult√©rieure) soit d√©j√† install√©e sur votre ordinateur. Pour v√©rifier si Python est install√© et trouver sa version, ouvrez une invite de commandes et tapez la commande suivante :\n",
    "\n",
    "```sh\n",
    "python --version\n",
    "```\n",
    "\n",
    "Si Python est install√© et configur√© pour fonctionner √† partir d'une invite de commandes, l'ex√©cution de la commande ci-dessus affichera les informations sur la version de Python dans la console. Par exemple, voici la sortie que j'ai obtenue sur mon ordinateur portable :\n",
    "\n",
    "```sh\n",
    "Python 2.7.10\n",
    "```\n",
    "\n",
    "Si vous obtenez plut√¥t un message tel que :\n",
    "\n",
    "```sh\n",
    "'python' n'est pas reconnu en tant que commande interne ou externe, un programme ex√©cutable ou un fichier de commandes.\n",
    "```\n",
    "\n",
    "Cela signifie que vous devez installer Python. Pour ce faire :\n",
    "1. Rendez-vous sur la page de t√©l√©chargement de Python.\n",
    "2. Cliquez sur le lien Latest Python 2 Release.\n",
    "3. T√©l√©chargez le fichier d'installation MSI Windows x86-64. Si vous utilisez une version 32 bits de Windows, t√©l√©chargez le fichier d'installation MSI Windows x86.\n",
    "4. Lorsque vous ex√©cutez l'installateur, assurez-vous que l'option Ajouter `python.exe` au chemin d'acc√®s est s√©lectionn√©e dans la section **Personnaliser Python**. Si cette option n'est pas s√©lectionn√©e, certaines des utilitaires PySpark telles que `pyspark` et `spark-submit` pourraient ne pas fonctionner.\n",
    "\n",
    "Ajouter `python.exe` au chemin d'acc√®s lors de l'installation de Python sur Windows\n",
    "\n",
    "![](http://deelesh.github.io/images/python-install.png)\n",
    "\n",
    "Apr√®s l'installation, fermez l'invite de commandes si elle √©tait d√©j√† ouverte, puis ouvrez-la √† nouveau et v√©rifiez si vous pouvez ex√©cuter avec succ√®s la commande python --version."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation d'Apache Spark\n",
    "\n",
    "1. Rendez-vous sur la page de t√©l√©chargement de Spark.\n",
    "2. Pour \"Choisir une version de Spark\", s√©lectionnez la derni√®re version stable de Spark.\n",
    "3. Pour \"Choisir un type de package\", s√©lectionnez une version pr√©construite pour la derni√®re version de Hadoop, comme \"Pr√©construit pour Hadoop 2.6\".\n",
    "4. Pour \"Choisir un type de t√©l√©chargement\", s√©lectionnez \"T√©l√©chargement direct\".\n",
    "5. Cliquez sur le lien √† c√¥t√© de \"T√©l√©charger Spark\" pour t√©l√©charger un fichier tarball compress√© au format .tgz, tel que `spark-1.6.2-bin-hadoop2.6.tgz`.\n",
    "6. Pour installer Apache Spark, il n'est pas n√©cessaire d'ex√©cuter un programme d'installation. Vous pouvez extraire les fichiers du tarball t√©l√©charg√© dans n'importe quel dossier de votre choix en utilisant l'outil 7Zip. *Assurez-vous que le chemin d'acc√®s du dossier et le nom du dossier contenant les fichiers Spark ne contiennent pas d'espaces.*\n",
    "\n",
    "Dans mon cas, j'ai cr√©√© le dossier `spark` sur mon lecteur C et j'ai extrait le tarball compress√© dans le dossier `spark-1.6.2-bin-hadoop2.6`. Ainsi, tous les fichiers Spark se trouvent dans le dossier `C:\\spark\\spark-1.6.2-bin-hadoop2.6`. √Ä partir de maintenant, je ferai r√©f√©rence √† ce dossier en tant que `SPARK_HOME` dans ce post.\n",
    "\n",
    "Pour tester si votre installation a r√©ussi, ouvrez une fen√™tre d'invite de commandes, allez dans le r√©pertoire `SPARK_HOME` et tapez `bin\\pyspark`. Cela devrait d√©marrer l'interpr√©teur PySpark qui peut √™tre utilis√© pour travailler avec Spark de mani√®re interactive. J'ai obtenu les messages suivants dans la console apr√®s avoir ex√©cut√© la commande `bin\\pyspark`.\n",
    "\n",
    "### üìå Ma version de ces instructions :\n",
    "\n",
    "```sh\n",
    "cd ~\n",
    "Invoke-WebRequest -Uri \"https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\" -OutFile \"spark-3.4.0-bin-hadoop3.tgz\"\n",
    "Expand-Archive -Path \"spark-3.4.0-bin-hadoop3.tgz\" -DestinationPath \"C:\\opt\\spark\"\n",
    "Remove-Item \"spark-3.4.0-bin-hadoop3.tgz\"\n",
    "```\n",
    "\n",
    "```sh\n",
    "cd ~\n",
    "Invoke-WebRequest -Uri \"https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\" -OutFile \"spark-3.4.0-bin-hadoop3.tgz\"\n",
    "wsl tar -xzf \"/mnt/c/Users/franc/spark-3.4.0-bin-hadoop3.tgz\" -C \"/mnt/c/Program Files\"\n",
    "Move-Item -Path \"C:\\Program Filesspark-3.4.0-bin-hadoop3\" -Destination \"C:\\Program Files\\Spark\"\n",
    "Remove-Item \"spark-3.4.0-bin-hadoop3.tgz\"\n",
    "```\n",
    "\n",
    "NB> `Expand-Archive` ne prend par en charge les .tgz, seulement les .zip.\n",
    "```sh\n",
    "xxx Expand-Archive -Path \"spark-3.4.0-bin-hadoop3.tgz\" -DestinationPath \"C:\\Program Files\\Spark\"\n",
    "```\n",
    "\n",
    "Pour √©viter d'invoquer `wsl tar xzf`, une alternative serait d'installer et utiliser 7-Zip.\n",
    "\n",
    "```sh\n",
    "& \"C:\\Program Files\\7-Zip\\7z.exe\" x spark-3.4.0-bin-hadoop3.tgz -o\"C:\\Program Files\\Spark\"\n",
    "```\n",
    "\n",
    "D√©finition de `SPARK_HOME`\n",
    "\n",
    "Variable d'environnement syst√®me :\n",
    "\n",
    "```sh\n",
    "[Environment]::SetEnvironmentVariable(\"SPARK_HOME\", \"C:\\Program Files\\Spark\", \"Machine\")\n",
    "```\n",
    "\n",
    "```sh\n",
    "cd $env:SPARK_HOME\n",
    "bin\\pyspark\n",
    "Python 3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)] on win32\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "23/05/11 12:10:43 WARN Shell: Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "23/05/11 12:10:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.4.0\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022 19:58:39)\n",
    "Spark context Web UI available at http://Belladonna:4040\n",
    "Spark context available as 'sc' (master = local[*], app id = local-1683799845699).\n",
    "SparkSession available as 'spark'.\n",
    ">>>\n",
    "```\n",
    "\n",
    "Les messages d'avertissement concernant `winutils.exe` et la biblioth√®que native-hadoop sont normaux sur Windows. Ces avertissements indiquent que certaines fonctionnalit√©s sp√©cifiques √† Hadoop ne sont pas disponibles, ce qui est attendu si vous n'avez pas configur√© Hadoop sur le syst√®me.\n",
    "\n",
    "Nous sommes √† pr√©sent dans l'invite Python interactive de PySpark, et nous pouvons commencer √† utiliser les fonctionnalit√©s de Spark.\n",
    "\n",
    "L'URL \"http://Belladonna:4040\" donne acc√®s √† l'interface utilisateur Web de Spark qui permet de surveiller et diagnostiquer les travaux Spark.\n",
    "\n",
    "La variable `sc` peut √™tre utilis√©e pour interagir avec le contexte Spark, et la variable `spark` pour interagir avec la session Spark.\n",
    "\n",
    "Par exemple, nous pouvons √† pr√©sent ex√©cuter des op√©rations sur des RDD (Resilient Distributed Datasets) √† l'aide de `sc`, ou ex√©cuter des op√©rations sur des DataFrames √† l'aide de `spark`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version IPython de PySpark\n",
    "\n",
    "Pour lancer la version IPython de PySpark, il faut utiliser la commande `pyspark` avec l'option `--master local[*] --conf spark.ui.showConsoleProgress=true --driver-memory <m√©moire_allou√©e>` depuis l'invite de commandes.\n",
    "\n",
    "Voici les √©tapes √† suivre :\n",
    "1. S'assurer d'√™tre dans l'environnement o√π vous a √©t√© install√© PySpark.\n",
    "2. Ouvrir une nouvelle fen√™tre de l'invite de commandes.\n",
    "3. Taper la commande suivante :\n",
    "\n",
    "```sh\n",
    "pyspark --master local[*] --conf spark.ui.showConsoleProgress=true --driver-memory <m√©moire_allou√©e>\n",
    "```\n",
    "\n",
    "O√π <m√©moire_allou√©e> par la quantit√© de m√©moire que vous souhaitez allouer au pilote Spark, par exemple 4g pour 4 Go. Cette valeur d√©pend de la configuration de votre syst√®me.\n",
    "\n",
    "4. Appuyez sur Entr√©e pour ex√©cuter la commande.\n",
    "\n",
    "Cela lancera PySpark avec l'interface IPython, qui offre une meilleure interactivit√© et des fonctionnalit√©s suppl√©mentaires par rapport √† l'invite de commande Python standard.\n",
    "\n",
    "<mark>\n",
    "MOUAIS : √ßa n'a pas l'air d'avoir lanc√© IPython..\n",
    "ce serait plut√¥t `pyspark --py-files /chemin/vers/le/module/ipython.zip`\n",
    "faire quelues recherches et construire un test.\n",
    "Le test de base, c'est de pouvoir faire display(sc)\n",
    "</mark>\n",
    "\n",
    "```sh\n",
    "pyspark --master local[*] --conf spark.ui.showConsoleProgress=true --driver-memory 4g\n",
    "```\n",
    "\n",
    "A ce stade, on n'observe pas beaucoup de diff√©rence, mais voici les b√©n√©fices attendus avec IPython :\n",
    "* **Syntaxe am√©lior√©e** : IPython offre une compl√©tion automatique avanc√©e, ce qui facilite la saisie du code et r√©duit les erreurs de syntaxe.\n",
    "* **Acc√®s √† l'historique des commandes** : Vous pouvez acc√©der √† l'historique des commandes que vous avez saisies pr√©c√©demment, naviguer dans l'historique et r√©ex√©cuter des commandes.\n",
    "* **Affichage riche** : IPython fournit un affichage riche pour les r√©sultats des commandes, notamment la coloration syntaxique, la mise en forme de tableaux, les graphiques int√©gr√©s, etc.\n",
    "* **Fonctionnalit√©s magiques** : IPython propose des \"fonctionnalit√©s magiques\" qui permettent d'ex√©cuter des commandes sp√©ciales. Par exemple, %timeit pour mesurer le temps d'ex√©cution d'une commande, %debug pour le d√©bogage, %matplotlib pour activer l'int√©gration de Matplotlib, etc.\n",
    "* **Extensions et int√©grations tierces** : IPython dispose d'une large communaut√© d'utilisateurs et de d√©veloppeurs qui ont cr√©√© des extensions et des int√©grations avec d'autres biblioth√®ques populaires telles que Pandas, NumPy, Matplotlib, etc.\n",
    "\n",
    "Ces fonctionnalit√©s peuvent √™tre particuli√®rement utiles pour travailler sur des t√¢ches plus complexes, des visualisations de donn√©es, des exp√©rimentations interactives, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suite du tuto\n",
    "\n",
    "Le dernier message donne une indication sur la fa√ßon de travailler avec Spark dans l'invite de commande PySpark en utilisant les noms `sc` ou `sqlContext`. Par exemple, en tapant `sc.version` dans l'invite de commande, la version de Spark devrait s'afficher. Vous pouvez quitter l'invite de commande PySpark de la m√™me mani√®re que vous quittez n'importe quelle invite de commande Python en tapant `exit()`. *L'invite de commande PySpark affiche quelques messages lors de la sortie. Vous devez donc appuyer sur Entr√©e pour revenir √† l'invite de commande.*\n",
    "\n",
    "## Configuration de l'installation de Spark\n",
    "\n",
    "Le d√©marrage de l'invite de commande PySpark g√©n√®re de nombreux messages de type `INFO`, `ERROR` et `WARN`. Dans cette section, nous verrons comment supprimer ces messages.\n",
    "\n",
    "Par d√©faut, l'installation de Spark sous Windows n'inclut pas l'utilitaire `winutils.exe` utilis√© par Spark. Si vous ne pr√©cisez pas √† votre installation de Spark o√π trouver `winutils.exe`, vous verrez des messages d'erreur lors de l'ex√©cution de l'invite de commande PySpark, tels que :\n",
    "\n",
    "```sh\n",
    "ERROR Shell: Impossible de localiser l'ex√©cutable winutils dans le chemin binaire de Hadoop\n",
    "java.io.IOException: Impossible de localiser l'ex√©cutable null\\bin\\winutils.exe dans les binaires de Hadoop.\n",
    "```\n",
    "\n",
    "Ce message d'erreur n'emp√™che pas l'invite de commande PySpark de d√©marrer. Cependant, si vous essayez d'ex√©cuter un script Python autonome √† l'aide de l'utilitaire `bin\\spark-submit`, vous obtiendrez une erreur. Par exemple, essayez d'ex√©cuter le script `wordcount.py` du dossier `examples` dans l'invite de commande lorsque vous √™tes dans le r√©pertoire `SPARK_HOME`.\n",
    "\n",
    "```sh\n",
    "bin\\spark-submit examples\\src\\main\\python\\wordcount.py README.md\n",
    "```\n",
    "\n",
    "ce qui produit l'erreur suivante qui indique √©galement l'absence de `winutils.exe`.\n",
    "\n",
    "```sh\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "16/07/09 16:23:27 INFO SparkContext: Running Spark version 1.6.2\n",
    "16/07/09 16:23:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "16/07/09 16:23:27 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path\n",
    "java.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\n",
    "        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:355)\n",
    "        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:370)\n",
    "...\n",
    "```\n",
    "\n",
    "Alors avec la version 3.4.0, on a pas vraiment d'erreur, cela parvient √† s'ex√©cuter jusqu'au bout :\n",
    "\n",
    "```sh\n",
    "23/05/11 12:40:18 WARN Shell: Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
    "23/05/11 12:40:20 INFO SparkContext: Running Spark version 3.4.0\n",
    "23/05/11 12:40:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "23/05/11 12:40:21 INFO ResourceUtils: ==============================================================\n",
    "23/05/11 12:40:21 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
    "23/05/11 12:40:21 INFO ResourceUtils: ==============================================================\n",
    "23/05/11 12:40:21 INFO SparkContext: Submitted application: PythonWordCount\n",
    "23/05/11 12:40:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
    "23/05/11 12:40:21 INFO ResourceProfile: Limiting resource is cpu\n",
    "23/05/11 12:40:21 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
    "23/05/11 12:40:21 INFO SecurityManager: Changing view acls to: franc\n",
    "23/05/11 12:40:21 INFO SecurityManager: Changing modify acls to: franc\n",
    "23/05/11 12:40:21 INFO SecurityManager: Changing view acls groups to:\n",
    "23/05/11 12:40:21 INFO SecurityManager: Changing modify acls groups to:\n",
    "23/05/11 12:40:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: franc; groups with view permissions: EMPTY; users with modify permissions: franc; groups with modify permissions: EMPTY\n",
    "23/05/11 12:40:21 INFO Utils: Successfully started service 'sparkDriver' on port 53367.\n",
    "...\n",
    "```\n",
    "\n",
    "Dans la version 3.4.0 de Spark, il n'est plus n√©cessaire d'installer explicitement `winutils.exe` sur Windows. L'avertissement mentionnant winutils dans les messages n'emp√™che pas le bon fonctionnement de Spark.\n",
    "\n",
    "La n√©cessit√© d'installer `winutils.exe` d√©pend de la version de Spark utilis√©e et de la configuration syst√®me. Dans certaines versions plus anciennes de Spark, winutils.exe √©tait requis pour des fonctionnalit√©s sp√©cifiques, notamment pour interagir avec Hadoop. Cependant, dans les versions plus r√©centes, certaines d√©pendances Hadoop sont fournies avec Spark, ce qui √©vite le besoin d'installer winutils.exe s√©par√©ment.\n",
    "\n",
    "Cependant, l'installation de winutils.exe peut aider √† am√©liorer les performances de certaines fonctionnalit√©s de Spark en exploitant des ressources natives sp√©cifiques √† Windows. Mais l'impact de winutils.exe sur les performances peut varier en fonction de la configuration sp√©cifique et des fonctionnalit√©s utilis√©es dans Spark.\n",
    "\n",
    "S'il est pr√©vu d'utiliser des fonctionnalit√©s de Spark qui n√©cessitent explicitement `winutils.exe`, comme l'acc√®s √† Hadoop Distributed File System (HDFS) ou d'autres d√©pendances sp√©cifiques √† Hadoop, alors l'installation de `winutils.exe` est recommand√©e. Cela permettra √† Spark de b√©n√©ficier pleinement des optimisations et des fonctionnalit√©s fournies par `winutils.exe`.\n",
    "\n",
    "Consid√©rons que nous menons l'exp√©rience de faire sans comme opportunit√© d'en explorer les √©ventuelles limites, et comme d√©monstration de la fausset√© du non support de Spark pour Windows.\n",
    "\n",
    "\n",
    "Retour ici le 13 mai. L'instruction suivante plante en l'absence de Winutils install√© :\n",
    "\n",
    "```python\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(im_path)\n",
    "```\n",
    "\n",
    "en provoquant cette erreur :\n",
    "```java\n",
    "Py4JJavaError: An error occurred while calling o45.load.\n",
    ": java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\n",
    "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n",
    "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\n",
    "\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\n",
    "\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\n",
    "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\n",
    "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
    "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
    "\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n",
    "\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\n",
    "\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\n",
    "```\n",
    "\n",
    "Installons donc Winutils pour Hadoop 3.0.0 :\n",
    "\n",
    "Seul le fichier suivant est n√©cessaire :\n",
    "https://github.com/steveloughran/winutils/tree/master/hadoop-3.0.0/bin/winutils.exe\n",
    "\n",
    "Les instructions suivantes t√©l√©chargeront le fichier `winutils.exe` depuis GitHub, cr√©eront le dossier `hadoop-3.0.0/bin/` sous le r√©pertoire `$env:SPARK_HOME`, y d√©placeront le fichier `winutils.exe` et d√©finiront la variable syst√®me `HADOOP_HOME` avec la valeur `%SPARK_HOME%\\hadoop-3.0.0`.\n",
    "\n",
    "```sh\n",
    "# Variable definition\n",
    "$githubUrl = \"https://github.com/steveloughran/winutils/tree/master/hadoop-3.0.0/bin/winutils.exe\"\n",
    "$hadoopHome = \"$env:SPARK_HOME\\hadoop-3.0.0\"\n",
    "$hadoopDir = \"$hadoopHome\\bin\"\n",
    "\n",
    "# Download winutils.exe from GitHub\n",
    "Invoke-WebRequest -Uri $githubUrl -OutFile \"winutils.exe\"\n",
    "\n",
    "# Create hadoop-3.0.0/bin/ directory\n",
    "New-Item -ItemType Directory -Path $hadoopDir -Force | Out-Null\n",
    "\n",
    "# Move winutils.exe file to hadoop-3.0.0/bin/\n",
    "Move-Item -Path \"winutils.exe\" -Destination $hadoopDir\n",
    "\n",
    "# Set HADOOP_HOME system variable\n",
    "[Environment]::SetEnvironmentVariable(\"HADOOP_HOME\", \"$hadoopHome\", \"Machine\")\n",
    "\n",
    "```\n",
    "\n",
    "Manifestement, il faut aussi la DLL cf. ce post :\n",
    "\n",
    "https://stackoverflow.com/questions/20584157/unsatisfiedlinkerror-nativeiowindows-access0-when-submitting-mapreduce-job-to\n",
    "\n",
    "\n",
    "\n",
    "Hum, finalement, √ßa n'aide pas.\n",
    "\n",
    "En revanche, si on suit les instructions de Microsoft, il faut √©galement compl√©ter la variable `PATH` :\n",
    "\n",
    "`PATH=%HADOOP_HOME%\\bin;%PATH%`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consignes d'installation de Winutils (s'il s'av√©rait n√©cessaire)\n",
    "\n",
    "Nous allons t√©l√©charger `winutils.exe` et configurer notre installation de Spark pour trouver `winutils.exe`.\n",
    "1. Cr√©ez un dossier `hadoop\\bin` √† l'int√©rieur du dossier `SPARK_HOME`.\n",
    "2. T√©l√©chargez [`winutils.exe`](https://github.com/steveloughran/winutils) correspondant √† la version de hadoop pour laquelle votre installation de Spark a √©t√© construite. Dans mon cas, la version de hadoop √©tait 2.6.0. J'ai donc t√©l√©charg√© winutils.exe pour hadoop 2.6.0 et l'ai copi√© dans le dossier `hadoop\\bin` du dossier `SPARK_HOME`.\n",
    "3. Cr√©ez une variable d'environnement syst√®me dans Windows appel√©e `SPARK_HOME` qui pointe vers le chemin du dossier `SPARK_HOME`. Recherchez sur Internet si vous avez besoin d'une mise √† jour sur la cr√©ation de variables d'environnement dans votre version de Windows, par exemple des articles comme ceux-ci.\n",
    "4. Cr√©ez une autre variable d'environnement syst√®me dans Windows appel√©e `HADOOP_HOME` qui pointe vers le dossier hadoop √† l'int√©rieur du dossier `SPARK_HOME`.\n",
    "\n",
    "√âtant donn√© que le dossier hadoop se trouve √† l'int√©rieur du dossier `SPARK_HOME`, il est pr√©f√©rable de cr√©er la variable d'environnement `HADOOP_HOME` en utilisant une valeur de `%SPARK_HOME%\\hadoop`. Ainsi, vous n'aurez pas √† modifier `HADOOP_HOME` si `SPARK_HOME` est mis √† jour.\n",
    "\n",
    "Si vous ex√©cutez maintenant le script `bin\\pyspark` √† partir d'une fen√™tre de commande Windows, les messages d'erreur li√©s √† `winutils.exe` devraient dispara√Ætre. Par exemple, j'ai obtenu les messages suivants apr√®s avoir ex√©cut√© l'utilitaire `bin\\pyspark` apr√®s avoir configur√© `winutils`.\n",
    "\n",
    "L'utilitaire `bin\\spark-submit` peut √©galement √™tre utilis√© avec succ√®s pour ex√©cuter le script `wordcount.py`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration du niveau de journalisation pour Spark\n",
    "\n",
    "Il y a encore beaucoup de messages INFO suppl√©mentaires dans la console chaque fois que vous d√©marrez ou quittez une session PySpark ou ex√©cutez l'utilitaire `spark-submit`. Nous allons donc apporter une autre modification √† notre installation Spark afin que seuls les messages d'avertissement et d'erreur soient affich√©s dans la console. Pour ce faire :\n",
    "* Copiez le fichier `log4j.properties.template` du dossier `SPARK_HOME\\conf` vers le dossier `SPARK_HOME\\conf` en tant que fichier `log4j.properties`.\n",
    "* D√©finissez la valeur de la propri√©t√© `log4j.rootCategory` sur `WARN, console`.\n",
    "* Enregistrez le fichier `log4j.properties`.\n",
    "\n",
    "Maintenant, aucun message informatif ne sera enregistr√© dans la console. Par exemple, voici les messages que j'ai obtenus apr√®s avoir ex√©cut√© l'utilitaire `bin\\pyspark` une fois que j'ai configur√© le niveau de journalisation sur `WARN`.\n",
    "\n",
    "----\n",
    "\n",
    "Ces instructions sont obsol√®tes avec Spark 3.4.0.\n",
    "\n",
    "Le fichier de configuration des logs s'appelle `log4j2.properties` et les noms des propri√©t√©s ne sont plus les m√™mes, ni m√™me le format des valeurs.\n",
    "\n",
    "Nouvelles instructions :\n",
    "* D√©finir la ligne `rootLogger.level` sur le niveau de journalisation souhait√©. Par exemple, si pour afficher uniquement les avertissements et les erreurs, d√©finir `rootLogger.level = warn`.\n",
    "* S'assurer que la ligne `rootLogger.appenderRef.stdout.ref` est configur√©e pour envoyer les journaux vers la console (`rootLogger.appenderRef.stdout.ref = console`).\n",
    "* Ajuster le format des messages de journal en modifiant la valeur de la ligne `appender.console.layout.pattern`. Par exemple, pour ajouter un horodatage aux messages de journal, utiliser `appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n%ex` (valeur par d√©faut du template).\n",
    "\n",
    "Apr√®s avoir effectu√© ces modifications, enregistrer le fichier `log4j2.properties`. Cela devrait r√©duire la quantit√© de logs affich√©s dans votre console PySpark en fonction de la configuration d√©finie.\n",
    "\n",
    "**NB** les nom des propri√©t√©s sont sensibles √† la casse, mais pas leurs valeurs. `WARN` et `warn` sont √©quivalents, et de m√™me `Console` et `console`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R√©sum√©\n",
    "\n",
    "Pour travailler avec PySpark, ouvrez une fen√™tre de commande Windows et rendez-vous dans votre r√©pertoire `SPARK_HOME`.\n",
    "* Pour d√©marrer un shell PySpark, ex√©cutez l'utilitaire `bin\\pyspark`. Une fois dans le shell PySpark, utilisez les noms `sc` et `sqlContext`, puis saisissez `exit()` pour revenir √† l'invite de commande.\n",
    "* Pour ex√©cuter un script Python autonome, ex√©cutez l'utilitaire `bin\\spark-submit` en sp√©cifiant le chemin de votre script Python ainsi que les √©ventuels arguments n√©cessaires dans l'invite de commande. Par exemple, pour ex√©cuter le script `wordcount.py` du r√©pertoire `examples` de votre dossier `SPARK_HOME`, vous pouvez ex√©cuter la commande suivante :\n",
    "\n",
    "```sh\n",
    "bin\\spark-submit examples\\src\\main\\python\\wordcount.py README.md\n",
    "```\n",
    "\n",
    "### R√©f√©rences\n",
    "\n",
    "J'ai utilis√© les r√©f√©rences suivantes pour recueillir des informations sur cet article.\n",
    "* Configuration de winutils.exe\n",
    "* T√©l√©chargement de Spark et mise en route (chapitre 2) du livre \"Learning Spark\" d'O'Reilly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Et dans le notebook dans VS Code\n",
    "\n",
    "\n",
    "```python\n",
    "spark = SparkSession.builder.appName(\"OTR\").config(\"spark.sql.caseSensitive\", \"True\").getOrCreate()\n",
    "display(spark)\n",
    "```\n",
    "\n",
    "Provoque cette erreur :\n",
    "\n",
    "```python\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "Cell In[3], line 1\n",
    "----> 1 spark = SparkSession.builder.appName(\"OTR\").config(\"spark.sql.caseSensitive\", \"True\").getOrCreate()\n",
    "      2 display(spark)\n",
    "\n",
    "File c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:477, in SparkSession.Builder.getOrCreate(self)\n",
    "    475     sparkConf.set(key, value)\n",
    "    476 # This SparkContext may be an existing one.\n",
    "--> 477 sc = SparkContext.getOrCreate(sparkConf)\n",
    "    478 # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n",
    "    479 # by all sessions.\n",
    "    480 session = SparkSession(sc, options=self._options)\n",
    "\n",
    "File c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:512, in SparkContext.getOrCreate(cls, conf)\n",
    "    510 with SparkContext._lock:\n",
    "    511     if SparkContext._active_spark_context is None:\n",
    "--> 512         SparkContext(conf=conf or SparkConf())\n",
    "    513     assert SparkContext._active_spark_context is not None\n",
    "    514     return SparkContext._active_spark_context\n",
    "\n",
    "File c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:198, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\n",
    "    192 if gateway is not None and gateway.gateway_parameters.auth_token is None:\n",
    "    193     raise ValueError(\n",
    "    194         \"You are trying to pass an insecure Py4j gateway to Spark. This\"\n",
    "    195         \" is not allowed as it is a security risk.\"\n",
    "    196     )\n",
    "--> 198 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n",
    "    199 try:\n",
    "    200     self._do_init(\n",
    "    201         master,\n",
    "    202         appName,\n",
    "   (...)\n",
    "    212         memory_profiler_cls,\n",
    "    213     )\n",
    "\n",
    "File c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:432, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\n",
    "    430 with SparkContext._lock:\n",
    "    431     if not SparkContext._gateway:\n",
    "--> 432         SparkContext._gateway = gateway or launch_gateway(conf)\n",
    "    433         SparkContext._jvm = SparkContext._gateway.jvm\n",
    "    435     if instance:\n",
    "\n",
    "File c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\java_gateway.py:106, in launch_gateway(conf, popen_kwargs)\n",
    "    103     time.sleep(0.1)\n",
    "    105 if not os.path.isfile(conn_info_file):\n",
    "--> 106     raise RuntimeError(\"Java gateway process exited before sending its port number\")\n",
    "    108 with open(conn_info_file, \"rb\") as info:\n",
    "    109     gateway_port = read_int(info)\n",
    "\n",
    "RuntimeError: Java gateway process exited before sending its port number\n",
    "```\n",
    "\n",
    "Il semblerait que l'installation de Java ne fixe pas par d√©faut la variable d'environnement `JAVA_HOME`.\n",
    "\n",
    "```sh\n",
    "[Environment]::SetEnvironmentVariable(\"JAVA_HOME\", \"C:\\Program Files\\Java\\jdk-17\", \"Machine\")\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
