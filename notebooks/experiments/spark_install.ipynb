{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå Retour d'exp√©rience installation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation sur Ubuntu\n",
    "\n",
    "*S'appliquer √† chaque √©tape pour ne pas devoir y revenir et y perdre un temps fou.*\n",
    "\n",
    "### Point sur la configuration syst√®me\n",
    "\n",
    "Version d'Ubuntu (22.04.2 LTS (jammy)) :\n",
    "\n",
    "```sh\n",
    "(base)$ lsb_release -a\n",
    "No LSB modules are available.\n",
    "Distributor ID: Ubuntu\n",
    "Description:    Ubuntu 22.04.2 LTS\n",
    "Release:        22.04\n",
    "Codename:       jammy\n",
    "```\n",
    "\n",
    "Version du JRE si install√© :\n",
    "\n",
    "```sh\n",
    "(base)$ java -version\n",
    "openjdk version \"11.0.18\" 2023-01-17\n",
    "OpenJDK Runtime Environment (build 11.0.18+10-Ubuntu-0ubuntu122.04)\n",
    "OpenJDK 64-Bit Server VM (build 11.0.18+10-Ubuntu-0ubuntu122.04, mixed mode, sharing)\n",
    "```\n",
    "\n",
    "Version de Python si install√© :\n",
    "\n",
    "```sh\n",
    "(base)$ python --version\n",
    "Python 3.9.12\n",
    "```\n",
    "\n",
    "```sh\n",
    "(base)$ pyenv versions\n",
    "* system (set by /home/pepper/.pyenv/version)\n",
    "  3.11.2\n",
    "```\n",
    "\n",
    "Activation globale ou locale (ce dossier et ses sous-dossiers) de la version 3.11.2 :\n",
    "\n",
    "```sh\n",
    "$ pyenv global 3.11.2\n",
    "```\n",
    "\n",
    "```sh\n",
    "$ pyenv local 3.11.2\n",
    "```\n",
    "\n",
    "Version de ipython install√©e, si install√©e :\n",
    "\n",
    "```sh\n",
    "(base)$ ipython --version\n",
    "8.2.0\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G√©rer les conflits d'environnement : PyEnv vs. Conda\n",
    "\n",
    "\n",
    "Activation de la version 3.11.2 (`system`) suivi d'un test de version :\n",
    "```sh\n",
    "(base)$ pyenv global system\n",
    "(base)$ python --version\n",
    "Python 3.9.12\n",
    "```\n",
    "\n",
    "Ce n'est pas le r√©sultat attendu (on s'attendait √† `Python 3.11.2`).\n",
    "\n",
    "```sh\n",
    "(base)$ /usr/bin/python3 --version\n",
    "Python 3.10.6\n",
    "```\n",
    "\n",
    "```sh\n",
    "(base)$ which python3\n",
    "/home/pepper/anaconda3/bin/python3\n",
    "```\n",
    "\n",
    "```sh\n",
    "(base)$ /home/pepper/anaconda3/bin/python3 --version\n",
    "Python 3.9.12\n",
    "```\n",
    "\n",
    "Manifestement il y a des conflits de versions et de gestion d'environnement entre `pyenv` et `conda`, et c'est en l'√©tat assez confus.\n",
    "\n",
    "```sh\n",
    "(base)$ echo $PATH\n",
    "~/anaconda3/bin:~/anaconda3/condabin:~/.pyenv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:usr/bin:/sbin:/bin:/usr/games:usr/local/games:/snap/bin:snap/bin\n",
    "```\n",
    "\n",
    "Interpr√©tation du probl√®me : une version 3.10.6 de Python est install√©e sur le syst√®me par d√©faut, mais Conda a install√© une version 3.9.12 dans son propre environnement et l'a ensuite ajout√© √† la variable `PATH`, avec priorit√© sur `pyenv`.\n",
    "\n",
    "On peut commencer par d√©sactiver l'environnement Conda pour la session en cours. Mais il faut se poser s√©rieusement la question de sa d√©sinstallation, vu que nous ne l'utilisons pas.\n",
    "\n",
    "```sh\n",
    "(base)$ conda deactivate\n",
    "$ python3 --version\n",
    "$ which python3\n",
    "Python 3.10.6\n",
    "/usr/bin/python3\n",
    "```\n",
    "\n",
    "Autre observation, le `PATH` ne contient pas le dossier `$HOME/.pyenv/shims`, donc `pyenv` n'a pas √©t√© correctement initialis√© apr√®s son installation.\n",
    "\n",
    "Commen√ßons par d√©sintaller Conda, pour n'avoir pas √† devoir faire de la gymnastique administrative co√ªteuse en temps pour un non administrateur syst√®me.\n",
    "\n",
    "```sh\n",
    "$ conda env list\n",
    "base * /home/pepper/anaconda3\n",
    "```\n",
    "\n",
    "Tips :\n",
    "* https://stackoverflow.com/questions/22585235/python-anaconda-how-to-safely-uninstall\n",
    "* https://docs.anaconda.com/free/anaconda/install/uninstall/\n",
    "\n",
    "\n",
    "```sh\n",
    "$ conda activate base\n",
    "(base)$ conda install anaconda-clean\n",
    "(base)$ anaconda-clean --yes\n",
    "(base)$ rm -rf ~/anaconda3\n",
    "```\n",
    "\n",
    "Et supprimer la section Anaconda du `~/.bashrc`.\n",
    "\n",
    "D√©cision finalement de ne pas le faire, `conda` pourrait √™tre exig√© par certains clients, donc il est utile de pouvoir le faire coexister avec pyenv sur mon syst√®me.\n",
    "\n",
    "Premi√®re modification : √©diter le fichier `~/.condarc` avec nano et ajouter la ligne `auto_activate_base: false\n",
    "`. Cette option, qui est √† `true` par d√©faut, active par d√©faut l'environnement `base` de Conda, ce qui interf√®re avec `pyenv init -` qui intervient ensuite dans le `.bashrc`\n",
    "\n",
    "R√©ex√©cution du `~/.bashrc` apr√®s cette modification :\n",
    "```sh\n",
    "$ source ~/.bashrc\n",
    "```\n",
    "\n",
    "D'une part, l'environnement `(base)` n'est plus activ√© (il apparaissait depuis le d√©but en pr√©fixe du prompt), d'autre part :\n",
    "```sh\n",
    "$ echo $PATH\n",
    "~/anaconda3/condabin:~/.pyenv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:usr/bin:/sbin:/bin:/usr/games:usr/local/games:/snap/bin:snap/bin\n",
    "```\n",
    "\n",
    "Le fait que `~/anaconda3/condabin` apparaisse toujours en t√™te de `PATH` montre (ce que l'on peut √©galement comprendre en d√©chiffrant la section `conda_setup` du `~/.bashrc`) que le `~/anaconda3/etc/profile.d/conda.sh` est toujours ex√©cut√©, et qu'il produit cette modification du `PATH`. Effectivement, il le fait dans sa derni√®re section `PATH=\"../condabin${PATH:+\"${PATH}\"}\"`\n",
    "\n",
    "Apr√®s pas mal d'investigation (et une demi-journ√©e cr√¢m√©e) pour tenter de faire coexister proprement conda et pyenv, voici o√π j'en suis :\n",
    "\n",
    "* Conda eexcute un hook de shell, un script qu'il g√©n√®re puis ex√©cute, donc sur lequel on n'a ni la main, ni de visibilit√© sur le contenu :\n",
    "* g√©n√©ration du script : `__conda_setup=\"$('/home/pepper/anaconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null)\"`\n",
    "* ex√©cution du script (s'il a √©t√© produit sans erreur : `if [ $? -eq 0 ]`) : `eval \"$__conda_setup\"`\n",
    "\n",
    "Je jette l'√©ponge, faute de temps. Je passe la section conda de `.bashrc` en commentaire.\n",
    "\n",
    "Au moins, cette demi-journ√©e m'aura-t-elle un peu r√©chauff√© sur le `shell` et sur l'utilisation de `nano`.\n",
    "\n",
    "On red√©marre de la v√©rification des versions :\n",
    "\n",
    "```sh\n",
    "$ python3 --version\n",
    "$ which python3\n",
    "Python 3.10.6\n",
    "/usr/bin/python3\n",
    "```\n",
    "\n",
    "Activation de la version 3.11.2 (`system`) suivi d'un test de version :\n",
    "```sh\n",
    "(base)$ pyenv global system\n",
    "(base)$ python --version\n",
    "Python 3.9.12\n",
    "```\n",
    "\n",
    "Je viens de comprendre, pour un co√ªt de 5h, que j'ai juste mal install√© pyenv :\n",
    "\n",
    "Dans le `bashrc`, j'ai un `eval \"(pyenv init -)\"` qui se contente de g√©n√©rer le script mais ne l'excute pas.\n",
    "\n",
    "Fix : `eval \"$(pyenv init -)\"`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mise √† jour et red√©marrage du syst√®me\n",
    "\n",
    "```sh\n",
    "$ sudo apt update && sudo apt -y full-upgrade\n",
    "```\n",
    "* `sudo apt update` : met √† jour la liste des paquets disponibles √† partir des sources de logiciels configur√©es sur le syst√®me. Il ne met pas √† jour les paquets eux-m√™mes.\n",
    "* `sudo apt -y full-upgrade` : met √† jour les paquets existants sur le syst√®me en installant les derni√®res versions disponibles. L'option -y r√©pond automatiquement ¬´ oui ¬ª √† toutes les questions de confirmation qui peuvent appara√Ætre pendant le processus.\n",
    "* Le `&&` est un op√©rateur logique qui permet d'ex√©cuter plusieurs commandes en m√™me temps, l'une apr√®s l'autre, √† condition que la commande pr√©c√©dente s'ex√©cute avec succ√®s. Dans cet exemple, la seconde commande `sudo apt -y full-upgrade` ne sera ex√©cut√©e que si la commande `sudo apt update` s'ex√©cute avec succ√®s.\n",
    "\n",
    "\n",
    "```sh\n",
    "$ [ -f /var/run/reboot-required ] && sudo reboot -f\n",
    "```\n",
    "\n",
    "* `[ -f /var/run/reboot-required ]` : v√©rifie si le fichier /var/run/reboot-required existe. Ce fichier est cr√©√© lorsqu'un paquet est install√© qui n√©cessite un red√©marrage pour prendre effet.\n",
    "* `&&` : l'op√©rateur logique ¬´ et ¬ª. Si la commande pr√©c√©dente ([ -f /var/run/reboot-required ]) est vraie (c'est-√†-dire que le fichier /var/run/reboot-required existe), alors la commande suivante (sudo reboot -f) est ex√©cut√©e.\n",
    "* `sudo reboot -f` : red√©marre le syst√®me en for√ßant le red√©marrage sans demander de confirmation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation des pr√©requis\n",
    "\n",
    "La doc de PySpark indique d'installer ces pr√©requs :\n",
    "\n",
    "```sh\n",
    "sudo apt-get install build-essential checkinstall\n",
    "sudo apt-get install libreadline-gplv2-dev libncursesw5-dev libssl-dev libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev\n",
    "sudo apt-get install python\n",
    "sudo easy_install pip\n",
    "sudo pip install ipython\n",
    "```\n",
    "\n",
    "On peut commencer par faire un point sur ce qui est d√©j√† install√© ou non :\n",
    "\n",
    "```sh\n",
    "dpkg-query -l | grep checkinstall           # Suivi d'installations\n",
    "dpkg-query -l | grep build-essential        # Compilation de programmes\n",
    "dpkg-query -l | grep libreadline-gplv2-dev  # √©dition interactive d'entr√©es en ligne de commande\n",
    "dpkg-query -l | grep libncursesw5-dev       # interfaces utilisateurs textuelles pour terminaux\n",
    "dpkg-query -l | grep libssl-dev             # Open SSL\n",
    "...\n",
    "ii build-essentiel 12.9ubuntu3 amd64 Informational list of builde-essentiel packages\n",
    "ii libssl-dev 3.0.2-0ubuntu1.9 amd64 Secure Sockets Layer toolkit - development files\n",
    "```\n",
    "\n",
    "Point rapide sur ces paquets :\n",
    "* **`build-essential`** est un ensemble de paquets contenant des outils n√©cessaires √† la compilation des programmes sur Ubuntu.\n",
    "* **`libreadline-gplv2-dev`** contient les fichiers de d√©veloppement n√©cessaires pour utiliser la biblioth√®que `readline` dans les programmes.\n",
    "* **`libncursesw5-dev`** contient les fichiers de d√©veloppement pour la biblioth√®que `ncurses`, qui est utilis√©e pour cr√©er des interfaces utilisateurs textuelles pour les terminaux.\n",
    "* **`libssl-dev`** contient les fichiers de d√©veloppement pour la biblioth√®que `OpenSSL`, qui est utilis√©e pour la cryptographie dans de nombreux programmes.\n",
    "* **`libsqlite3-dev`** contient les fichiers de d√©veloppement pour la biblioth√®que `SQLite3`, qui est utilis√©e pour la gestion de base de donn√©es.\n",
    "* **`tk-dev`** contient les fichiers de d√©veloppement pour le toolkit `Tk`, qui est utilis√© pour cr√©er des interfaces graphiques pour les applications.\n",
    "* **`libgdbm-dev`** contient les fichiers de d√©veloppement pour la biblioth√®que `GDBM`, qui est utilis√©e pour stocker des paires cl√©/valeur dans des fichiers.\n",
    "* **`libc6-dev`** contient les fichiers de d√©veloppement pour la biblioth√®que `C`, qui est la biblioth√®que standard de manipulation de cha√Ænes, de fichiers et d'entr√©es/sorties pour les programmes en `C`.\n",
    "* **`libbz2-dev`** contient les fichiers de d√©veloppement pour la biblioth√®que `Bzip2`, qui est utilis√©e pour la compression de fichiers.\n",
    "* **`checkinstall`** est un outil qui permet de cr√©er des paquets Debian √† partir des programmes install√©s depuis les sources. Cela facilite la gestion des programmes install√©s sur le syst√®me, permettant de les installer, les d√©sinstaller et les mettre √† jour plus facilement.\n",
    "\n",
    "`PySpark` utilise la biblioth√®que `Py4J` pour √©tablir une communication entre Python et le processus JVM de Spark. `Py4J` utilise la biblioth√®que `Readline` pour l'interpr√©teur Python interactif, et la biblioth√®que `Ncurses` pour l'interface utilisateur textuelle de son outil `py4j-asyncio`. Ces deux librairies sont √©galement n√©cessaires pour am√©liorer l'exp√©rience utilisateur avec le terminal interactif `iPython`.\n",
    "\n",
    "La biblioth√®que `SQLite3` est utilis√©e par `PySpark` pour stocker des informations sur les t√¢ches en cours d'ex√©cution dans la base de donn√©es int√©gr√©e √† `Spark`. En outre, `PySpark` peut utiliser la biblioth√®que `Bzip2` pour la compression des donn√©es.\n",
    "\n",
    "Dans le contexte de l'installation de `PySpark`, `checkinstall` n'est pas strictement n√©cessaire, car l'installation de `PySpark` se fait g√©n√©ralement √† partir de fichiers binaires pr√©-compil√©s, plut√¥t que depuis les sources. Cependant, l'utilisation de `checkinstall` peut √™tre utile si vous souhaitez installer d'autres programmes √† partir des sources √† l'avenir.\n",
    "\n",
    "```sh\n",
    "...\n",
    "le paquet \"libreadline-gplv2-dev\" n'a pas de version susceptible d'√™tre install√©e.\n",
    "```\n",
    "\n",
    "On fait quoi pour ne pas avoir des probl√®mes plus tard ?\n",
    "\n",
    "Si le paquet `libreadline-gplv2-dev` n'a pas de version disponible, il est possible que le nom du paquet ait chang√© ou qu'il ne soit plus disponible dans les sources de la distribution Ubuntu.\n",
    "\n",
    "Prenons une alternative √† la biblioth√®que `readline`, comme la biblioth√®que GNU readline.\n",
    "\n",
    "```sh\n",
    "sudo apt-get install libreadline-dev\n",
    "```\n",
    "\n",
    "Il sera peut-√™tre n√©cessaire, plus tard dans l'installation, de modifier les configurations de `Py4J` et de `PySpark` pour qu'ils utilisent la nouvelle biblioth√®que.\n",
    "\n",
    "\n",
    "On nous demande d'installer un `pip` pour le syst√®me entier (hors environnement virtuel)\n",
    "\n",
    "```sh\n",
    "sudo easy_install pip\n",
    "```\n",
    "\n",
    "Restons sur la version li√©e √† un environnement :\n",
    "```sh\n",
    "sudo apt-get install python3-pip \n",
    "```\n",
    "\n",
    "Nous verrons plus tard si cela pose un probl√®me.\n",
    "\n",
    "\n",
    "Installation de `iPython`\n",
    "\n",
    "Dans le contexte `PySpark`, `iPython` qui joue habituellement le r√¥le de back-office n√©cessaire pour l'utilisateur de notebooks va jouer son r√¥le premier qui est de fournir un terminal de substitution au terminal par d√©faut. Ce terminal plus √©volu√© permet de b√©n√©ficier de fonctionnalit√©s suppl√©mentaires telles que la coloration syntaxique, l'auto-compl√©tion et l'historique des commandes.\n",
    "\n",
    "```sh\n",
    "$ sudo pip install ipython \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation de Java\n",
    "\n",
    "Version du JRE si install√© :\n",
    "\n",
    "```sh\n",
    "$ java -version\n",
    "openjdk version \"11.0.18\" 2023-01-17\n",
    "OpenJDK Runtime Environment (build 11.0.18+10-Ubuntu-0ubuntu122.04)\n",
    "OpenJDK 64-Bit Server VM (build 11.0.18+10-Ubuntu-0ubuntu122.04, mixed mode, sharing)\n",
    "```\n",
    "\n",
    "Sinon, cf. doc pr√©conis√©e :\n",
    "```sh\n",
    "sudo apt install curl mlocate default-jdk -y\n",
    "```\n",
    "\n",
    "et cf. manuel `PySpark` :\n",
    "```sh\n",
    "sudo apt-add-repository ppa:webupd8team/java\n",
    "sudo apt-get update\n",
    "sudo apt-get install oracle-java8-installer\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation d'`Apache Spark`\n",
    "\n",
    "A ne pas confondre avec `PySpark` qui est l'API pour interagir avec `Apache Spark` depuis un environnement de d√©veloppement Python (comme alternative √† `Java` ou `Scala`).\n",
    "\n",
    "Deux √©tapes :\n",
    "* t√©l√©charger et installer\n",
    "* configurer\n",
    "\n",
    "Cf. doc pr√©conis√©e :\n",
    "```sh\n",
    "wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "tar xvf spark-3.2.1-bin-hadoop3.2.tgz\n",
    "sudo mv spark-3.2.1-bin-hadoop3.2/ /opt/spark \n",
    "```\n",
    "\n",
    "et cf. manuel `PySpark` :\n",
    "* Download: You can get the Pre-built Apache Spark‚Ñ¢ from [**Download Apache Spark‚Ñ¢**](https://spark.apache.org/downloads.html).\n",
    "* Unpack: Unpack the Apache Spark‚Ñ¢ to the path where you want to install the Spark.\n",
    "\n",
    "On va rester dans une approche wget etc, mais en prenant la derni√®re version, la 3.4.0 (du 13 avril 2023).\n",
    "\n",
    "https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
    "\n",
    "```sh\n",
    "wget https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
    "tar xvf spark-3.4.0-bin-hadoop3.tgz\n",
    "sudo mv spark-3.4.0-bin-hadoop3/ /opt/spark \n",
    "```\n",
    "\n",
    "Attention, si on visite le dossier de t√©l√©chargement `https://dlcdn.apache.org/spark/spark-3.4.0/`, on constate qu'il y a plusieurs archives qui correspondent √† des contenus de diff√©rentes natures :\n",
    "* le noyau d'infrastructure Spark:\n",
    "    * qui n'embarque pas Hadoop (qui pourrait √™tre d√©j√† install√© sur le syst√®me)\n",
    "    * qui l'embarque\n",
    "        * version valid√©e pour fonctionner avec Scala 2.13\n",
    "        * version qui fonctionne avec une version de Scala ant√©rieure\n",
    "* des packages d'API pour interagir avec cette infrastructure qu'il faut donc avoir pr√©alablement install√©e :\n",
    "    * SparkR : programmation R\n",
    "    * PySpark : programmation Python\n",
    "\n",
    "La distinction entre les distributions spark et spark_witout Hadoop n'est pas imm√©diatement claire :\n",
    "* `spark-3.4.0-bin-without-hadoop` ne contient pas les biblioth√®ques Hadoop n√©cessaires √† l'ex√©cution de Spark sur un cluster Hadoop existant. Il est destin√© aux utilisateurs qui souhaitent ex√©cuter Spark sur un cluster non-Hadoop ou qui souhaitent int√©grer Spark avec une version Hadoop sp√©cifique qu'ils ont d√©j√† install√©e.\n",
    "* `spark-3.4.0`, quant √† lui, est livr√© avec des biblioth√®ques Hadoop pr√©-compil√©es pour Hadoop 3.2 et n'a donc pas besoin d'une installation Hadoop s√©par√©e pour fonctionner. Il est destin√© aux utilisateurs qui veulent une distribution Spark autonome, pr√™te √† l'emploi.\n",
    "\n",
    "Nous utiliserons la version `Spark + Hadoop` et `PySpark` pour une interaction en Python.\n",
    "\n",
    "Le r√©pertoire `/opt` est une convention de l'*Unix Filesystem Hierarchy Standard* qui permet de stocker des donn√©es d'applications suppl√©mentaires qui ne font pas partie du syst√®me d'exploitation. Les applications tierces peuvent √™tre install√©es dans ce dossier sans interf√©rer avec les fichiers syst√®me existants. Dans le cas de `Spark`, il est recommand√© de stocker l'installation dans `/opt` pour des raisons de s√©curit√© et de gestion de version.\n",
    "\n",
    "`https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz` fait 370M\n",
    "\n",
    "C'est parti :\n",
    "\n",
    "```sh\n",
    "wget https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
    "tar xvf spark-3.4.0-bin-hadoop3.tgz\n",
    "sudo mv spark-3.4.0-bin-hadoop3/ /opt/spark \n",
    "```\n",
    "\n",
    "Ce n'est pas pr√©cis√©, mais :\n",
    "```sh\n",
    "rm spark-3.4.0-bin-hadoop3.tgz \n",
    "```\n",
    "n'est pas inutile.\n",
    "\n",
    "Reste √† configurer le `.bashrc` pour la mise √† jour du `PATH`:\n",
    "\n",
    "```sh\n",
    "nano ~/.bashrc\n",
    "```\n",
    "\n",
    "Ajout des lignes\n",
    "```sh\n",
    "export SPARK_HOME=/opt/spark\n",
    "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "```\n",
    "\n",
    "et mise √† jour de l'environnement du terminal :\n",
    "```sh\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "Test de bon fonctionnement, avec le lancement d'un serveur ma√Ætre autonome (Standalone Master Server) :\n",
    "```sh\n",
    "$ start-master.sh \n",
    "starting org.apache.spark.deploy.master.Master, logging to /opt/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-<Ma machine>.out\n",
    "```\n",
    "\n",
    "Le processus √©coute sur le port TCP 8080 :\n",
    "```sh\n",
    "$ sudo ss -tunelp | grep 8080\n",
    "tcp   LISTEN  0       1                           *:8080                *:*      users:((\"java\",pid=15356,fd=307)) ... sk:5 v6only:0 <-> \n",
    "```\n",
    "\n",
    "Ouverture d'un navigateur sur l'URL `http://localhost:8080` :\n",
    "\n",
    "L'intreface Web de supervision s'affiche : `Spark Master at spark://SalviaDivinorum:7077`\n",
    "\n",
    "Mon URL spark est `spark://SalviaDivinorum:7077`\n",
    "\n",
    "Cette URL correspond √† l'adresse du serveur Spark utilis√©e par le client Spark pour communiquer avec le cluster Spark. Plus pr√©cis√©ment, cette URL sp√©cifie le mode de connexion \"standalone\", utilis√© pour ex√©cuter des applications Spark sur un cluster autonome.\n",
    "\n",
    "La partie \"SalviaDivinorum\" correspond au nom de la machine sur laquelle le serveur Spark est ex√©cut√© et le port 7077 est le port par d√©faut sur lequel le serveur Spark √©coute les connexions entrantes. Si vous avez sp√©cifi√© un port diff√©rent lors de la configuration du serveur Spark, alors le num√©ro de port dans l'URL sera diff√©rent.\n",
    "\n",
    "**NB** les url en g√©n√©ral ne sont pas sensibles √† la casse, mais il faudra s'assurer que l'emploi de `spark://salviadivinorum:7077` ne pose pas de probl√®me.\n",
    "\n",
    "On va pouvoir le faire de suite avec le test de client.\n",
    "\n",
    "Lancement un Spark Worker Process esclave :\n",
    "\n",
    "```sh\n",
    "$ start-slave.sh spark://ubuntu:7077\n",
    "starting org.apache.spark.deploy.worker.Worker, logging to /opt/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-<Ma machine>.out\n",
    "```\n",
    "\n",
    "Pas de probl√®me avec l'URL, le Worker client est lanc√©.\n",
    "\n",
    "Dan l'UI, le worker est bien visible.\n",
    "\n",
    "Lancement du `Spark shell`\n",
    "\n",
    "```sh\n",
    "$ spark-shell\n",
    "...\n",
    "scala> ... si on ne ma√Ætrise pas Scala, on a besoin de PySpark !^^\n",
    "```\n",
    "\n",
    "Le Spark Shell pour Scala est un REPL (Read-Eval-Print Loop) qui s'ex√©cute dans le terminal standard du syst√®me d'exploitation, et non dans une interface comme `iPython`. Pour b√©n√©ficier du terminal `iPython` avec `Spark`, il faut utiliser `PySpark`.\n",
    "\n",
    "Deux bonnes raisons de passer √† la derni√®re √©tape d'installation avec `PySpark`.\n",
    "\n",
    "Pour fermer proprement les processus ma√Ætre et esclave lanc√©s pr√©c√©demment :\n",
    "\n",
    "Pas cela, c'est d√©pr√©ci√© :\n",
    "```sh\n",
    "$ SPARK_HOME/sbin/stop-slave.sh\n",
    "$ SPARK_HOME/sbin/stop-master.sh\n",
    "```\n",
    "\n",
    "Mais cela:\n",
    "```sh\n",
    "$ SPARK_HOME/sbin/stop-worker.sh\n",
    "$ SPARK_HOME/sbin/stop-master.sh\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation de PySpark\n",
    "\n",
    "Nous souhaitons disposer de toutes les fonctionnalit√©s suppl√©mentaires qui ne font pas partie de la version de base :\n",
    "* **`pyspark[sql]`** : ajoute le support pour les DataFrames SQL, ce qui permet d'utiliser SQL pour interagir avec les donn√©es.\n",
    "* **`pyspark[pandas_on_spark]`** : ajoute le support pour l'utilisation de Pandas avec Spark, ce qui permet de travailler avec des donn√©es Pandas sur un cluster Spark.\n",
    "* **`pyspark[plotly]`** : ajoute le support pour la visualisation avec Plotly, une biblioth√®que de visualisation interactive.\n",
    "* **`pyspark[connect]`** : ajoute le support pour la connexion √† des bases de donn√©es externes telles que Cassandra, HBase, etc.\n",
    "\n",
    "Installation compl√®te :\n",
    "\n",
    "Attention, m√™me si cela est tentant, ne pas ajouter d'espace derri√®re les virgules.\n",
    "\n",
    "```sh\n",
    "$ pip install pyspark\n",
    "$ pip install pyspark[sql,pandas_on_spark,plotly,connect]\n",
    "```\n",
    "\n",
    "Lancement du shell PySpark :\n",
    "\n",
    "```sh\n",
    "$ pyspark\n",
    "```\n",
    "\n",
    "Lancement avec le terminal √©volu√© `IPython` :\n",
    "\n",
    "Cela reste √† cette heure une inconnue m√™me si j'ai quelques tips ici :\n",
    "\n",
    "https://stackoverflow.com/questions/31862293/how-to-load-ipython-shell-with-pyspark\n",
    "\n",
    "Ici https://github.com/apache/spark/blob/master/bin/pyspark\n",
    "\n",
    "Il y un message plut√¥t explicite :\n",
    "\n",
    "```sh\n",
    "# Fail noisily if removed options are set\n",
    "if [[ -n \"$IPYTHON\" || -n \"$IPYTHON_OPTS\" ]]; then\n",
    "  echo \"Error in pyspark startup:\"\n",
    "  echo \"IPYTHON and IPYTHON_OPTS are removed in Spark 2.0+. Remove these from the environment and set PYSPARK_DRIVER_PYTHON and PYSPARK_DRIVER_PYTHON_OPTS instead.\"\n",
    "  exit 1\n",
    "fi\n",
    "```\n",
    "\n",
    "Comme c'est du confort, nous √©tudierons les deux variables `PYSPARK_DRIVER_PYTHON` et `PYSPARK_DRIVER_PYTHON_OPTS` √† une prochaine it√©ration.\n",
    "\n",
    "Installation sur Ubuntu compl√©t√©e !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation sur Windows\n",
    "\n",
    "*S'appliquer √† chaque √©tape pour ne pas devoir y revenir et y perdre un temps fou.*\n",
    "\n",
    "https://sparkbyexamples.com/spark/apache-spark-installation-on-windows/\n",
    "\n",
    "La section 3.3 de [Learning Apache Spark with Python](https://runawayhorse001.github.io/LearningApacheSpark/pyspark.pdf) renvoie directement sur ce tutoriel : [Getting Started with PySpark on Windows](http://deelesh.github.io/pyspark-windows.html)\n",
    "\n",
    "Ce tutoriel comme le pr√©c√©dent, qui me semblent un peu dat√©s, m'invitent √† installer Winutils :\n",
    "\n",
    "* **Spark with winutils.exe on Windows** :\n",
    "\n",
    "*Many beginners think Apache Spark needs a Hadoop cluster installed to run but that‚Äôs not true, Spark can run on AWS by using S3, Azure by using blob storage without Hadoop and HDFS e.t.c.*\n",
    "\n",
    "*To run Apache Spark on windows, you need winutils.exe as it uses POSIX like file access operations in windows using windows API.*\n",
    "\n",
    "*winutils.exe enables Spark to use Windows-specific services including running shell commands on a windows environment.*\n",
    "\n",
    "*Download winutils.exe for Hadoop 2.7 and copy it to %SPARK_HOME%\\bin folder. Winutils are different for each Hadoop version hence download the right version based on your Spark vs Hadoop distribution from https://github.com/steveloughran/winutils*\n",
    "\n",
    "\n",
    "Je me demande si c'est vraiment une bonne id√©e d'entrer dans une telle usine √† gaz.\n",
    "\n",
    "En outre, maintenant, sous Windows, on a WSL, donc la distribution Linux de son choix, pourquoi donc ne pas en tirer parti ?\n",
    "\n",
    "L'√©poque du dual boot est pass√©e, en je peux donc en pratique :\n",
    "* Installer Spark dans WSL\n",
    "* Partager des fichiers entre la partie Windows et la partie Linux du syst√®me.\n",
    "\n",
    "En d'autres termes :\n",
    "* Je peux continuer √† d√©velopper dans mon VS Code pr√©f√©r√© c√¥t√© Windows.\n",
    "* Et tester sur mon Spark local, les m√™mes fichiers, juste en changeant de fen√™tre.\n",
    "* Rien ne m'emp√™che d'installer PySpark √©galement c√¥t√© Windows pour :\n",
    "  * B√©n√©fichier du check pylance des imports et des dosctrings, m√™me si je peux ex√©cuter directement en local\n",
    "  * Pour directement d√©ployer dans le cloud ce que j'ai ainsi pu tester en local.\n",
    "\n",
    "\n",
    "### Point sur la configuration syst√®me\n",
    "\n",
    "Version de Windows - depuis PowerShell :\n",
    "\n",
    "```sh\n",
    "systeminfo   # | Select-String \"^OS Name\",\"^OS Version\"\n",
    "Nom de l‚Äôh√¥te:                              BELLADONNA\n",
    "Nom du syst√®me d‚Äôexploitation:              Microsoft Windows 11 Famille\n",
    "Version du syst√®me:                         10.0.22621 N/A build 22621\n",
    "Fabricant du syst√®me d‚Äôexploitation:        Microsoft Corporation\n",
    "Configuration du syst√®me d‚Äôexploitation:    Station de travail autonome\n",
    "Type de build du syst√®me d‚Äôexploitation:    Multiprocessor Free\n",
    "Propri√©taire enregistr√©:                    franck.lepoivre@platypus.academy\n",
    "Organisation enregistr√©e:                   N/A\n",
    "Identificateur de produit:                  00325-81958-50222-AAOEM\n",
    "Date d‚Äôinstallation originale:              05/02/2023, 20:34:23\n",
    "Heure de d√©marrage du syst√®me:              06/05/2023, 16:17:25\n",
    "Fabricant du syst√®me:                       Acer\n",
    "Mod√®le du syst√®me:                          Nitro AN517-51\n",
    "Type du syst√®me:                            x64-based PC\n",
    "Processeur(s):                              1 processeur(s) install√©(s).\n",
    "                                            [01]¬†: Intel64 Family 6 Model 158 Stepping 10 GenuineIntel ~2592 MHz\n",
    "Version du BIOS:                            Insyde Corp. V1.31, 29/06/2020\n",
    "R√©pertoire Windows:                         C:\\WINDOWS\n",
    "R√©pertoire syst√®me:                         C:\\WINDOWS\\system32\n",
    "P√©riph√©rique d‚Äôamor√ßage:                    \\Device\\HarddiskVolume1\n",
    "Option r√©gionale du syst√®me:                fr;Fran√ßais (France)\n",
    "Param√®tres r√©gionaux d‚Äôentr√©e:              fr;Fran√ßais (France)\n",
    "Fuseau horaire:                             (UTC+01:00) Bruxelles, Copenhague, Madrid, Paris\n",
    "M√©moire physique totale:                    16¬†221 Mo\n",
    "M√©moire physique disponible:                3¬†434 Mo\n",
    "M√©moire virtuelle¬†: taille maximale:        65¬†373 Mo\n",
    "M√©moire virtuelle¬†: disponible:             44¬†513 Mo\n",
    "M√©moire virtuelle¬†: en cours d‚Äôutilisation: 20¬†860 Mo\n",
    "Emplacements des fichiers d‚Äô√©change:        C:\\pagefile.sys\n",
    "Domaine:                                    WORKGROUP\n",
    "Serveur d‚Äôouverture de session:             \\\\BELLADONNA\n",
    "Correctif(s):                               4 Corrections install√©es.\n",
    "                                            [01]: KB5022497\n",
    "                                            [02]: KB5012170\n",
    "                                            [03]: KB5025239\n",
    "                                            [04]: KB5025749\n",
    "Carte(s) r√©seau:                            2 carte(s) r√©seau install√©e(s).\n",
    "                                            [01]: Killer E2500 Gigabit Ethernet Controller\n",
    "                                                  Nom de la connexion¬†: Ethernet\n",
    "                                                  √âtat¬†:                Support d√©connect√©\n",
    "                                            [02]: Intel(R) Wi-Fi 6 AX200 160MHz\n",
    "                                                  Nom de la connexion¬†: Wi-Fi\n",
    "                                                  DHCP activ√©¬†:         Oui\n",
    "                                                  Serveur DHCP¬†:        192.168.1.254\n",
    "                                                  Adresse(s) IP\n",
    "                                                  [01]: 192.168.1.168\n",
    "                                                  [02]: fe80::307a:8990:2f21:b062\n",
    "                                                  [03]: 2a01:e0a:272:1f00:91ba:73cb:9a8b:6729\n",
    "                                                  [04]: 2a01:e0a:272:1f00:6c68:bf4:ce90:39fa\n",
    "                                                  [05]: 2a01:e0a:272:1f00:607d:4e80:fbb0:a6dd\n",
    "                                                  [06]: 2a01:e0a:272:1f00:fabb:fe7d:63ce:c470\n",
    "Configuration requise pour Hyper-V:         Un hyperviseur a √©t√© d√©tect√©. Les fonctionnalit√©s n√©cessaires √† Hyper-V ne seront pas affich√©es.\n",
    "...\n",
    "```\n",
    "\n",
    "Quelles distributions Linux install√©e sur WSL ? Depuis PowerShell :\n",
    "\n",
    "```sh\n",
    "wsl --list --verbose\n",
    "  NAME      STATE           VERSION\n",
    "* Ubuntu    Running         2\n",
    "```\n",
    "\n",
    "Depuis WSL :\n",
    "\n",
    "```sh\n",
    "cat /etc/os-release\n",
    "NAME=\"Ubuntu\"\n",
    "VERSION=\"20.04.4 LTS (Focal Fossa)\"\n",
    "ID=ubuntu\n",
    "ID_LIKE=debian\n",
    "PRETTY_NAME=\"Ubuntu 20.04.4 LTS\"\n",
    "VERSION_ID=\"20.04\"\n",
    "HOME_URL=\"https://www.ubuntu.com/\"\n",
    "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
    "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
    "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
    "VERSION_CODENAME=focal\n",
    "UBUNTU_CODENAME=focal\n",
    "```\n",
    "\n",
    "Version du JRE si install√© :\n",
    "\n",
    "C√¥t√© Ubuntu/WSL :\n",
    "```sh\n",
    "$ java --version\n",
    "openjdk 11.0.15 2022-04-19\n",
    "OpenJDK Runtime Environment (build 11.0.15+10-Ubuntu-0ubuntu0.20.04.1)\n",
    "OpenJDK 64-Bit Server VM (build 11.0.15+10-Ubuntu-0ubuntu0.20.04.1, mixed mode, sharing)\n",
    "```\n",
    "\n",
    "Une mise √† jour ne serait pas inutile :\n",
    "\n",
    "```sh\n",
    "sudo apt-get update && sudo apt-get dist-upgrade\n",
    "```\n",
    "\n",
    "```sh\n",
    "$ java --version\n",
    "openjdk 11.0.18 2023-01-17\n",
    "OpenJDK Runtime Environment (build 11.0.18+10-post-Ubuntu-0ubuntu120.04.1)\n",
    "OpenJDK 64-Bit Server VM (build 11.0.18+10-post-Ubuntu-0ubuntu120.04.1, mixed mode, sharing)\n",
    "```\n",
    "\n",
    "C'est mieux!\n",
    "\n",
    "```sh\n",
    "$ python3 --version\n",
    "Python 3.8.10\n",
    "$ which python3\n",
    "/usr/bin/python3\n",
    "$ pyenv versions\n",
    "/mnt/c/Users/franc/.pyenv/pyenv-win/bin/pyenv: 3: cygpath: not found\n",
    "/mnt/c/Users/franc/.pyenv/pyenv-win/bin/pyenv: 3: exec: cmd: not found\n",
    "```\n",
    "\n",
    "Installation `pyenv` : https://kfields.netlify.app/blog/pyenv_on_ubuntu_22\n",
    "\n",
    "```sh\n",
    "sudo apt-get update; sudo apt-get install make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev\n",
    "curl https://pyenv.run | bash\n",
    "```\n",
    "\n",
    "Maj. `.bashrc`:\n",
    "\n",
    "```sh\n",
    "$ nano ~/.bashrc\n",
    "# Ajout √† la fin de :\n",
    "export PATH=\"$HOME/.pyenv/bin:$PATH\"\n",
    "eval \"$(pyenv init --path)\"\n",
    "eval \"$(pyenv virtualenv-init -)\"\n",
    "```\n",
    "\n",
    "Prise en compte dans la session en cours:\n",
    "\n",
    "```sh\n",
    "$ source ~/.bashrc\n",
    "```\n",
    "\n",
    "Installation de Python 3.11 dans Pyenv et activation globale.\n",
    "\n",
    "```sh\n",
    "$ pyenv versions\n",
    "* system (set by ~/.pyenv/version)\n",
    "$ pyenv install 3.11\n",
    "$ pyenv install 3.11\n",
    "Downloading Python-3.11.3.tar.xz...\n",
    "-> https://www.python.org/ftp/python/3.11.3/Python-3.11.3.tar.xz\n",
    "Installing Python-3.11.3...\n",
    "Installed Python-3.11.3 to ~/.pyenv/versions/3.11.3\n",
    "$ pyenv versions\n",
    "* system (set by ~/.pyenv/version)\n",
    "  3.11.3\n",
    "$ python3 --version\n",
    "Python 3.8.10\n",
    "$ which python3\n",
    "~/.pyenv/shims/python3\n",
    "$ pyenv global 3.11\n",
    "$ python3 --version\n",
    "Python 3.11.3\n",
    "```\n",
    "\n",
    "Version de ipython install√©e, si install√©e :\n",
    "\n",
    "```sh\n",
    "(base)$ ipython --version\n",
    "8.2.0\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation des packages n√©cessaires\n",
    "\n",
    "```sh\n",
    "$ sudo apt-get install build-essential checkinstall\n",
    "$ sudo apt-get install libreadline-gplv2-dev    # Hum, ici pas de pb, et c'est libreadline-dev qui a √©t√© remplac√©e\n",
    "$ sudo apt-get install libncursesw5-dev libssl-dev libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev\n",
    "$ sudo pip install ipython\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation d'`Apache Spark`\n",
    "\n",
    "C'est parti cf. plus haut, avec un petit d√©placement pr√©alable vers ~ (c'est plus propre).\n",
    "\n",
    "```sh\n",
    "cd ~\n",
    "wget https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
    "tar xvf spark-3.4.0-bin-hadoop3.tgz\n",
    "sudo mv spark-3.4.0-bin-hadoop3/ /opt/spark\n",
    "rm spark-3.4.0-bin-hadoop3.tgz  \n",
    "```\n",
    "\n",
    "Reste √† configurer le `.bashrc` pour la mise √† jour du `PATH`:\n",
    "\n",
    "```sh\n",
    "nano ~/.bashrc\n",
    "# Ajout des lignes\n",
    "export SPARK_HOME=/opt/spark\n",
    "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "# mise √† jour de l'environnement du terminal :\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "Test de bon fonctionnement, avec le lancement d'un serveur ma√Ætre autonome (Standalone Master Server) :\n",
    "```sh\n",
    "$ start-master.sh \n",
    "starting org.apache.spark.deploy.master.Master, logging to /opt/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-<Ma machine>.out\n",
    "```\n",
    "\n",
    "Le processus √©coute sur le port TCP 8080 :\n",
    "```sh\n",
    "$ sudo ss -tunelp | grep 8080\n",
    "tcp   LISTEN  0       1                           *:8080                *:*      users:((\"java\",pid=15356,fd=307)) ... sk:5 v6only:0 <-> \n",
    "```\n",
    "\n",
    "Ouverture d'un navigateur sur l'URL `http://localhost:8080` :\n",
    "\n",
    "Reste √† installer un navigateur (merci √† wslg!):\n",
    "\n",
    "```sh\n",
    "$ sudo apt install firefox\n",
    "$ firefox   # --no-remote (pour √©viter err : [ERROR glean_core] Error setting metrics feature config: Json(Error(\"EOF while parsing a value\", line: 1, column: 0)))\n",
    "```\n",
    "\n",
    "Ok : `spark://Belladonna.localdomain:7077`\n",
    "\n",
    "Peut-on former un r√©seau local pour acc√©der depuis Windows au port HTTP d'Ubuntu/WSL ?\n",
    "\n",
    "C√¥t√© Ubuntu/WSL : IP **172.28.176.216**\n",
    "```sh\n",
    "$ sudo apt install net-tools\n",
    "$ ifconfig\n",
    "eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n",
    "        inet 172.28.176.216  netmask 255.255.240.0  broadcast 172.28.191.255\n",
    "        inet6 fe80::215:5dff:fefc:75ad  prefixlen 64  scopeid 0x20<link>\n",
    "        ether 00:15:5d:fc:75:ad  txqueuelen 1000  (Ethernet)\n",
    "        RX packets 559833  bytes 825660163 (825.6 MB)\n",
    "        RX errors 0  dropped 0  overruns 0  frame 0\n",
    "        TX packets 58719  bytes 4282355 (4.2 MB)\n",
    "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
    "\n",
    "lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536\n",
    "        inet 127.0.0.1  netmask 255.0.0.0\n",
    "        inet6 ::1  prefixlen 128  scopeid 0x10<host>\n",
    "        loop  txqueuelen 1000  (Local Loopback)\n",
    "        RX packets 290  bytes 438843 (438.8 KB)\n",
    "        RX errors 0  dropped 0  overruns 0  frame 0\n",
    "        TX packets 290  bytes 438843 (438.8 KB)\n",
    "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
    "```\n",
    "\n",
    "C√¥t√© PowerShell/Windows : IP **172.28.176.1**\n",
    "```sh\n",
    "$ ipconfig\n",
    "\n",
    "Configuration IP de Windows\n",
    "\n",
    "\n",
    "Carte Ethernet Ethernet :\n",
    "\n",
    "   Statut du m√©dia. . . . . . . . . . . . : M√©dia d√©connect√©\n",
    "   Suffixe DNS propre √† la connexion. . . :\n",
    "\n",
    "Carte r√©seau sans fil Connexion au r√©seau local* 1¬†:\n",
    "\n",
    "   Statut du m√©dia. . . . . . . . . . . . : M√©dia d√©connect√©\n",
    "   Suffixe DNS propre √† la connexion. . . :\n",
    "\n",
    "Carte r√©seau sans fil Connexion au r√©seau local* 3¬†:\n",
    "\n",
    "   Statut du m√©dia. . . . . . . . . . . . : M√©dia d√©connect√©\n",
    "   Suffixe DNS propre √† la connexion. . . :\n",
    "\n",
    "Carte r√©seau sans fil Wi-Fi¬†:\n",
    "\n",
    "   Suffixe DNS propre √† la connexion. . . :\n",
    "   Adresse IPv6. . . . . . . . . . .¬†. . .: 2a01:e0a:272:1f00:fabb:fe7d:63ce:c470\n",
    "   Adresse IPv6 temporaire . . . . . . . .: 2a01:e0a:272:1f00:607d:4e80:fbb0:a6dd\n",
    "   Adresse IPv6 temporaire . . . . . . . .: 2a01:e0a:272:1f00:6c68:bf4:ce90:39fa\n",
    "   Adresse IPv6 temporaire . . . . . . . .: 2a01:e0a:272:1f00:91ba:73cb:9a8b:6729\n",
    "   Adresse IPv6 de liaison locale. . . . .: fe80::307a:8990:2f21:b062%15\n",
    "   Adresse IPv4. . . . . . . . . . . . . .: 192.168.1.168\n",
    "   Masque de sous-r√©seau. . . .¬†. . . . . : 255.255.255.0\n",
    "   Passerelle par d√©faut. . . .¬†. . . . . : fe80::72fc:8fff:fe50:f53c%15\n",
    "                                       192.168.1.254\n",
    "\n",
    "Carte Ethernet vEthernet (WSL) :\n",
    "\n",
    "   Suffixe DNS propre √† la connexion. . . :\n",
    "   Adresse IPv6 de liaison locale. . . . .: fe80::c684:16ec:dac1:9bf2%44\n",
    "   Adresse IPv4. . . . . . . . . . . . . .: 172.28.176.1\n",
    "   Masque de sous-r√©seau. . . .¬†. . . . . : 255.255.240.0\n",
    "   Passerelle par d√©faut. . . .¬†. . . . . :\n",
    "```\n",
    "\n",
    "\n",
    "Windows Defender :\n",
    "\n",
    "\n",
    "Ouverture des ports Spark/Ubuntu/WSL pour permettre un acc√®s depuis un navigateur depuis la m√™me machine (ou une autre du r√©seau priv√©) √† l'UI de Spark.\n",
    "1. Ouverture de Windows Defender.\n",
    "2. Acc√®s √† la section 'Param√®tres avanc√©s' dans le volet de gauche\n",
    "3. Cliquer sur 'R√®gles de trafic entrant' dans le volet de gauche, puis sur 'Nouvelle r√®gle' dans le volet de droite.\n",
    "4. S√©lectionner 'Port' et cliquer sur 'Suivant'.\n",
    "5. S√©lectionner 'TCP' et entrer \"8080, 7077\" dans le champ 'Ports locaux'. Cliquer sur 'Suivant'.\n",
    "6. S√©lectionner 'Autoriser la connexion' et cliquer sur 'Suivant'.\n",
    "7. S√©lectionnez les profils pour lesquels autoriser la connexion ('Priv√©') et cliquez sur 'Suivant'.\n",
    "8. Nommer la r√®gle (par exemple, 'Spark Ports'), ajouter un descriptif et cliquer sur 'Terminer'.\n",
    "\n",
    "*Ouverture des ports Spark/Ubuntu/WSL pour permettre un acc√®s depuis un navigateur depuis la m√™me machine (ou une autre du r√©seau priv√©) √† l'UI de Spark.*\n",
    "\n",
    "http://172.28.176.216:8080/ depuis un navigateur Windows. Great job, √ßa fonctionne!\n",
    "\n",
    "\n",
    "Les adresse de mon r√©seau priv√© attribu√©es par mon routeur sont en 192.168.1.*.\n",
    "Les connexions passent principalement par les cartes Wifi.\n",
    "Mais les machines sont √©videmment √©quip√©es de cartes ethernet filaires, m√™me si elles ne sont pas utilis√©es.\n",
    "Les deux adresses associ√©es par WSL le sont √† la m√™me carte ethernet, et sont respectivement 172.28.176.1 et 172.28.176.216 : cela signifie que WSL forme son propre r√©seau local entre les diff√©rentes instances de machines (de syst√®mes) qui coexistent sur la m√™me machine, et que ce r√©seau priv√© est donc distinct de mon r√©seau domestique.\n",
    "\n",
    "\n",
    "How do I allow Windows Defender to allow access via HTTP to machines in my home network?\n",
    "https://social.technet.microsoft.com/Forums/en-US/7076fce9-3a42-48c3-b2aa-2c09fc3a064a/how-do-i-allow-windows-defender-to-allow-access-via-http-to-machines-in-my-home-network?forum=win10itpronetworking\n",
    "\n",
    "\n",
    "Lancement un Spark Worker Process esclave :\n",
    "\n",
    "```sh\n",
    "$ start-slave.sh spark://ubuntu:7077\n",
    "starting org.apache.spark.deploy.worker.Worker, logging to /opt/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-<Ma machine>.out\n",
    "```\n",
    "\n",
    "Pas de probl√®me avec l'URL, le Worker client est lanc√©.\n",
    "\n",
    "Dans l'UI, le worker est bien visible.\n",
    "\n",
    "Lancement du `Spark shell`\n",
    "\n",
    "```sh\n",
    "$ spark-shell\n",
    "...\n",
    "scala> ... si on ne ma√Ætrise pas Scala, on a besoin de PySpark !^^\n",
    "```\n",
    "\n",
    "Le Spark Shell pour Scala est un REPL (Read-Eval-Print Loop) qui s'ex√©cute dans le terminal standard du syst√®me d'exploitation, et non dans une interface comme `iPython`. Pour b√©n√©ficier du terminal `iPython` avec `Spark`, il faut utiliser `PySpark`.\n",
    "\n",
    "Deux bonnes raisons de passer √† la derni√®re √©tape d'installation avec `PySpark`.\n",
    "\n",
    "Pour fermer proprement les processus ma√Ætre et esclave lanc√©s pr√©c√©demment :\n",
    "\n",
    "Pas cela, c'est d√©pr√©ci√© :\n",
    "```sh\n",
    "$ SPARK_HOME/sbin/stop-slave.sh\n",
    "$ SPARK_HOME/sbin/stop-master.sh\n",
    "```\n",
    "\n",
    "Mais cela:\n",
    "```sh\n",
    "$ SPARK_HOME/sbin/stop-worker.sh\n",
    "$ SPARK_HOME/sbin/stop-master.sh\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Arrow\n",
    "\n",
    "https://arrow.apache.org/docs/index.html\n",
    "\n",
    "https://pypi.org/project/pyarrow/\n",
    "\n",
    "\n",
    "Apache Arrow est une plate-forme de d√©veloppement pour l'analyse en m√©moire vive. Elle contient un ensemble de technologies qui permettent aux syst√®mes de traitement de donn√©es volumineuses de traiter et de d√©placer rapidement les donn√©es. Elle sp√©cifie un format de m√©moire colonnaire normalis√© ind√©pendant du langage pour les donn√©es plates et hi√©rarchiques, organis√© pour des op√©rations d'analyse efficaces sur le mat√©riel moderne.\n",
    "\n",
    "Le projet d√©veloppe une collection multilingue de biblioth√®ques pour r√©soudre des probl√®mes de syst√®mes li√©s au traitement de donn√©es analytiques en m√©moire vive. Cela inclut des sujets tels que :\n",
    "* Partage de m√©moire et d√©placement de donn√©es bas√©s sur RPC sans copie\n",
    "* Lecture et √©criture de formats de fichiers (comme CSV, Apache ORC et Apache Parquet)\n",
    "* Analyse en m√©moire vive et traitement de requ√™tes\n",
    "\n",
    "On pourrait penser √† tort qu'Apache Arrow est la brique technologique qui permet √† Hadoop de passer en m√©moire vive les concepts de calcul d'Hadoop (paradigme du diviser pour r√©gner impl√©ment√© en le framwork Map/Reduce). Il n'en est rien.\n",
    "\n",
    "Apache Arrow est une technologie distincte de Spark, mais elle peut √™tre utilis√©e en conjonction avec Spark pour am√©liorer les performances de traitement de donn√©es in-memory. Apache Arrow fournit une repr√©sentation efficace et standardis√©e des donn√©es en m√©moire, ind√©pendante de tout langage de programmation, qui peut √™tre utilis√©e pour faciliter le partage de donn√©es entre diff√©rents syst√®mes de traitement de donn√©es. Cela permet de r√©duire les co√ªts de traitement et de stockage des donn√©es en √©vitant les conversions de formats de donn√©es co√ªteuses. Spark utilise la technologie de l'in-memory computing pour ex√©cuter des t√¢ches de traitement de donn√©es plus rapidement que Hadoop MapReduce, mais Apache Arrow peut √™tre utilis√© pour am√©liorer encore ces performances.\n",
    "\n",
    "Apache Arrow et Spark sont tous deux bas√©s sur des concepts similaires de traitement de donn√©es, mais ils sont des technologies compl√©mentaires plut√¥t que concurrentes. Apache Arrow fournit une plate-forme pour le traitement de donn√©es en m√©moire et permet le partage de donn√©es entre diff√©rents syst√®mes, tandis que Spark fournit un framework pour le traitement distribu√© de donn√©es. Ils peuvent √™tre utilis√©s ensemble pour obtenir des performances am√©lior√©es et une int√©gration plus transparente des donn√©es entre diff√©rents syst√®mes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point sur les ressources mat√©rielles disponibles\n",
    "\n",
    "Il s'agit de dimensionner et de bien partager entre le noeud ma√Ætre et l'executor.\n",
    "\n",
    "## C√¥t√© Windows\n",
    "\n",
    "### Processeurs (CPU et GPUs)\n",
    "\n",
    "Pour conna√Ætre le nombre de coeurs et leur capacit√© √† l'hyperthreading sur Windows, il faut ouvrir le gestionnaire des t√¢ches (`Ctrl+Shift+Esc`), puis aller dans l'onglet \"Performances\". On peut y voir le nombre de coeurs physiques et logiques du processeur.\n",
    "\n",
    "**Belladonna** :\n",
    "* CPU - Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz\n",
    "    * Vitesse de base : 2,59 GHz\n",
    "    * Sockets : 1\n",
    "    * Coeurs : 6\n",
    "    * Processeurs logiques : 12\n",
    "    * Virtualisation : Activ√©e\n",
    "    * Cache de niveau 1 : 384 Ko\n",
    "    * Cache de niveau 2 : 1,5 Mo\n",
    "    * Cache de niveau 3 : 12,0 Mo\n",
    "* GPU 0 - Intel(R) UHD Graphics 630\n",
    "    * M√©moire partag√©e : 7,9 Go\n",
    "* GPU 1 - NVIDIA GeForce RTX 2060\n",
    "    * M√©moire d√©di√©e : 6 Go\n",
    "    * M√©moire partag√©e : 7,9 Go\n",
    "\n",
    "\n",
    "On peut aussi utiliser la commande PowerShell suivante :\n",
    "\n",
    "```sh\n",
    "Get-WmiObject -Class Win32_Processor | Select-Object -Property Name, NumberOfCores, NumberOfLogicalProcessors\n",
    "Name                                     NumberOfCores NumberOfLogicalProcessors\n",
    "----                                     ------------- -------------------------\n",
    "Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz             6                        12\n",
    "```\n",
    "\n",
    "Depuis Ubuntu :\n",
    "\n",
    "```sh\n",
    "lscpu\n",
    "Architecture:                    x86_64\n",
    "CPU op-mode(s):                  32-bit, 64-bit\n",
    "Byte Order:                      Little Endian\n",
    "Address sizes:                   39 bits physical, 48 bits virtual\n",
    "CPU(s):                          12\n",
    "On-line CPU(s) list:             0-11\n",
    "Thread(s) per core:              2\n",
    "Core(s) per socket:              6\n",
    "Socket(s):                       1\n",
    "Vendor ID:                       GenuineIntel\n",
    "CPU family:                      6\n",
    "Model:                           158\n",
    "Model name:                      Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz\n",
    "Stepping:                        10\n",
    "CPU MHz:                         2592.007\n",
    "BogoMIPS:                        5184.01\n",
    "Virtualization:                  VT-x\n",
    "Hypervisor vendor:               Microsoft\n",
    "Virtualization type:             full\n",
    "L1d cache:                       192 KiB\n",
    "L1i cache:                       192 KiB\n",
    "L2 cache:                        1.5 MiB\n",
    "L3 cache:                        12 MiB\n",
    "Vulnerability Itlb multihit:     KVM: Mitigation: VMX disabled\n",
    "Vulnerability L1tf:              Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable\n",
    "Vulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\n",
    "Vulnerability Meltdown:          Mitigation; PTI\n",
    "Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\n",
    "Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
    "Vulnerability Spectre v2:        Mitigation; Full generic retpoline, IBPB conditional, IBRS_FW, STIBP conditional, RSB f\n",
    "                                 illing\n",
    "Vulnerability Srbds:             Unknown: Dependent on hypervisor status\n",
    "Vulnerability Tsx async abort:   Not affected\n",
    "Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxs\n",
    "                                 r sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpui\n",
    "                                 d pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c r\n",
    "                                 drand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp tpr_\n",
    "                                 shadow vnmi ept vpid ept_ad fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap c\n",
    "                                 lflushopt xsaveopt xsavec xgetbv1 xsaves flush_l1d arch_capabilities\n",
    "```\n",
    "\n",
    "\n",
    "### M√©moire\n",
    "\n",
    "Pour conna√Ætre la capacit√© m√©moire r√©elle disponible sur Windows, c'est √©galement par le gestionnaire des t√¢ches (`Ctrl+Shift+Esc`), onglet \"Performances\". On peut y voir la quantit√© de m√©moire physique install√©e sur le syst√®me.\n",
    "\n",
    "**Belladonna** :\n",
    "    * 16 Go\n",
    "    * Facteur de forme : SODIMM\n",
    "    * Vitesse : 2667 MHz\n",
    "    * Mat√©riel reserv√© : 163 Mo\n",
    "\n",
    "On peut aussi utiliser la commande PowerShell suivante :\n",
    "\n",
    "```sh\n",
    "(Get-CimInstance -ClassName Win32_PhysicalMemory | Measure-Object -Property Capacity -Sum).Sum / 1GB\n",
    "16\n",
    "```\n",
    "\n",
    "Depuis Ubuntu :\n",
    "\n",
    "```sh\n",
    "$ free -h\n",
    "              total        used        free      shared  buff/cache   available\n",
    "Mem:          7.7Gi       574Mi       3.6Gi       2.0Mi       3.5Gi       6.8Gi\n",
    "Swap:         2.0Gi          0B       2.0Gi\n",
    "```\n",
    "\n",
    "Hum, ce n'est pas assez pr√©cis..\n",
    "\n",
    "```sh\n",
    "$ free -m\n",
    "              total        used        free      shared  buff/cache   available\n",
    "Mem:           7873         574        3732           2        3566        7002\n",
    "Swap:          2048           0        2048\n",
    "```\n",
    "\n",
    "On ne retrouve pas vraiment nos 16 GB!\n",
    "\n",
    "```sh\n",
    "$ cat /proc/meminfo\n",
    "MemTotal:        8062644 kB\n",
    "MemFree:         3822152 kB\n",
    "MemAvailable:    7170564 kB\n",
    "Buffers:          131012 kB\n",
    "Cached:          3315812 kB\n",
    "SwapCached:            0 kB\n",
    "Active:           817908 kB\n",
    "Inactive:        2940468 kB\n",
    "Active(anon):        316 kB\n",
    "Inactive(anon):   313724 kB\n",
    "Active(file):     817592 kB\n",
    "Inactive(file):  2626744 kB\n",
    "...\n",
    "```\n",
    "\n",
    "8062644 kB = 7,7 GB\n",
    "\n",
    "On ne retrouve donc pas c√¥t√© Ubuntu l'int√©gralit√© de la m√©moire mat√©rielle, mais un petit peu moins que la moiti√©.\n",
    "\n",
    "Cela semble indiquer que Windows n'alloue √† son sous-syst√®me WSL qu'une partie de la m√©moire disponible pour assurer ses propres besoins de fonctionnement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation des GPU ?\n",
    "\n",
    "‚ùì Le rapport lscpu est infiniment plus d√©taill√© que ce que l'on obtient c√¥t√© Windows.\n",
    "\n",
    "Question : j'ai deux processeurs graphiques, et j'ai entendu dire qu'ils pouvaient √™tre exploit√©s notamment pour faire de la data.\n",
    "\n",
    "* GPU 0 - Intel(R) UHD Graphics 630\n",
    "    * M√©moire partag√©e : 7,9 Go\n",
    "* GPU 1 - NVIDIA GeForce RTX 2060\n",
    "    * M√©moire d√©di√©e : 6 Go\n",
    "    * M√©moire partag√©e : 7,9 Go\n",
    "\n",
    "Mais les rapports ne d√©taillent pas plus leurs capacit√©s.\n",
    "\n",
    "Comment en pratique les mobiliser par exemple dans le cadre d'un cluster Spark ?\n",
    "\n",
    "üìå Pour utiliser les GPU dans le cadre d'un cluster Spark, nous pouvons utiliser la biblioth√®que `Spark-GPU`, qui permet d'ex√©cuter des calculs sur les GPU avec Spark.\n",
    "\n",
    "C'est une approche √† envisager si les t√¢ches √† ex√©cuter peuvent tirer parti des performances des GPU. A r√©server aux t√¢ches Spark qui impl√©mentent des op√©rations qui peuvent √™tre acc√©l√©r√©es par les GPU, comme des calculs matriciels, des op√©rations de filtrage ou de transformation d'images, etc.\n",
    "\n",
    "Configuration du cluster Spark pour qu'il utilise ces GPU : il faut installer les pilotes GPU appropri√©s sur tous les n≈ìuds du cluster, ainsi que les biblioth√®ques `CUDA` et `cuDNN`. Il faut √©galement configurer la variable d'environnement `PYSPARK_SUBMIT_ARGS` pour inclure les options n√©cessaires pour activer la prise en charge GPU dans Spark.\n",
    "\n",
    "Soumission des t√¢ches Spark : il faut sp√©cifier le nombre de GPU √† utiliser pour chaque t√¢che √† l'aide de la m√©thode `SparkContext.addPyFile` pour ajouter les fichiers Python contenant les fonctions GPU √† utiliser."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå Compl√©ment de configuration client / serveur\n",
    "\n",
    "Dans notre cas de figure o√π le client PySpark est sur l'OS Windows et le serveur (cluster) Spark sur l'OS Ubuntu/WSL, il convient de param√©trer les deux parties pour qu'elles puissent interagir. Voici les √©tapes √† suivre pour modifier les param√®tres de configuration par d√©faut :\n",
    "1. Ouvrir le fichier de configuration `spark-defaults.conf` dans le dossier `conf` de l'installation Spark sur le n≈ìud ma√Ætre Ubuntu.\n",
    "2. Ajoutez les lignes suivantes en modifiant les valeurs pour correspondre √† la configuration :\n",
    "```sh\n",
    "spark.master spark://<adresse_ip_du_noeud_master_ubuntu>:7077\n",
    "spark.driver.host <adresse_ip_du_noeud_windows>\n",
    "```\n",
    "\n",
    "master : 172.28.176.216\n",
    "client : 172.28.176.1\n",
    "\n",
    "3. Sur la machine Windows, ouvrir un terminal et configurer la variable d'environnement `PYSPARK_SUBMIT_ARGS` avec les options de configuration Spark pr√©c√©dentes :\n",
    "\n",
    "```sh\n",
    "set PYSPARK_SUBMIT_ARGS=--master spark://<adresse_ip_du_noeud_master_ubuntu>:7077 --conf spark.driver.host=<adresse_ip_du_noeud_windows>\n",
    "```\n",
    "\n",
    "Test en session, mais en l'occurrence sans int√©r√™t \n",
    "```sh\n",
    "$env:PYSPARK_SUBMIT_ARGS=\"--master spark://172.28.176.216:7077 --conf spark.driver.host=172.28.176.1\"\n",
    "```\n",
    "\n",
    "Variable d'environnement syst√®me :\n",
    "```sh\n",
    "[Environment]::SetEnvironmentVariable(\"PYSPARK_SUBMIT_ARGS\", \"--master spark://172.28.176.216:7077 --conf spark.driver.host=172.28.176.1\", \"Machine\")\n",
    "```\n",
    "\n",
    "4. Lancer PySpark en tapant `pyspark` dans le terminal. PySpark devrait maintenant communiquer avec le cluster Spark sur le n≈ìud ma√Ætre Ubuntu."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå Attention, il faudra peut-√™tre aussi pr√©ciser le param√®tre `spark.driver.bindAddress` qui sp√©cifie l'adresse IP √† laquelle le driver Spark doit √™tre li√© lorsqu'il s'ex√©cute sur une machine √† plusieurs interfaces r√©seau.\n",
    "\n",
    "Par d√©faut, Spark essaie de deviner l'adresse IP en interrogeant la machine pour toutes les adresses IP disponibles et en s√©lectionnant la premi√®re adresse IP non locale qu'il trouve. Le param√®tre `spark.driver.bindAddress` permet de d√©signer l'une de ces adresse en particulier.\n",
    "\n",
    "Si Spark est ex√©cut√© en mode cluster, le param√®tre `spark.driver.bindAddress` doit √™tre d√©fini sur l'adresse IP du n≈ìud ma√Ætre du cluster."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
